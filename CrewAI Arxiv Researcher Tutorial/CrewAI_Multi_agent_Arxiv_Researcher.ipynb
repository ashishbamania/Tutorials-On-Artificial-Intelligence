{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FvZm2G3pe8xC",
        "outputId": "a7af9210-a962-45ba-eacf-5bcf4aa0403b"
      },
      "outputs": [],
      "source": [
        "!uv pip install crewai crewai_tools arxiv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rikLL95NgPnG"
      },
      "outputs": [],
      "source": [
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBLOA1g9gvZC"
      },
      "outputs": [],
      "source": [
        "# Set up environment variables\n",
        "import os\n",
        "\n",
        "openai_api_key = \"YOUR_API_KEY\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "os.environ[\"OPENAI_MODEL_NAME\"] = \"gpt-4o-mini\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQpo_sCpgphT"
      },
      "outputs": [],
      "source": [
        "from crewai import Agent, Task, Crew"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8yjzT39jinu"
      },
      "source": [
        "# Creating Arxiv Search Tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnbP7_pq68PA"
      },
      "outputs": [],
      "source": [
        "from typing import Type, List\n",
        "from pydantic import BaseModel, Field\n",
        "from crewai.tools import BaseTool\n",
        "import arxiv\n",
        "import datetime\n",
        "import time\n",
        "\n",
        "class FetchArxivPapersInput(BaseModel):\n",
        "    \"\"\"Input schema for FetchArxivPapersTool.\"\"\"\n",
        "    target_date: datetime.date = Field(..., description=\"Target date to fetch papers for.\")\n",
        "\n",
        "class FetchArxivPapersTool(BaseTool):\n",
        "    name: str = \"fetch_arxiv_papers\"\n",
        "    description: str = \"Fetches all arXiv papers from selected categories submitted on the target date.\"\n",
        "    args_schema: Type[BaseModel] = FetchArxivPapersInput\n",
        "\n",
        "    def _run(self, target_date: datetime.date) -> List[dict]:\n",
        "        # List of AI-related categories\n",
        "        AI_CATEGORIES = [\"cs.AI\"]\n",
        "\n",
        "        # Define the date range for the target date\n",
        "        start_date = target_date.strftime('%Y%m%d%H%M')\n",
        "        end_date = (target_date + datetime.timedelta(days=1)).strftime('%Y%m%d%H%M')\n",
        "\n",
        "        # Initialize the arXiv client\n",
        "        client = arxiv.Client(\n",
        "            page_size=100,  # Fetch 100 results per page\n",
        "            delay_seconds=3  # Delay between requests to respect rate limits\n",
        "        )\n",
        "\n",
        "        all_papers = []\n",
        "\n",
        "        for category in AI_CATEGORIES:\n",
        "            print(f\"Fetching papers for category: {category}\")\n",
        "\n",
        "            search_query = f\"cat:{category} AND submittedDate:[{start_date} TO {end_date}]\"\n",
        "\n",
        "            search = arxiv.Search(\n",
        "                query=search_query,\n",
        "                sort_by=arxiv.SortCriterion.SubmittedDate,\n",
        "                max_results=None  # Fetch all results\n",
        "            )\n",
        "\n",
        "            # Collect results for the category\n",
        "            category_papers = []\n",
        "            for result in client.results(search):\n",
        "                category_papers.append({\n",
        "                    'title': result.title,\n",
        "                    'authors': [author.name for author in result.authors],\n",
        "                    'summary': result.summary,\n",
        "                    'published': result.published,\n",
        "                    'url': result.entry_id\n",
        "                })\n",
        "\n",
        "                # Delay between requests to respect rate limits\n",
        "                time.sleep(3)\n",
        "\n",
        "            print(f\"Fetched {len(category_papers)} papers from {category}\")\n",
        "            all_papers.extend(category_papers)\n",
        "\n",
        "        return all_papers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ccKLLE47EPC"
      },
      "outputs": [],
      "source": [
        "arxiv_search_tool = FetchArxivPapersTool()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JFURTXuivkx"
      },
      "source": [
        "# Creating Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMRoMkQVik9f"
      },
      "outputs": [],
      "source": [
        "# Agent 1: Arxiv Researcher\n",
        "\n",
        "researcher = Agent(\n",
        "    role = \"Senior Researcher\",\n",
        "    goal = \"Find the top 10 papers from the search results from arXiv on {date}.\"\n",
        "            \"Rank them appropirately.\",\n",
        "    backstory = \"You are a senior researcher with a deep understanding of all topics in AI and AI research.\"\n",
        "                \"You are able to identify the best research papers based on the title and abstract.\",\n",
        "    verbose = True,\n",
        "    tools = [arxiv_search_tool],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVT56bX3kLgM"
      },
      "outputs": [],
      "source": [
        "# Agent 2: Frontend Engineer\n",
        "\n",
        "frontend_engineer = Agent(\n",
        "    role = \"Senior Frontend & AI Engineer\",\n",
        "    goal = \"Compile the results into a HTML file.\",\n",
        "    backstory = \"You are a competent frontend engineer writing HTML and CSS with decades of experience.\"\n",
        "                \"You have also been working with AI for decades and understand it well\",\n",
        "    verbose = True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldIgfxzFkzDz"
      },
      "source": [
        "# Create Tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bklxi-sskwtj"
      },
      "outputs": [],
      "source": [
        "# Task for Arxiv Researcher\n",
        "\n",
        "research_task = Task(\n",
        "    description = (\" Find the top 10 research papers from the search results from arXiv on {date}.\"),\n",
        "    expected_output = (\n",
        "        \"A list of top 10 research papers with the following information in the following format:\"\n",
        "        \"- Title\"\n",
        "        \"- Authors\"\n",
        "        \"- Abstract\"\n",
        "        \"- Link to the paper\"\n",
        "    ),\n",
        "    agent = researcher,\n",
        "    human_input = True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4or6YHH0kvK5"
      },
      "outputs": [],
      "source": [
        "# Task for Frontend Engineer\n",
        "\n",
        "reporting_task = Task(\n",
        "    description = (\"Compile the results into a detailed report in a HTML file.\"),\n",
        "    expected_output = (\n",
        "        \"An HTML file with the results in the following format:\"\n",
        "        \"Top 10 AI Research Papers published on {date}\"\n",
        "        \"Use the tabular format for the following:\"\n",
        "        \"- Title (which on clicking opens the paper in a new tab)\"\n",
        "        \"- Authors\"\n",
        "        \"- Short summary of the abstract (2-4 sentences)\"\n",
        "        \"Please do not add '''html''' to the top and bottom of the final file.\"\n",
        "    ),\n",
        "    agent = frontend_engineer,\n",
        "    context = [research_task],\n",
        "    output_file = \"./ai_research_report.html\",\n",
        "    human_input = True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXdh2RYymlDx"
      },
      "source": [
        "# Creating the Crew"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5lIRY1RmSdn"
      },
      "outputs": [],
      "source": [
        "arxiv_research_crew = Crew(\n",
        "    agents = [researcher, frontend_engineer],\n",
        "    tasks = [research_task, reporting_task],\n",
        "    verbose = True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgCN8WJMmz8Y"
      },
      "source": [
        "# Running the Crew"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-cGhd7Smzhc"
      },
      "outputs": [],
      "source": [
        "crew_inputs = {\n",
        "    \"date\" : \"2025-03-13\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXqlS_ucnTrj",
        "outputId": "13a99e24-af1d-47d6-dd25-9963c4104c86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m\u001b[94m \n",
            "[2025-03-15 00:49:04][🚀 CREW 'CREW' STARTED, 03B5FF71-49EF-43F9-85D3-3B37AFB8712D]: 2025-03-15 00:49:04.230703\u001b[00m\n",
            "\u001b[1m\u001b[94m \n",
            "[2025-03-15 00:49:04][📋 TASK STARTED:  FIND THE TOP 10 RESEARCH PAPERS FROM THE SEARCH RESULTS FROM ARXIV ON 2025-03-13.]: 2025-03-15 00:49:04.248097\u001b[00m\n",
            "\u001b[1m\u001b[94m \n",
            "[2025-03-15 00:49:04][🤖 AGENT 'SENIOR RESEARCHER' STARTED TASK]: 2025-03-15 00:49:04.249293\u001b[00m\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSenior Researcher\u001b[00m\n",
            "\u001b[95m## Task:\u001b[00m \u001b[92m Find the top 10 research papers from the search results from arXiv on 2025-03-13.\u001b[00m\n",
            "\u001b[1m\u001b[94m \n",
            "[2025-03-15 00:49:04][🤖 LLM CALL STARTED]: 2025-03-15 00:49:04.249499\u001b[00m\n",
            "\u001b[1m\u001b[94m \n",
            "[2025-03-15 00:49:05][✅ LLM CALL COMPLETED]: 2025-03-15 00:49:05.901017\u001b[00m\n",
            "\u001b[1m\u001b[94m \n",
            "[2025-03-15 00:49:05][🤖 TOOL USAGE STARTED: 'FETCH_ARXIV_PAPERS']: 2025-03-15 00:49:05.901755\u001b[00m\n",
            "Fetching papers for category: cs.AI\n",
            "Fetched 81 papers from cs.AI\n",
            "\u001b[1m\u001b[94m \n",
            "[2025-03-15 00:53:09][✅ TOOL USAGE FINISHED: 'FETCH_ARXIV_PAPERS']: 2025-03-15 00:53:09.992238\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSenior Researcher\u001b[00m\n",
            "\u001b[95m## Thought:\u001b[00m \u001b[92mI need to fetch the latest research papers from arXiv submitted on the target date of March 13, 2025, to identify the top 10 papers based on their relevance and quality.\u001b[00m\n",
            "\u001b[95m## Using tool:\u001b[00m \u001b[92mfetch_arxiv_papers\u001b[00m\n",
            "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
            "\"{\\\"target_date\\\": \\\"2025-03-13\\\"}\"\u001b[00m\n",
            "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
            "[{'title': 'Studying Classifier(-Free) Guidance From a Classifier-Centric Perspective', 'authors': ['Xiaoming Zhao', 'Alexander G. Schwing'], 'summary': 'Classifier-free guidance has become a staple for conditional generation with\\ndenoising diffusion models. However, a comprehensive understanding of\\nclassifier-free guidance is still missing. In this work, we carry out an\\nempirical study to provide a fresh perspective on classifier-free guidance.\\nConcretely, instead of solely focusing on classifier-free guidance, we trace\\nback to the root, i.e., classifier guidance, pinpoint the key assumption for\\nthe derivation, and conduct a systematic study to understand the role of the\\nclassifier. We find that both classifier guidance and classifier-free guidance\\nachieve conditional generation by pushing the denoising diffusion trajectories\\naway from decision boundaries, i.e., areas where conditional information is\\nusually entangled and is hard to learn. Based on this classifier-centric\\nunderstanding, we propose a generic postprocessing step built upon\\nflow-matching to shrink the gap between the learned distribution for a\\npre-trained denoising diffusion model and the real data distribution, majorly\\naround the decision boundaries. Experiments on various datasets verify the\\neffectiveness of the proposed approach.', 'published': datetime.datetime(2025, 3, 13, 17, 59, 59, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10638v1'}, {'title': 'A Frustratingly Simple Yet Highly Effective Attack Baseline: Over 90% Success Rate Against the Strong Black-box Models of GPT-4.5/4o/o1', 'authors': ['Zhaoyi Li', 'Xiaohan Zhao', 'Dong-Dong Wu', 'Jiacheng Cui', 'Zhiqiang Shen'], 'summary': 'Despite promising performance on open-source large vision-language models\\n(LVLMs), transfer-based targeted attacks often fail against black-box\\ncommercial LVLMs. Analyzing failed adversarial perturbations reveals that the\\nlearned perturbations typically originate from a uniform distribution and lack\\nclear semantic details, resulting in unintended responses. This critical\\nabsence of semantic information leads commercial LVLMs to either ignore the\\nperturbation entirely or misinterpret its embedded semantics, thereby causing\\nthe attack to fail. To overcome these issues, we notice that identifying core\\nsemantic objects is a key objective for models trained with various datasets\\nand methodologies. This insight motivates our approach that refines semantic\\nclarity by encoding explicit semantic details within local regions, thus\\nensuring interoperability and capturing finer-grained features, and by\\nconcentrating modifications on semantically rich areas rather than applying\\nthem uniformly. To achieve this, we propose a simple yet highly effective\\nsolution: at each optimization step, the adversarial image is cropped randomly\\nby a controlled aspect ratio and scale, resized, and then aligned with the\\ntarget image in the embedding space. Experimental results confirm our\\nhypothesis. Our adversarial examples crafted with local-aggregated\\nperturbations focused on crucial regions exhibit surprisingly good\\ntransferability to commercial LVLMs, including GPT-4.5, GPT-4o,\\nGemini-2.0-flash, Claude-3.5-sonnet, Claude-3.7-sonnet, and even reasoning\\nmodels like o1, Claude-3.7-thinking and Gemini-2.0-flash-thinking. Our approach\\nachieves success rates exceeding 90% on GPT-4.5, 4o, and o1, significantly\\noutperforming all prior state-of-the-art attack methods. Our optimized\\nadversarial examples under different configurations and training code are\\navailable at https://github.com/VILA-Lab/M-Attack.', 'published': datetime.datetime(2025, 3, 13, 17, 59, 55, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10635v1'}, {'title': 'Uncertainty in Action: Confidence Elicitation in Embodied Agents', 'authors': ['Tianjiao Yu', 'Vedant Shah', 'Muntasir Wahed', 'Kiet A. Nguyen', 'Adheesh Juvekar', 'Tal August', 'Ismini Lourentzou'], 'summary': 'Expressing confidence is challenging for embodied agents navigating dynamic\\nmultimodal environments, where uncertainty arises from both perception and\\ndecision-making processes. We present the first work investigating embodied\\nconfidence elicitation in open-ended multimodal environments. We introduce\\nElicitation Policies, which structure confidence assessment across inductive,\\ndeductive, and abductive reasoning, along with Execution Policies, which\\nenhance confidence calibration through scenario reinterpretation, action\\nsampling, and hypothetical reasoning. Evaluating agents in calibration and\\nfailure prediction tasks within the Minecraft environment, we show that\\nstructured reasoning approaches, such as Chain-of-Thoughts, improve confidence\\ncalibration. However, our findings also reveal persistent challenges in\\ndistinguishing uncertainty, particularly under abductive settings, underscoring\\nthe need for more sophisticated embodied confidence elicitation methods.', 'published': datetime.datetime(2025, 3, 13, 17, 59, 41, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10628v1'}, {'title': 'SciVerse: Unveiling the Knowledge Comprehension and Visual Reasoning of LMMs on Multi-modal Scientific Problems', 'authors': ['Ziyu Guo', 'Ray Zhang', 'Hao Chen', 'Jialin Gao', 'Dongzhi Jiang', 'Jiaze Wang', 'Pheng-Ann Heng'], 'summary': 'The rapid advancement of Large Multi-modal Models (LMMs) has enabled their\\napplication in scientific problem-solving, yet their fine-grained capabilities\\nremain under-explored. In this paper, we introduce SciVerse, a multi-modal\\nscientific evaluation benchmark to thoroughly assess LMMs across 5,735 test\\ninstances in five distinct versions. We aim to investigate three key dimensions\\nof LMMs: scientific knowledge comprehension, multi-modal content\\ninterpretation, and Chain-of-Thought (CoT) reasoning. To unveil whether LMMs\\npossess sufficient scientific expertise, we first transform each problem into\\nthree versions containing different levels of knowledge required for solving,\\ni.e., Knowledge-free, -lite, and -rich. Then, to explore how LMMs interpret\\nmulti-modal scientific content, we annotate another two versions, i.e.,\\nVision-rich and -only, marking more question information from texts to\\ndiagrams. Comparing the results of different versions, SciVerse systematically\\nexamines the professional knowledge stock and visual perception skills of LMMs\\nin scientific domains. In addition, to rigorously assess CoT reasoning, we\\npropose a new scientific CoT evaluation strategy, conducting a step-wise\\nassessment on knowledge and logical errors in model outputs. Our extensive\\nevaluation of different LMMs on SciVerse reveals critical limitations in their\\nscientific proficiency and provides new insights into future developments.\\nProject page: https://sciverse-cuhk.github.io', 'published': datetime.datetime(2025, 3, 13, 17, 59, 32, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10627v1'}, {'title': 'NIL: No-data Imitation Learning by Leveraging Pre-trained Video Diffusion Models', 'authors': ['Mert Albaba', 'Chenhao Li', 'Markos Diomataris', 'Omid Taheri', 'Andreas Krause', 'Michael Black'], 'summary': \"Acquiring physically plausible motor skills across diverse and unconventional\\nmorphologies-including humanoid robots, quadrupeds, and animals-is essential\\nfor advancing character simulation and robotics. Traditional methods, such as\\nreinforcement learning (RL) are task- and body-specific, require extensive\\nreward function engineering, and do not generalize well. Imitation learning\\noffers an alternative but relies heavily on high-quality expert demonstrations,\\nwhich are difficult to obtain for non-human morphologies. Video diffusion\\nmodels, on the other hand, are capable of generating realistic videos of\\nvarious morphologies, from humans to ants. Leveraging this capability, we\\npropose a data-independent approach for skill acquisition that learns 3D motor\\nskills from 2D-generated videos, with generalization capability to\\nunconventional and non-human forms. Specifically, we guide the imitation\\nlearning process by leveraging vision transformers for video-based comparisons\\nby calculating pair-wise distance between video embeddings. Along with\\nvideo-encoding distance, we also use a computed similarity between segmented\\nvideo frames as a guidance reward. We validate our method on locomotion tasks\\ninvolving unique body configurations. In humanoid robot locomotion tasks, we\\ndemonstrate that 'No-data Imitation Learning' (NIL) outperforms baselines\\ntrained on 3D motion-capture data. Our results highlight the potential of\\nleveraging generative video models for physically plausible skill learning with\\ndiverse morphologies, effectively replacing data collection with data\\ngeneration for imitation learning.\", 'published': datetime.datetime(2025, 3, 13, 17, 59, 24, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10626v1'}, {'title': 'LHM: Large Animatable Human Reconstruction Model from a Single Image in Seconds', 'authors': ['Lingteng Qiu', 'Xiaodong Gu', 'Peihao Li', 'Qi Zuo', 'Weichao Shen', 'Junfei Zhang', 'Kejie Qiu', 'Weihao Yuan', 'Guanying Chen', 'Zilong Dong', 'Liefeng Bo'], 'summary': 'Animatable 3D human reconstruction from a single image is a challenging\\nproblem due to the ambiguity in decoupling geometry, appearance, and\\ndeformation. Recent advances in 3D human reconstruction mainly focus on static\\nhuman modeling, and the reliance of using synthetic 3D scans for training\\nlimits their generalization ability. Conversely, optimization-based video\\nmethods achieve higher fidelity but demand controlled capture conditions and\\ncomputationally intensive refinement processes. Motivated by the emergence of\\nlarge reconstruction models for efficient static reconstruction, we propose LHM\\n(Large Animatable Human Reconstruction Model) to infer high-fidelity avatars\\nrepresented as 3D Gaussian splatting in a feed-forward pass. Our model\\nleverages a multimodal transformer architecture to effectively encode the human\\nbody positional features and image features with attention mechanism, enabling\\ndetailed preservation of clothing geometry and texture. To further boost the\\nface identity preservation and fine detail recovery, we propose a head feature\\npyramid encoding scheme to aggregate multi-scale features of the head regions.\\nExtensive experiments demonstrate that our LHM generates plausible animatable\\nhuman in seconds without post-processing for face and hands, outperforming\\nexisting methods in both reconstruction accuracy and generalization ability.', 'published': datetime.datetime(2025, 3, 13, 17, 59, 21, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10625v1'}, {'title': 'ETCH: Generalizing Body Fitting to Clothed Humans via Equivariant Tightness', 'authors': ['Boqian Li', 'Haiwen Feng', 'Zeyu Cai', 'Michael J. Black', 'Yuliang Xiu'], 'summary': 'Fitting a body to a 3D clothed human point cloud is a common yet challenging\\ntask. Traditional optimization-based approaches use multi-stage pipelines that\\nare sensitive to pose initialization, while recent learning-based methods often\\nstruggle with generalization across diverse poses and garment types. We propose\\nEquivariant Tightness Fitting for Clothed Humans, or ETCH, a novel pipeline\\nthat estimates cloth-to-body surface mapping through locally approximate SE(3)\\nequivariance, encoding tightness as displacement vectors from the cloth surface\\nto the underlying body. Following this mapping, pose-invariant body features\\nregress sparse body markers, simplifying clothed human fitting into an\\ninner-body marker fitting task. Extensive experiments on CAPE and 4D-Dress show\\nthat ETCH significantly outperforms state-of-the-art methods -- both\\ntightness-agnostic and tightness-aware -- in body fitting accuracy on loose\\nclothing (16.7% ~ 69.5%) and shape accuracy (average 49.9%). Our equivariant\\ntightness design can even reduce directional errors by (67.2% ~ 89.8%) in\\none-shot (or out-of-distribution) settings. Qualitative results demonstrate\\nstrong generalization of ETCH, regardless of challenging poses, unseen shapes,\\nloose clothing, and non-rigid dynamics. We will release the code and models\\nsoon for research purposes at https://boqian-li.github.io/ETCH/.', 'published': datetime.datetime(2025, 3, 13, 17, 59, 14, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10624v1'}, {'title': 'Transformers without Normalization', 'authors': ['Jiachen Zhu', 'Xinlei Chen', 'Kaiming He', 'Yann LeCun', 'Zhuang Liu'], 'summary': 'Normalization layers are ubiquitous in modern neural networks and have long\\nbeen considered essential. This work demonstrates that Transformers without\\nnormalization can achieve the same or better performance using a remarkably\\nsimple technique. We introduce Dynamic Tanh (DyT), an element-wise operation\\n$DyT($x$) = \\\\tanh(\\\\alpha $x$)$, as a drop-in replacement for normalization\\nlayers in Transformers. DyT is inspired by the observation that layer\\nnormalization in Transformers often produces tanh-like, $S$-shaped input-output\\nmappings. By incorporating DyT, Transformers without normalization can match or\\nexceed the performance of their normalized counterparts, mostly without\\nhyperparameter tuning. We validate the effectiveness of Transformers with DyT\\nacross diverse settings, ranging from recognition to generation, supervised to\\nself-supervised learning, and computer vision to language models. These\\nfindings challenge the conventional understanding that normalization layers are\\nindispensable in modern neural networks, and offer new insights into their role\\nin deep networks.', 'published': datetime.datetime(2025, 3, 13, 17, 59, 6, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10622v1'}, {'title': 'Siege: Autonomous Multi-Turn Jailbreaking of Large Language Models with Tree Search', 'authors': ['Andy Zhou'], 'summary': 'We introduce Siege, a multi-turn adversarial framework that models the\\ngradual erosion of Large Language Model (LLM) safety through a tree search\\nperspective. Unlike single-turn jailbreaks that rely on one meticulously\\nengineered prompt, Siege expands the conversation at each turn in a\\nbreadth-first fashion, branching out multiple adversarial prompts that exploit\\npartial compliance from previous responses. By tracking these incremental\\npolicy leaks and re-injecting them into subsequent queries, Siege reveals how\\nminor concessions can accumulate into fully disallowed outputs. Evaluations on\\nthe JailbreakBench dataset show that Siege achieves a 100% success rate on\\nGPT-3.5-turbo and 97% on GPT-4 in a single multi-turn run, using fewer queries\\nthan baselines such as Crescendo or GOAT. This tree search methodology offers\\nan in-depth view of how model safeguards degrade over successive dialogue\\nturns, underscoring the urgency of robust multi-turn testing procedures for\\nlanguage models.', 'published': datetime.datetime(2025, 3, 13, 17, 57, 32, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10619v1'}, {'title': 'Compositional Subspace Representation Fine-tuning for Adaptive Large Language Models', 'authors': ['Andy Zhou'], 'summary': 'Adapting large language models to multiple tasks can cause cross-skill\\ninterference, where improvements for one skill degrade another. While methods\\nsuch as LoRA impose orthogonality constraints at the weight level, they do not\\nfully address interference in hidden-state representations. We propose\\nCompositional Subspace Representation Fine-tuning (CS-ReFT), a novel\\nrepresentation-based approach that learns multiple orthonormal subspace\\ntransformations, each specializing in a distinct skill, and composes them via a\\nlightweight router. By isolating these subspace edits in the hidden state,\\nrather than weight matrices, CS-ReFT prevents cross-task conflicts more\\neffectively. On the AlpacaEval benchmark, applying CS-ReFT to Llama-2-7B\\nachieves a 93.94% win rate, surpassing GPT-3.5 Turbo (86.30%) while requiring\\nonly 0.0098% of model parameters. These findings show that specialized\\nrepresentation edits, composed via a simple router, significantly enhance\\nmulti-task instruction following with minimal overhead.', 'published': datetime.datetime(2025, 3, 13, 17, 57, 4, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10617v1'}, {'title': 'Dual-Stage Cross-Modal Network with Dynamic Feature Fusion for Emotional Mimicry Intensity Estimation', 'authors': ['Jun Yu', 'Lingsi Zhu', 'Yanjun Chi', 'Yunxiang Zhang', 'Yang Zheng', 'Yongqi Wang', 'Xilong Lu'], 'summary': 'Emotional Mimicry Intensity (EMI) estimation serves as a critical technology\\nfor understanding human social behavior and enhancing human-computer\\ninteraction experiences, where the core challenge lies in dynamic correlation\\nmodeling and robust fusion of multimodal temporal signals. To address the\\nlimitations of existing methods in insufficient exploitation of modal\\nsynergistic effects, noise sensitivity, and limited fine-grained alignment\\ncapabilities, this paper proposes a dual-stage cross-modal alignment framework.\\nFirst, we construct vision-text and audio-text contrastive learning networks\\nbased on an improved CLIP architecture, achieving preliminary alignment in the\\nfeature space through modality-decoupled pre-training. Subsequently, we design\\na temporal-aware dynamic fusion module that combines Temporal Convolutional\\nNetworks (TCN) and gated bidirectional LSTM to respectively capture the\\nmacro-evolution patterns of facial expressions and local dynamics of acoustic\\nfeatures. Innovatively, we introduce a quality-guided modality fusion strategy\\nthat enables modality compensation under occlusion and noisy scenarios through\\ndifferentiable weight allocation. Experimental results on the Hume-Vidmimic2\\ndataset demonstrate that our method achieves an average Pearson correlation\\ncoefficient of 0.35 across six emotion dimensions, outperforming the best\\nbaseline by 40\\\\%. Ablation studies further validate the effectiveness of the\\ndual-stage training strategy and dynamic fusion mechanism, providing a novel\\ntechnical pathway for fine-grained emotion analysis in open environments.', 'published': datetime.datetime(2025, 3, 13, 17, 46, 16, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10603v1'}, {'title': 'TruthPrInt: Mitigating LVLM Object Hallucination Via Latent Truthful-Guided Pre-Intervention', 'authors': ['Jinhao Duan', 'Fei Kong', 'Hao Cheng', 'James Diffenderfer', 'Bhavya Kailkhura', 'Lichao Sun', 'Xiaofeng Zhu', 'Xiaoshuang Shi', 'Kaidi Xu'], 'summary': 'Object Hallucination (OH) has been acknowledged as one of the major\\ntrustworthy challenges in Large Vision-Language Models (LVLMs). Recent\\nadvancements in Large Language Models (LLMs) indicate that internal states,\\nsuch as hidden states, encode the \"overall truthfulness\" of generated\\nresponses. However, it remains under-explored how internal states in LVLMs\\nfunction and whether they could serve as \"per-token\" hallucination indicators,\\nwhich is essential for mitigating OH. In this paper, we first conduct an\\nin-depth exploration of LVLM internal states in relation to OH issues and\\ndiscover that (1) LVLM internal states are high-specificity per-token\\nindicators of hallucination behaviors. Moreover, (2) different LVLMs encode\\nuniversal patterns of hallucinations in common latent subspaces, indicating\\nthat there exist \"generic truthful directions\" shared by various LVLMs. Based\\non these discoveries, we propose Truthful-Guided Pre-Intervention (TruthPrInt)\\nthat first learns the truthful direction of LVLM decoding and then applies\\ntruthful-guided inference-time intervention during LVLM decoding. We further\\npropose ComnHallu to enhance both cross-LVLM and cross-data hallucination\\ndetection transferability by constructing and aligning hallucination latent\\nsubspaces. We evaluate TruthPrInt in extensive experimental settings, including\\nin-domain and out-of-domain scenarios, over popular LVLMs and OH benchmarks.\\nExperimental results indicate that TruthPrInt significantly outperforms\\nstate-of-the-art methods. Codes will be available at\\nhttps://github.com/jinhaoduan/TruthPrInt.', 'published': datetime.datetime(2025, 3, 13, 17, 46, 6, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10602v1'}, {'title': 'The Spectral Bias of Shallow Neural Network Learning is Shaped by the Choice of Non-linearity', 'authors': ['Justin Sahs', 'Ryan Pyle', 'Fabio Anselmi', 'Ankit Patel'], 'summary': \"Despite classical statistical theory predicting severe overfitting, modern\\nmassively overparameterized neural networks still generalize well. This\\nunexpected property is attributed to the network's so-called implicit bias,\\nwhich describes its propensity to converge to solutions that generalize\\neffectively, among the many possible that correctly label the training data.\\nThe aim of our research is to explore this bias from a new perspective,\\nfocusing on how non-linear activation functions contribute to shaping it.\\nFirst, we introduce a reparameterization which removes a continuous weight\\nrescaling symmetry. Second, in the kernel regime, we leverage this\\nreparameterization to generalize recent findings that relate shallow Neural\\nNetworks to the Radon transform, deriving an explicit formula for the implicit\\nbias induced by a broad class of activation functions. Specifically, by\\nutilizing the connection between the Radon transform and the Fourier transform,\\nwe interpret the kernel regime's inductive bias as minimizing a spectral\\nseminorm that penalizes high-frequency components, in a manner dependent on the\\nactivation function. Finally, in the adaptive regime, we demonstrate the\\nexistence of local dynamical attractors that facilitate the formation of\\nclusters of hyperplanes where the input to a neuron's activation function is\\nzero, yielding alignment between many neurons' response functions. We confirm\\nthese theoretical results with simulations. All together, our work provides a\\ndeeper understanding of the mechanisms underlying the generalization\\ncapabilities of overparameterized neural networks and its relation with the\\nimplicit bias, offering potential pathways for designing more efficient and\\nrobust models.\", 'published': datetime.datetime(2025, 3, 13, 17, 36, 46, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10587v1'}, {'title': 'VisualWebInstruct: Scaling up Multimodal Instruction Data through Web Search', 'authors': ['Yiming Jia', 'Jiachen Li', 'Xiang Yue', 'Bo Li', 'Ping Nie', 'Kai Zou', 'Wenhu Chen'], 'summary': \"Vision-Language Models have made significant progress on many\\nperception-focused tasks, however, their progress on reasoning-focused tasks\\nseem to be limited due to the lack of high-quality and diverse training data.\\nIn this work, we aim to address the scarcity issue of reasoning-focused\\nmultimodal datasets. We propose VisualWebInstruct - a novel approach that\\nleverages search engine to create a diverse, and high-quality dataset spanning\\nmultiple disciplines like math, physics, finance, chemistry, etc. Starting with\\nmeticulously selected 30,000 seed images, we employ Google Image search to\\nidentify websites containing similar images. We collect and process the HTMLs\\nfrom over 700K unique URL sources. Through a pipeline of content extraction,\\nfiltering and synthesis, we build a dataset of approximately 900K\\nquestion-answer pairs, with 40% being visual QA pairs and the rest as text QA\\npairs. Models fine-tuned on VisualWebInstruct demonstrate significant\\nperformance gains: (1) training from Llava-OV-mid shows 10-20% absolute point\\ngains across benchmarks, (2) training from MAmmoTH-VL shows 5% absoluate gain.\\nOur best model MAmmoTH-VL2 shows state-of-the-art performance within the 10B\\nparameter class on MMMU-Pro-std (40.7%), MathVerse (42.6%), and DynaMath\\n(55.7%). These remarkable results highlight the effectiveness of our dataset in\\nenhancing VLMs' reasoning capabilities for complex multimodal tasks.\", 'published': datetime.datetime(2025, 3, 13, 17, 32, 48, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10582v1'}, {'title': 'KUDA: Keypoints to Unify Dynamics Learning and Visual Prompting for Open-Vocabulary Robotic Manipulation', 'authors': ['Zixian Liu', 'Mingtong Zhang', 'Yunzhu Li'], 'summary': 'With the rapid advancement of large language models (LLMs) and\\nvision-language models (VLMs), significant progress has been made in developing\\nopen-vocabulary robotic manipulation systems. However, many existing approaches\\noverlook the importance of object dynamics, limiting their applicability to\\nmore complex, dynamic tasks. In this work, we introduce KUDA, an\\nopen-vocabulary manipulation system that integrates dynamics learning and\\nvisual prompting through keypoints, leveraging both VLMs and learning-based\\nneural dynamics models. Our key insight is that a keypoint-based target\\nspecification is simultaneously interpretable by VLMs and can be efficiently\\ntranslated into cost functions for model-based planning. Given language\\ninstructions and visual observations, KUDA first assigns keypoints to the RGB\\nimage and queries the VLM to generate target specifications. These abstract\\nkeypoint-based representations are then converted into cost functions, which\\nare optimized using a learned dynamics model to produce robotic trajectories.\\nWe evaluate KUDA on a range of manipulation tasks, including free-form language\\ninstructions across diverse object categories, multi-object interactions, and\\ndeformable or granular objects, demonstrating the effectiveness of our\\nframework. The project page is available at http://kuda-dynamics.github.io.', 'published': datetime.datetime(2025, 3, 13, 16, 59, 17, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10546v1'}, {'title': 'Language Models, Graph Searching, and Supervision Adulteration: When More Supervision is Less and How to Make More More', 'authors': ['Arvid Frydenlund'], 'summary': \"This work concerns the path-star task, a minimal example of searching over a\\ngraph. The graph, $G$, is star-shaped with $D$ arms radiating from a start\\nnode, $s$. A language model (LM) is given $G$, $s$, and a target node $t$,\\nwhich ends one of the arms and is tasked with generating the arm containing\\n$t$. The minimal nature of this task means only a single choice needs to be\\nmade: which of the $D$ arms contains $t$?\\n  Decoder-only LMs fail to solve this elementary task above $1/D$ chance due to\\na learned shortcut that absorbs training supervision. We show how this\\npathology is caused by excess supervision and we present a series of solutions\\ndemonstrating that the task is solvable via decoder-only LMs. We find that the\\ntask's minimal nature causes its difficulty, as it prevents task decomposition.\\nOur solutions provide insight into the pathology and its implications for LMs\\ntrained via next-token prediction.\", 'published': datetime.datetime(2025, 3, 13, 16, 56, 47, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10542v1'}, {'title': 'GBSVR: Granular Ball Support Vector Regression', 'authors': ['Reshma Rastogi', 'Ankush Bisht', 'Sanjay Kumar', 'Suresh Chandra'], 'summary': 'Support Vector Regression (SVR) and its variants are widely used to handle\\nregression tasks, however, since their solution involves solving an expensive\\nquadratic programming problem, it limits its application, especially when\\ndealing with large datasets. Additionally, SVR uses an epsilon-insensitive loss\\nfunction which is sensitive to outliers and therefore can adversely affect its\\nperformance. We propose Granular Ball Support Vector Regression (GBSVR) to\\ntackle problem of regression by using granular ball concept. These balls are\\nuseful in simplifying complex data spaces for machine learning tasks, however,\\nto the best of our knowledge, they have not been sufficiently explored for\\nregression problems. Granular balls group the data points into balls based on\\ntheir proximity and reduce the computational cost in SVR by replacing the large\\nnumber of data points with far fewer granular balls. This work also suggests a\\ndiscretization method for continuous-valued attributes to facilitate the\\nconstruction of granular balls. The effectiveness of the proposed approach is\\nevaluated on several benchmark datasets and it outperforms existing\\nstate-of-the-art approaches', 'published': datetime.datetime(2025, 3, 13, 16, 52, 43, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10539v1'}, {'title': 'The Impact of Item-Writing Flaws on Difficulty and Discrimination in Item Response Theory', 'authors': ['Robin Schmucker', 'Steven Moore'], 'summary': 'High-quality test items are essential for educational assessments,\\nparticularly within Item Response Theory (IRT). Traditional validation methods\\nrely on resource-intensive pilot testing to estimate item difficulty and\\ndiscrimination. More recently, Item-Writing Flaw (IWF) rubrics emerged as a\\ndomain-general approach for evaluating test items based on textual features.\\nHowever, their relationship to IRT parameters remains underexplored. To address\\nthis gap, we conducted a study involving over 7,000 multiple-choice questions\\nacross various STEM subjects (e.g., math and biology). Using an automated\\napproach, we annotated each question with a 19-criteria IWF rubric and studied\\nrelationships to data-driven IRT parameters. Our analysis revealed\\nstatistically significant links between the number of IWFs and IRT difficulty\\nand discrimination parameters, particularly in life and physical science\\ndomains. We further observed how specific IWF criteria can impact item quality\\nmore and less severely (e.g., negative wording vs. implausible distractors).\\nOverall, while IWFs are useful for predicting IRT parameters--particularly for\\nscreening low-difficulty MCQs--they cannot replace traditional data-driven\\nvalidation methods. Our findings highlight the need for further research on\\ndomain-general evaluation rubrics and algorithms that understand\\ndomain-specific content for robust item validation.', 'published': datetime.datetime(2025, 3, 13, 16, 47, 7, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10533v1'}, {'title': 'Lightweight Models for Emotional Analysis in Video', 'authors': ['Quoc-Tien Nguyen', 'Hong-Hai Nguyen', 'Van-Thong Huynh'], 'summary': 'In this study, we present an approach for efficient spatiotemporal feature\\nextraction using MobileNetV4 and a multi-scale 3D MLP-Mixer-based temporal\\naggregation module. MobileNetV4, with its Universal Inverted Bottleneck (UIB)\\nblocks, serves as the backbone for extracting hierarchical feature\\nrepresentations from input image sequences, ensuring both computational\\nefficiency and rich semantic encoding. To capture temporal dependencies, we\\nintroduce a three-level MLP-Mixer module, which processes spatial features at\\nmultiple resolutions while maintaining structural integrity. Experimental\\nresults on the ABAW 8th competition demonstrate the effectiveness of our\\napproach, showing promising performance in affective behavior analysis. By\\nintegrating an efficient vision backbone with a structured temporal modeling\\nmechanism, the proposed framework achieves a balance between computational\\nefficiency and predictive accuracy, making it well-suited for real-time\\napplications in mobile and embedded computing environments.', 'published': datetime.datetime(2025, 3, 13, 16, 38, 33, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10530v1'}, {'title': 'PiSA: A Self-Augmented Data Engine and Training Strategy for 3D Understanding with Large Models', 'authors': ['Zilu Guo', 'Hongbin Lin', 'Zhihao Yuan', 'Chaoda Zheng', 'Pengshuo Qiu', 'Dongzhi Jiang', 'Renrui Zhang', 'Chun-Mei Feng', 'Zhen Li'], 'summary': \"3D Multimodal Large Language Models (MLLMs) have recently made substantial\\nadvancements. However, their potential remains untapped, primarily due to the\\nlimited quantity and suboptimal quality of 3D datasets. Current approaches\\nattempt to transfer knowledge from 2D MLLMs to expand 3D instruction data, but\\nstill face modality and domain gaps. To this end, we introduce PiSA-Engine\\n(Point-Self-Augmented-Engine), a new framework for generating instruction\\npoint-language datasets enriched with 3D spatial semantics. We observe that\\nexisting 3D MLLMs offer a comprehensive understanding of point clouds for\\nannotation, while 2D MLLMs excel at cross-validation by providing complementary\\ninformation. By integrating holistic 2D and 3D insights from off-the-shelf\\nMLLMs, PiSA-Engine enables a continuous cycle of high-quality data generation.\\nWe select PointLLM as the baseline and adopt this co-evolution training\\nframework to develop an enhanced 3D MLLM, termed PointLLM-PiSA. Additionally,\\nwe identify limitations in previous 3D benchmarks, which often feature coarse\\nlanguage captions and insufficient category diversity, resulting in inaccurate\\nevaluations. To address this gap, we further introduce PiSA-Bench, a\\ncomprehensive 3D benchmark covering six key aspects with detailed and diverse\\nlabels. Experimental results demonstrate PointLLM-PiSA's state-of-the-art\\nperformance in zero-shot 3D object captioning and generative classification on\\nour PiSA-Bench, achieving significant improvements of 46.45% (+8.33%) and\\n63.75% (+16.25%), respectively. We will release the code, datasets, and\\nbenchmark.\", 'published': datetime.datetime(2025, 3, 13, 16, 37, 26, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10529v1'}, {'title': 'CountPath: Automating Fragment Counting in Digital Pathology', 'authors': ['Ana Beatriz Vieira', 'Maria Valente', 'Diana Montezuma', 'Tomé Albuquerque', 'Liliana Ribeiro', 'Domingos Oliveira', 'João Monteiro', 'Sofia Gonçalves', 'Isabel M. Pinto', 'Jaime S. Cardoso', 'Arlindo L. Oliveira'], 'summary': 'Quality control of medical images is a critical component of digital\\npathology, ensuring that diagnostic images meet required standards. A\\npre-analytical task within this process is the verification of the number of\\nspecimen fragments, a process that ensures that the number of fragments on a\\nslide matches the number documented in the macroscopic report. This step is\\nimportant to ensure that the slides contain the appropriate diagnostic material\\nfrom the grossing process, thereby guaranteeing the accuracy of subsequent\\nmicroscopic examination and diagnosis. Traditionally, this assessment is\\nperformed manually, requiring significant time and effort while being subject\\nto significant variability due to its subjective nature. To address these\\nchallenges, this study explores an automated approach to fragment counting\\nusing the YOLOv9 and Vision Transformer models. Our results demonstrate that\\nthe automated system achieves a level of performance comparable to expert\\nassessments, offering a reliable and efficient alternative to manual counting.\\nAdditionally, we present findings on interobserver variability, showing that\\nthe automated approach achieves an accuracy of 86%, which falls within the\\nrange of variation observed among experts (82-88%), further supporting its\\npotential for integration into routine pathology workflows.', 'published': datetime.datetime(2025, 3, 13, 16, 29, 16, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10520v1'}, {'title': 'Why the Brain Cannot Be a Digital Computer: History-Dependence and the Computational Limits of Consciousness', 'authors': ['Andrew Knight'], 'summary': 'This paper presents a novel information-theoretic proof demonstrating that\\nthe human brain as currently understood cannot function as a classical digital\\ncomputer. Through systematic quantification of distinguishable conscious states\\nand their historical dependencies, we establish that the minimum information\\nrequired to specify a conscious state exceeds the physical information capacity\\nof the human brain by a significant factor. Our analysis calculates the\\nbit-length requirements for representing consciously distinguishable sensory\\n\"stimulus frames\" and demonstrates that consciousness exhibits mandatory\\ntemporal-historical dependencies that multiply these requirements beyond the\\nbrain\\'s storage capabilities. This mathematical approach offers new insights\\ninto the fundamental limitations of computational models of consciousness and\\nsuggests that non-classical information processing mechanisms may be necessary\\nto account for conscious experience.', 'published': datetime.datetime(2025, 3, 13, 16, 27, 42, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10518v1'}, {'title': 'Conformal Prediction Sets for Deep Generative Models via Reduction to Conformal Regression', 'authors': ['Hooman Shahrokhi', 'Devjeet Raj Roy', 'Yan Yan', 'Venera Arnaoudova', 'Janaradhan Rao Doppa'], 'summary': 'We consider the problem of generating valid and small prediction sets by\\nsampling outputs (e.g., software code and natural language text) from a\\nblack-box deep generative model for a given input (e.g., textual prompt). The\\nvalidity of a prediction set is determined by a user-defined binary\\nadmissibility function depending on the target application. For example,\\nrequiring at least one program in the set to pass all test cases in code\\ngeneration application. To address this problem, we develop a simple and\\neffective conformal inference algorithm referred to as Generative Prediction\\nSets (GPS). Given a set of calibration examples and black-box access to a deep\\ngenerative model, GPS can generate prediction sets with provable guarantees.\\nThe key insight behind GPS is to exploit the inherent structure within the\\ndistribution over the minimum number of samples needed to obtain an admissible\\noutput to develop a simple conformal regression approach over the minimum\\nnumber of samples. Experiments on multiple datasets for code and math word\\nproblems using different large language models demonstrate the efficacy of GPS\\nover state-of-the-art methods.', 'published': datetime.datetime(2025, 3, 13, 16, 16, 23, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10512v1'}, {'title': 'Explainable Bayesian deep learning through input-skip Latent Binary Bayesian Neural Networks', 'authors': ['Eirik Høyheim', 'Lars Skaaret-Lund', 'Solve Sæbø', 'Aliaksandr Hubin'], 'summary': 'Modeling natural phenomena with artificial neural networks (ANNs) often\\nprovides highly accurate predictions. However, ANNs often suffer from\\nover-parameterization, complicating interpretation and raising uncertainty\\nissues. Bayesian neural networks (BNNs) address the latter by representing\\nweights as probability distributions, allowing for predictive uncertainty\\nevaluation. Latent binary Bayesian neural networks (LBBNNs) further handle\\nstructural uncertainty and sparsify models by removing redundant weights. This\\narticle advances LBBNNs by enabling covariates to skip to any succeeding layer\\nor be excluded, simplifying networks and clarifying input impacts on\\npredictions. Ultimately, a linear model or even a constant can be found to be\\noptimal for a specific problem at hand. Furthermore, the input-skip LBBNN\\napproach reduces network density significantly compared to standard LBBNNs,\\nachieving over 99% reduction for small networks and over 99.9% for larger ones,\\nwhile still maintaining high predictive accuracy and uncertainty measurement.\\nFor example, on MNIST, we reached 97% accuracy and great calibration with just\\n935 weights, reaching state-of-the-art for compression of neural networks.\\nFurthermore, the proposed method accurately identifies the true covariates and\\nadjusts for system non-linearity. The main contribution is the introduction of\\nactive paths, enhancing directly designed global and local explanations within\\nthe LBBNN framework, that have theoretical guarantees and do not require post\\nhoc external tools for explanations.', 'published': datetime.datetime(2025, 3, 13, 15, 59, 3, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10496v1'}, {'title': 'LLMs in Disease Diagnosis: A Comparative Study of DeepSeek-R1 and O3 Mini Across Chronic Health Conditions', 'authors': ['Gaurav Kumar Gupta', 'Pranal Pande'], 'summary': 'Large Language Models (LLMs) are revolutionizing medical diagnostics by\\nenhancing both disease classification and clinical decision-making. In this\\nstudy, we evaluate the performance of two LLM- based diagnostic tools, DeepSeek\\nR1 and O3 Mini, using a structured dataset of symptoms and diagnoses. We\\nassessed their predictive accuracy at both the disease and category levels, as\\nwell as the reliability of their confidence scores. DeepSeek R1 achieved a\\ndisease-level accuracy of 76% and an overall accuracy of 82%, outperforming O3\\nMini, which attained 72% and 75% respectively. Notably, DeepSeek R1\\ndemonstrated exceptional performance in Mental Health, Neurological Disorders,\\nand Oncology, where it reached 100% accuracy, while O3 Mini excelled in\\nAutoimmune Disease classification with 100% accuracy. Both models, however,\\nstruggled with Respiratory Disease classification, recording accuracies of only\\n40% for DeepSeek R1 and 20% for O3 Mini. Additionally, the analysis of\\nconfidence scores revealed that DeepSeek R1 provided high-confidence\\npredictions in 92% of cases, compared to 68% for O3 Mini. Ethical\\nconsiderations regarding bias, model interpretability, and data privacy are\\nalso discussed to ensure the responsible integration of LLMs into clinical\\npractice. Overall, our findings offer valuable insights into the strengths and\\nlimitations of LLM-based diagnostic systems and provide a roadmap for future\\nenhancements in AI-driven healthcare.', 'published': datetime.datetime(2025, 3, 13, 15, 54, 26, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10486v1'}, {'title': 'DeclareAligner: A Leap Towards Efficient Optimal Alignments for Declarative Process Model Conformance Checking', 'authors': ['Jacobo Casas-Ramos', 'Manuel Lama', 'Manuel Mucientes'], 'summary': 'In many engineering applications, processes must be followed precisely,\\nmaking conformance checking between event logs and declarative process models\\ncrucial for ensuring adherence to desired behaviors. This is a critical area\\nwhere Artificial Intelligence (AI) plays a pivotal role in driving effective\\nprocess improvement. However, computing optimal alignments poses significant\\ncomputational challenges due to the vast search space inherent in these models.\\nConsequently, existing approaches often struggle with scalability and\\nefficiency, limiting their applicability in real-world settings. This paper\\nintroduces DeclareAligner, a novel algorithm that uses the A* search algorithm,\\nan established AI pathfinding technique, to tackle the problem from a fresh\\nperspective leveraging the flexibility of declarative models. Key features of\\nDeclareAligner include only performing actions that actively contribute to\\nfixing constraint violations, utilizing a tailored heuristic to navigate\\ntowards optimal solutions, and employing early pruning to eliminate\\nunproductive branches, while also streamlining the process through\\npreprocessing and consolidating multiple fixes into unified actions. The\\nproposed method is evaluated using 8,054 synthetic and real-life alignment\\nproblems, demonstrating its ability to efficiently compute optimal alignments\\nby significantly outperforming the current state of the art. By enabling\\nprocess analysts to more effectively identify and understand conformance\\nissues, DeclareAligner has the potential to drive meaningful process\\nimprovement and management.', 'published': datetime.datetime(2025, 3, 13, 15, 49, 29, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10479v1'}, {'title': 'Siamese Foundation Models for Crystal Structure Prediction', 'authors': ['Liming Wu', 'Wenbing Huang', 'Rui Jiao', 'Jianxing Huang', 'Liwei Liu', 'Yipeng Zhou', 'Hao Sun', 'Yang Liu', 'Fuchun Sun', 'Yuxiang Ren', 'Jirong Wen'], 'summary': \"Crystal Structure Prediction (CSP), which aims to generate stable crystal\\nstructures from compositions, represents a critical pathway for discovering\\nnovel materials. While structure prediction tasks in other domains, such as\\nproteins, have seen remarkable progress, CSP remains a relatively underexplored\\narea due to the more complex geometries inherent in crystal structures. In this\\npaper, we propose Siamese foundation models specifically designed to address\\nCSP. Our pretrain-finetune framework, named DAO, comprises two complementary\\nfoundation models: DAO-G for structure generation and DAO-P for energy\\nprediction. Experiments on CSP benchmarks (MP-20 and MPTS-52) demonstrate that\\nour DAO-G significantly surpasses state-of-the-art (SOTA) methods across all\\nmetrics. Extensive ablation studies further confirm that DAO-G excels in\\ngenerating diverse polymorphic structures, and the dataset relaxation and\\nenergy guidance provided by DAO-P are essential for enhancing DAO-G's\\nperformance. When applied to three real-world superconductors\\n($\\\\text{CsV}_3\\\\text{Sb}_5$, $ \\\\text{Zr}_{16}\\\\text{Rh}_8\\\\text{O}_4$ and\\n$\\\\text{Zr}_{16}\\\\text{Pd}_8\\\\text{O}_4$) that are known to be challenging to\\nanalyze, our foundation models achieve accurate critical temperature\\npredictions and structure generations. For instance, on\\n$\\\\text{CsV}_3\\\\text{Sb}_5$, DAO-G generates a structure close to the\\nexperimental one with an RMSE of 0.0085; DAO-P predicts the $T_c$ value with\\nhigh accuracy (2.26 K vs. the ground-truth value of 2.30 K). In contrast,\\nconventional DFT calculators like Quantum Espresso only successfully derive the\\nstructure of the first superconductor within an acceptable time, while the RMSE\\nis nearly 8 times larger, and the computation speed is more than 1000 times\\nslower. These compelling results collectively highlight the potential of our\\napproach for advancing materials science research and development.\", 'published': datetime.datetime(2025, 3, 13, 15, 44, 16, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10471v1'}, {'title': 'DynaCode: A Dynamic Complexity-Aware Code Benchmark for Evaluating Large Language Models in Code Generation', 'authors': ['Wenhao Hu', 'Jinhao Duan', 'Chunchen Wei', 'Li Zhang', 'Yue Zhang', 'Kaidi Xu'], 'summary': \"The rapid advancement of large language models (LLMs) has significantly\\nimproved their performance in code generation tasks. However, existing code\\nbenchmarks remain static, consisting of fixed datasets with predefined\\nproblems. This makes them vulnerable to memorization during training, where\\nLLMs recall specific test cases instead of generalizing to new problems,\\nleading to data contamination and unreliable evaluation results. To address\\nthese issues, we introduce DynaCode, a dynamic, complexity-aware benchmark that\\novercomes the limitations of static datasets. DynaCode evaluates LLMs\\nsystematically using a complexity-aware metric, incorporating both code\\ncomplexity and call-graph structures. DynaCode achieves large-scale diversity,\\ngenerating up to 189 million unique nested code problems across four distinct\\nlevels of code complexity, referred to as units, and 16 types of call graphs.\\nResults on 12 latest LLMs show an average performance drop of 16.8% to 45.7%\\ncompared to MBPP+, a static code generation benchmark, with performance\\nprogressively decreasing as complexity increases. This demonstrates DynaCode's\\nability to effectively differentiate LLMs. Additionally, by leveraging call\\ngraphs, we gain insights into LLM behavior, particularly their preference for\\nhandling subfunction interactions within nested code.\", 'published': datetime.datetime(2025, 3, 13, 15, 18, 56, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10452v1'}, {'title': 'Whisper Speaker Identification: Leveraging Pre-Trained Multilingual Transformers for Robust Speaker Embeddings', 'authors': ['Jakaria Islam Emon', 'Md Abu Salek', 'Kazi Tamanna Alam'], 'summary': 'Speaker identification in multilingual settings presents unique challenges,\\nparticularly when conventional models are predominantly trained on English\\ndata. In this paper, we propose WSI (Whisper Speaker Identification), a\\nframework that repurposes the encoder of the Whisper automatic speech\\nrecognition model pre trained on extensive multilingual data to generate robust\\nspeaker embeddings via a joint loss optimization strategy that leverages online\\nhard triplet mining and self supervised Normalized Temperature-scaled Cross\\nEntropy loss. By capitalizing on Whisper language-agnostic acoustic\\nrepresentations, our approach effectively distinguishes speakers across diverse\\nlanguages and recording conditions. Extensive evaluations on multiple corpora,\\nincluding VoxTube (multilingual), JVS (Japanese), CallHome (German, Spanish,\\nChinese, and Japanese), and Voxconverse (English), demonstrate that WSI\\nconsistently outperforms state-of-the-art baselines, namely Pyannote Embedding,\\nECAPA TDNN, and Xvector, in terms of lower equal error rates and higher AUC\\nscores. These results validate our hypothesis that a multilingual pre-trained\\nASR encoder, combined with joint loss optimization, substantially improves\\nspeaker identification performance in non-English languages.', 'published': datetime.datetime(2025, 3, 13, 15, 11, 28, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10446v1'}, {'title': 'dFLMoE: Decentralized Federated Learning via Mixture of Experts for Medical Data Analysis', 'authors': ['Luyuan Xie', 'Tianyu Luan', 'Wenyuan Cai', 'Guochen Yan', 'Zhaoyu Chen', 'Nan Xi', 'Yuejian Fang', 'Qingni Shen', 'Zhonghai Wu', 'Junsong Yuan'], 'summary': \"Federated learning has wide applications in the medical field. It enables\\nknowledge sharing among different healthcare institutes while protecting\\npatients' privacy. However, existing federated learning systems are typically\\ncentralized, requiring clients to upload client-specific knowledge to a central\\nserver for aggregation. This centralized approach would integrate the knowledge\\nfrom each client into a centralized server, and the knowledge would be already\\nundermined during the centralized integration before it reaches back to each\\nclient. Besides, the centralized approach also creates a dependency on the\\ncentral server, which may affect training stability if the server malfunctions\\nor connections are unstable. To address these issues, we propose a\\ndecentralized federated learning framework named dFLMoE. In our framework,\\nclients directly exchange lightweight head models with each other. After\\nexchanging, each client treats both local and received head models as\\nindividual experts, and utilizes a client-specific Mixture of Experts (MoE)\\napproach to make collective decisions. This design not only reduces the\\nknowledge damage with client-specific aggregations but also removes the\\ndependency on the central server to enhance the robustness of the framework. We\\nvalidate our framework on multiple medical tasks, demonstrating that our method\\nevidently outperforms state-of-the-art approaches under both model homogeneity\\nand heterogeneity settings.\", 'published': datetime.datetime(2025, 3, 13, 14, 35, 47, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10412v1'}, {'title': 'RealGeneral: Unifying Visual Generation via Temporal In-Context Learning with Video Models', 'authors': ['Yijing Lin', 'Mengqi Huang', 'Shuhan Zhuang', 'Zhendong Mao'], 'summary': 'Unifying diverse image generation tasks within a single framework remains a\\nfundamental challenge in visual generation. While large language models (LLMs)\\nachieve unification through task-agnostic data and generation, existing visual\\ngeneration models fail to meet these principles. Current approaches either rely\\non per-task datasets and large-scale training or adapt pre-trained image models\\nwith task-specific modifications, limiting their generalizability. In this\\nwork, we explore video models as a foundation for unified image generation,\\nleveraging their inherent ability to model temporal correlations. We introduce\\nRealGeneral, a novel framework that reformulates image generation as a\\nconditional frame prediction task, analogous to in-context learning in LLMs. To\\nbridge the gap between video models and condition-image pairs, we propose (1) a\\nUnified Conditional Embedding module for multi-modal alignment and (2) a\\nUnified Stream DiT Block with decoupled adaptive LayerNorm and attention mask\\nto mitigate cross-modal interference. RealGeneral demonstrates effectiveness in\\nmultiple important visual generation tasks, e.g., it achieves a 14.5%\\nimprovement in subject similarity for customized generation and a 10%\\nenhancement in image quality for canny-to-image task. Project page:\\nhttps://lyne1.github.io/RealGeneral/', 'published': datetime.datetime(2025, 3, 13, 14, 31, 52, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10406v1'}, {'title': 'RoMA: Scaling up Mamba-based Foundation Models for Remote Sensing', 'authors': ['Fengxiang Wang', 'Hongzhen Wang', 'Yulin Wang', 'Di Wang', 'Mingshuo Chen', 'Haiyan Zhao', 'Yangang Sun', 'Shuo Wang', 'Long Lan', 'Wenjing Yang', 'Jing Zhang'], 'summary': 'Recent advances in self-supervised learning for Vision Transformers (ViTs)\\nhave fueled breakthroughs in remote sensing (RS) foundation models. However,\\nthe quadratic complexity of self-attention poses a significant barrier to\\nscalability, particularly for large models and high-resolution images. While\\nthe linear-complexity Mamba architecture offers a promising alternative,\\nexisting RS applications of Mamba remain limited to supervised tasks on small,\\ndomain-specific datasets. To address these challenges, we propose RoMA, a\\nframework that enables scalable self-supervised pretraining of Mamba-based RS\\nfoundation models using large-scale, diverse, unlabeled data. RoMA enhances\\nscalability for high-resolution images through a tailored auto-regressive\\nlearning strategy, incorporating two key innovations: 1) a rotation-aware\\npretraining mechanism combining adaptive cropping with angular embeddings to\\nhandle sparsely distributed objects with arbitrary orientations, and 2)\\nmulti-scale token prediction objectives that address the extreme variations in\\nobject scales inherent to RS imagery. Systematic empirical studies validate\\nthat Mamba adheres to RS data and parameter scaling laws, with performance\\nscaling reliably as model and data size increase. Furthermore, experiments\\nacross scene classification, object detection, and semantic segmentation tasks\\ndemonstrate that RoMA-pretrained Mamba models consistently outperform ViT-based\\ncounterparts in both accuracy and computational efficiency. The source code and\\npretrained models will be released at https://github.com/MiliLab/RoMA.', 'published': datetime.datetime(2025, 3, 13, 14, 9, 18, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10392v1'}, {'title': 'CINEMA: Coherent Multi-Subject Video Generation via MLLM-Based Guidance', 'authors': ['Yufan Deng', 'Xun Guo', 'Yizhi Wang', 'Jacob Zhiyuan Fang', 'Angtian Wang', 'Shenghai Yuan', 'Yiding Yang', 'Bo Liu', 'Haibin Huang', 'Chongyang Ma'], 'summary': 'Video generation has witnessed remarkable progress with the advent of deep\\ngenerative models, particularly diffusion models. While existing methods excel\\nin generating high-quality videos from text prompts or single images,\\npersonalized multi-subject video generation remains a largely unexplored\\nchallenge. This task involves synthesizing videos that incorporate multiple\\ndistinct subjects, each defined by separate reference images, while ensuring\\ntemporal and spatial consistency. Current approaches primarily rely on mapping\\nsubject images to keywords in text prompts, which introduces ambiguity and\\nlimits their ability to model subject relationships effectively. In this paper,\\nwe propose CINEMA, a novel framework for coherent multi-subject video\\ngeneration by leveraging Multimodal Large Language Model (MLLM). Our approach\\neliminates the need for explicit correspondences between subject images and\\ntext entities, mitigating ambiguity and reducing annotation effort. By\\nleveraging MLLM to interpret subject relationships, our method facilitates\\nscalability, enabling the use of large and diverse datasets for training.\\nFurthermore, our framework can be conditioned on varying numbers of subjects,\\noffering greater flexibility in personalized content creation. Through\\nextensive evaluations, we demonstrate that our approach significantly improves\\nsubject consistency, and overall video coherence, paving the way for advanced\\napplications in storytelling, interactive media, and personalized video\\ngeneration.', 'published': datetime.datetime(2025, 3, 13, 14, 7, 58, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10391v1'}, {'title': 'A Multimodal Fusion Model Leveraging MLP Mixer and Handcrafted Features-based Deep Learning Networks for Facial Palsy Detection', 'authors': ['Heng Yim Nicole Oo', 'Min Hun Lee', 'Jeong Hoon Lim'], 'summary': 'Algorithmic detection of facial palsy offers the potential to improve current\\npractices, which usually involve labor-intensive and subjective assessments by\\nclinicians. In this paper, we present a multimodal fusion-based deep learning\\nmodel that utilizes an MLP mixer-based model to process unstructured data (i.e.\\nRGB images or images with facial line segments) and a feed-forward neural\\nnetwork to process structured data (i.e. facial landmark coordinates, features\\nof facial expressions, or handcrafted features) for detecting facial palsy. We\\nthen contribute to a study to analyze the effect of different data modalities\\nand the benefits of a multimodal fusion-based approach using videos of 20\\nfacial palsy patients and 20 healthy subjects. Our multimodal fusion model\\nachieved 96.00 F1, which is significantly higher than the feed-forward neural\\nnetwork trained on handcrafted features alone (82.80 F1) and an MLP mixer-based\\nmodel trained on raw RGB images (89.00 F1).', 'published': datetime.datetime(2025, 3, 13, 13, 48, 35, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10371v1'}, {'title': 'G-Boost: Boosting Private SLMs with General LLMs', 'authors': ['Yijiang Fan', 'Yuren Mao', 'Longbin Lai', 'Ying Zhang', 'Zhengping Qian', 'Yunjun Gao'], 'summary': 'Due to the limited computational resources, most Large Language Models (LLMs)\\ndevelopers can only fine-tune Small Language Models (SLMs) on their own data.\\nThese private SLMs typically have limited effectiveness. To boost the\\nperformance of private SLMs, this paper proposes to ask general LLMs for help.\\nThe general LLMs can be APIs or larger LLMs whose inference cost the developers\\ncan afford. Specifically, we propose the G-Boost framework where a private SLM\\nadaptively performs collaborative inference with a general LLM under the guide\\nof process reward. Experiments demonstrate that our framework can significantly\\nboost the performance of private SLMs.', 'published': datetime.datetime(2025, 3, 13, 13, 47, 3, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10367v1'}, {'title': 'Object detection characteristics in a learning factory environment using YOLOv8', 'authors': ['Toni Schneidereit', 'Stefan Gohrenz', 'Michael Breuß'], 'summary': 'AI-based object detection, and efforts to explain and investigate their\\ncharacteristics, is a topic of high interest. The impact of, e.g., complex\\nbackground structures with similar appearances as the objects of interest, on\\nthe detection accuracy and, beforehand, the necessary dataset composition are\\ntopics of ongoing research. In this paper, we present a systematic\\ninvestigation of background influences and different features of the object to\\nbe detected. The latter includes various materials and surfaces, partially\\ntransparent and with shiny reflections in the context of an Industry 4.0\\nlearning factory. Different YOLOv8 models have been trained for each of the\\nmaterials on different sized datasets, where the appearance was the only\\nchanging parameter. In the end, similar characteristics tend to show different\\nbehaviours and sometimes unexpected results. While some background components\\ntend to be detected, others with the same features are not part of the\\ndetection. Additionally, some more precise conclusions can be drawn from the\\nresults. Therefore, we contribute a challenging dataset with detailed\\ninvestigations on 92 trained YOLO models, addressing some issues on the\\ndetection accuracy and possible overfitting.', 'published': datetime.datetime(2025, 3, 13, 13, 33, 27, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10356v1'}, {'title': 'KV-Distill: Nearly Lossless Learnable Context Compression for LLMs', 'authors': ['Vivek Chari', 'Guanghui Qin', 'Benjamin Van Durme'], 'summary': 'Sequence-to-sequence tasks often benefit from long contexts, but the\\nquadratic complexity of self-attention in standard Transformers renders this\\nnon-trivial. During generation, temporary representations -stored in the\\nso-called KV cache-account for a large portion of GPU memory usage and scale\\nlinearly with context length. We introduce KV-Distill, a Transformer\\ncompression framework that distills long context KV caches into significantly\\nshorter representations in a question-independent fashion. KV-Distill can be\\ntrained as a parameter-efficient adaptor for pretrained models, and enables the\\ncompression of arbitrary spans of a context while preserving pre-trained model\\ncapabilities. We treat a compressed-uncompressed cache as a student-teacher\\npairing and apply a KL-type divergence to match the generated outputs.\\nKV-Distill outperforms other compression techniques in worst-case extractive\\ntasks and approaches uncompressed performance in long context question\\nanswering and summarization, and it can be fine-tuned on domain-specific\\ncontexts to reduce lengths by up to 99% while preserving downstream\\nperformance. We demonstrate the generalizability of KV-Distill across various\\nmodel sizes and architectures.', 'published': datetime.datetime(2025, 3, 13, 13, 15, 28, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10337v1'}, {'title': 'OSMa-Bench: Evaluating Open Semantic Mapping Under Varying Lighting Conditions', 'authors': ['Maxim Popov', 'Regina Kurkova', 'Mikhail Iumanov', 'Jaafar Mahmoud', 'Sergey Kolyubin'], 'summary': 'Open Semantic Mapping (OSM) is a key technology in robotic perception,\\ncombining semantic segmentation and SLAM techniques. This paper introduces a\\ndynamically configurable and highly automated LLM/LVLM-powered pipeline for\\nevaluating OSM solutions called OSMa-Bench (Open Semantic Mapping Benchmark).\\nThe study focuses on evaluating state-of-the-art semantic mapping algorithms\\nunder varying indoor lighting conditions, a critical challenge in indoor\\nenvironments. We introduce a novel dataset with simulated RGB-D sequences and\\nground truth 3D reconstructions, facilitating the rigorous analysis of mapping\\nperformance across different lighting conditions. Through experiments on\\nleading models such as ConceptGraphs, BBQ and OpenScene, we evaluate the\\nsemantic fidelity of object recognition and segmentation. Additionally, we\\nintroduce a Scene Graph evaluation method to analyze the ability of models to\\ninterpret semantic structure. The results provide insights into the robustness\\nof these models, forming future research directions for developing resilient\\nand adaptable robotic systems. Our code is available at\\nhttps://be2rlab.github.io/OSMa-Bench/.', 'published': datetime.datetime(2025, 3, 13, 13, 7, 51, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10331v1'}, {'title': 'Enhance Exploration in Safe Reinforcement Learning with Contrastive Representation Learning', 'authors': ['Duc Kien Doan', 'Bang Giang Le', 'Viet Cuong Ta'], 'summary': 'In safe reinforcement learning, agent needs to balance between exploration\\nactions and safety constraints. Following this paradigm, domain transfer\\napproaches learn a prior Q-function from the related environments to prevent\\nunsafe actions. However, because of the large number of false positives, some\\nsafe actions are never executed, leading to inadequate exploration in\\nsparse-reward environments. In this work, we aim to learn an efficient state\\nrepresentation to balance the exploration and safety-prefer action in a\\nsparse-reward environment. Firstly, the image input is mapped to latent\\nrepresentation by an auto-encoder. A further contrastive learning objective is\\nemployed to distinguish safe and unsafe states. In the learning phase, the\\nlatent distance is used to construct an additional safety check, which allows\\nthe agent to bias the exploration if it visits an unsafe state. To verify the\\neffectiveness of our method, the experiment is carried out in three\\nnavigation-based MiniGrid environments. The result highlights that our method\\ncan explore the environment better while maintaining a good balance between\\nsafety and efficiency.', 'published': datetime.datetime(2025, 3, 13, 12, 53, 42, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10318v1'}, {'title': 'Nash Equilibrium Constrained Auto-bidding With Bi-level Reinforcement Learning', 'authors': ['Zhiyu Mou', 'Miao Xu', 'Rongquan Bai', 'Zhuoran Yang', 'Chuan Yu', 'Jian Xu', 'Bo Zheng'], 'summary': \"Many online advertising platforms provide advertisers with auto-bidding\\nservices to enhance their advertising performance. However, most existing\\nauto-bidding algorithms fail to accurately capture the auto-bidding problem\\nformulation that the platform truly faces, let alone solve it. Actually, we\\nargue that the platform should try to help optimize each advertiser's\\nperformance to the greatest extent -- which makes $\\\\epsilon$-Nash Equilibrium\\n($\\\\epsilon$-NE) a necessary solution concept -- while maximizing the social\\nwelfare of all the advertisers for the platform's long-term value. Based on\\nthis, we introduce the \\\\emph{Nash-Equilibrium Constrained Bidding} (NCB), a new\\nformulation of the auto-bidding problem from the platform's perspective.\\nSpecifically, it aims to maximize the social welfare of all advertisers under\\nthe $\\\\epsilon$-NE constraint. However, the NCB problem presents significant\\nchallenges due to its constrained bi-level structure and the typically large\\nnumber of advertisers involved. To address these challenges, we propose a\\n\\\\emph{Bi-level Policy Gradient} (BPG) framework with theoretical guarantees.\\nNotably, its computational complexity is independent of the number of\\nadvertisers, and the associated gradients are straightforward to compute.\\nExtensive simulated and real-world experiments validate the effectiveness of\\nthe BPG framework.\", 'published': datetime.datetime(2025, 3, 13, 12, 25, 36, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10304v1'}, {'title': \"Bilingual Dual-Head Deep Model for Parkinson's Disease Detection from Speech\", 'authors': ['Moreno La Quatra', 'Juan Rafael Orozco-Arroyave', 'Marco Sabato Siniscalchi'], 'summary': \"This work aims to tackle the Parkinson's disease (PD) detection problem from\\nthe speech signal in a bilingual setting by proposing an ad-hoc dual-head deep\\nneural architecture for type-based binary classification. One head is\\nspecialized for diadochokinetic patterns. The other head looks for natural\\nspeech patterns present in continuous spoken utterances. Only one of the two\\nheads is operative accordingly to the nature of the input. Speech\\nrepresentations are extracted from self-supervised learning (SSL) models and\\nwavelet transforms. Adaptive layers, convolutional bottlenecks, and contrastive\\nlearning are exploited to reduce variations across languages. Our solution is\\nassessed against two distinct datasets, EWA-DB, and PC-GITA, which cover Slovak\\nand Spanish languages, respectively. Results indicate that conventional models\\ntrained on a single language dataset struggle with cross-linguistic\\ngeneralization, and naive combinations of datasets are suboptimal. In contrast,\\nour model improves generalization on both languages, simultaneously.\", 'published': datetime.datetime(2025, 3, 13, 12, 23, 11, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10301v1'}, {'title': 'CODEI: Resource-Efficient Task-Driven Co-Design of Perception and Decision Making for Mobile Robots Applied to Autonomous Vehicles', 'authors': ['Dejan Milojevic', 'Gioele Zardini', 'Miriam Elser', 'Andrea Censi', 'Emilio Frazzoli'], 'summary': 'This paper discusses the integration challenges and strategies for designing\\nmobile robots, by focusing on the task-driven, optimal selection of hardware\\nand software to balance safety, efficiency, and minimal usage of resources such\\nas costs, energy, computational requirements, and weight. We emphasize the\\ninterplay between perception and motion planning in decision-making by\\nintroducing the concept of occupancy queries to quantify the perception\\nrequirements for sampling-based motion planners. Sensor and algorithm\\nperformance are evaluated using False Negative Rates (FPR) and False Positive\\nRates (FPR) across various factors such as geometric relationships, object\\nproperties, sensor resolution, and environmental conditions. By integrating\\nperception requirements with perception performance, an Integer Linear\\nProgramming (ILP) approach is proposed for efficient sensor and algorithm\\nselection and placement. This forms the basis for a co-design optimization that\\nincludes the robot body, motion planner, perception pipeline, and computing\\nunit. We refer to this framework for solving the co-design problem of mobile\\nrobots as CODEI, short for Co-design of Embodied Intelligence. A case study on\\ndeveloping an Autonomous Vehicle (AV) for urban scenarios provides actionable\\ninformation for designers, and shows that complex tasks escalate resource\\ndemands, with task performance affecting choices of the autonomy stack. The\\nstudy demonstrates that resource prioritization influences sensor choice:\\ncameras are preferred for cost-effective and lightweight designs, while lidar\\nsensors are chosen for better energy and computational efficiency.', 'published': datetime.datetime(2025, 3, 13, 12, 12, 44, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10296v1'}, {'title': 'PyGDA: A Python Library for Graph Domain Adaptation', 'authors': ['Zhen Zhang', 'Meihan Liu', 'Bingsheng He'], 'summary': 'Graph domain adaptation has emerged as a promising approach to facilitate\\nknowledge transfer across different domains. Recently, numerous models have\\nbeen proposed to enhance their generalization capabilities in this field.\\nHowever, there is still no unified library that brings together existing\\ntechniques and simplifies their implementation. To fill this gap, we introduce\\nPyGDA, an open-source Python library tailored for graph domain adaptation. As\\nthe first comprehensive library in this area, PyGDA covers more than 20 widely\\nused graph domain adaptation methods together with different types of graph\\ndatasets. Specifically, PyGDA offers modular components, enabling users to\\nseamlessly build custom models with a variety of commonly used utility\\nfunctions. To handle large-scale graphs, PyGDA includes support for features\\nsuch as sampling and mini-batch processing, ensuring efficient computation. In\\naddition, PyGDA also includes comprehensive performance benchmarks and\\nwell-documented user-friendly API for both researchers and practitioners. To\\nfoster convenient accessibility, PyGDA is released under the MIT license at\\nhttps://github.com/pygda-team/pygda, and the API documentation is\\nhttps://pygda.readthedocs.io/en/stable/.', 'published': datetime.datetime(2025, 3, 13, 11, 52, 23, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10284v1'}, {'title': 'SurgRAW: Multi-Agent Workflow with Chain-of-Thought Reasoning for Surgical Intelligence', 'authors': ['Chang Han Low', 'Ziyue Wang', 'Tianyi Zhang', 'Zhitao Zeng', 'Zhu Zhuo', 'Evangelos B. Mazomenos', 'Yueming Jin'], 'summary': 'Integration of Vision-Language Models (VLMs) in surgical intelligence is\\nhindered by hallucinations, domain knowledge gaps, and limited understanding of\\ntask interdependencies within surgical scenes, undermining clinical\\nreliability. While recent VLMs demonstrate strong general reasoning and\\nthinking capabilities, they still lack the domain expertise and task-awareness\\nrequired for precise surgical scene interpretation. Although Chain-of-Thought\\n(CoT) can structure reasoning more effectively, current approaches rely on\\nself-generated CoT steps, which often exacerbate inherent domain gaps and\\nhallucinations. To overcome this, we present SurgRAW, a CoT-driven multi-agent\\nframework that delivers transparent, interpretable insights for most tasks in\\nrobotic-assisted surgery. By employing specialized CoT prompts across five\\ntasks: instrument recognition, action recognition, action prediction, patient\\ndata extraction, and outcome assessment, SurgRAW mitigates hallucinations\\nthrough structured, domain-aware reasoning. Retrieval-Augmented Generation\\n(RAG) is also integrated to external medical knowledge to bridge domain gaps\\nand improve response reliability. Most importantly, a hierarchical agentic\\nsystem ensures that CoT-embedded VLM agents collaborate effectively while\\nunderstanding task interdependencies, with a panel discussion mechanism\\npromotes logical consistency. To evaluate our method, we introduce\\nSurgCoTBench, the first reasoning-based dataset with structured frame-level\\nannotations. With comprehensive experiments, we demonstrate the effectiveness\\nof proposed SurgRAW with 29.32% accuracy improvement over baseline VLMs on 12\\nrobotic procedures, achieving the state-of-the-art performance and advancing\\nexplainable, trustworthy, and autonomous surgical assistance.', 'published': datetime.datetime(2025, 3, 13, 11, 23, 13, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10265v1'}, {'title': 'PIMRL: Physics-Informed Multi-Scale Recurrent Learning for Spatiotemporal Prediction', 'authors': ['Han Wan', 'Qi Wang', 'Hao Sun'], 'summary': 'Simulation of spatiotemporal systems governed by partial differential\\nequations is widely applied in fields such as biology, chemistry, aerospace\\ndynamics, and meteorology. Traditional numerical methods incur high\\ncomputational costs due to the requirement of small time steps for accurate\\npredictions. While machine learning has reduced these costs, long-term\\npredictions remain challenged by error accumulation, particularly in scenarios\\nwith insufficient data or varying time scales, where stability and accuracy are\\ncompromised. Existing methods often neglect the effective utilization of\\nmulti-scale data, leading to suboptimal robustness in predictions. To address\\nthese issues, we propose a novel multi-scale learning framework, namely, the\\nPhysics-Informed Multi-Scale Recurrent Learning (PIMRL), to effectively\\nleverage multi-scale data for spatiotemporal dynamics prediction. The PIMRL\\nframework comprises two modules: the micro-scale module embeds physical\\nknowledge into neural networks via pretraining, and the macro-scale module\\nadopts a data-driven approach to learn the temporal evolution of physics in the\\nlatent space. Experimental results demonstrate that the PIMRL framework\\nconsistently achieves state-of-the-art performance across five benchmark\\ndatasets ranging from one to three dimensions, showing average improvements of\\nover 9\\\\% in both RMSE and MAE evaluation metrics, with maximum enhancements\\nreaching up to 80%.', 'published': datetime.datetime(2025, 3, 13, 11, 1, 3, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10253v1'}, {'title': 'LLM Agents Display Human Biases but Exhibit Distinct Learning Patterns', 'authors': ['Idan Horowitz', 'Ori Plonsky'], 'summary': 'We investigate the choice patterns of Large Language Models (LLMs) in the\\ncontext of Decisions from Experience tasks that involve repeated choice and\\nlearning from feedback, and compare their behavior to human participants. We\\nfind that on the aggregate, LLMs appear to display behavioral biases similar to\\nhumans: both exhibit underweighting rare events and correlation effects.\\nHowever, more nuanced analyses of the choice patterns reveal that this happens\\nfor very different reasons. LLMs exhibit strong recency biases, unlike humans,\\nwho appear to respond in more sophisticated ways. While these different\\nprocesses may lead to similar behavior on average, choice patterns contingent\\non recent events differ vastly between the two groups. Specifically, phenomena\\nsuch as ``surprise triggers change\" and the ``wavy recency effect of rare\\nevents\" are robustly observed in humans, but entirely absent in LLMs. Our\\nfindings provide insights into the limitations of using LLMs to simulate and\\npredict humans in learning environments and highlight the need for refined\\nanalyses of their behavior when investigating whether they replicate human\\ndecision making tendencies.', 'published': datetime.datetime(2025, 3, 13, 10, 47, 3, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10248v1'}, {'title': 'MinorBench: A hand-built benchmark for content-based risks for children', 'authors': ['Shaun Khoo', 'Gabriel Chua', 'Rachel Shong'], 'summary': \"Large Language Models (LLMs) are rapidly entering children's lives - through\\nparent-driven adoption, schools, and peer networks - yet current AI ethics and\\nsafety research do not adequately address content-related risks specific to\\nminors. In this paper, we highlight these gaps with a real-world case study of\\nan LLM-based chatbot deployed in a middle school setting, revealing how\\nstudents used and sometimes misused the system. Building on these findings, we\\npropose a new taxonomy of content-based risks for minors and introduce\\nMinorBench, an open-source benchmark designed to evaluate LLMs on their ability\\nto refuse unsafe or inappropriate queries from children. We evaluate six\\nprominent LLMs under different system prompts, demonstrating substantial\\nvariability in their child-safety compliance. Our results inform practical\\nsteps for more robust, child-focused safety mechanisms and underscore the\\nurgency of tailoring AI systems to safeguard young users.\", 'published': datetime.datetime(2025, 3, 13, 10, 34, 43, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10242v1'}, {'title': 'Efficient Federated Fine-Tuning of Large Language Models with Layer Dropout', 'authors': ['Shilong Wang', 'Jianchun Liu', 'Hongli Xu', 'Jiaming Yan', 'Xianjun Gao'], 'summary': 'Fine-tuning plays a crucial role in enabling pre-trained LLMs to evolve from\\ngeneral language comprehension to task-specific expertise. To preserve user\\ndata privacy, federated fine-tuning is often employed and has emerged as the de\\nfacto paradigm. However, federated fine-tuning is prohibitively inefficient due\\nto the tension between LLM complexity and the resource constraint of end\\ndevices, incurring unaffordable fine-tuning overhead. Existing literature\\nprimarily utilizes parameter-efficient fine-tuning techniques to mitigate\\ncommunication costs, yet computational and memory burdens continue to pose\\nsignificant challenges for developers. This work proposes DropPEFT, an\\ninnovative federated PEFT framework that employs a novel stochastic transformer\\nlayer dropout method, enabling devices to deactivate a considerable fraction of\\nLLMs layers during training, thereby eliminating the associated computational\\nload and memory footprint. In DropPEFT, a key challenge is the proper\\nconfiguration of dropout ratios for layers, as overhead and training\\nperformance are highly sensitive to this setting. To address this challenge, we\\nadaptively assign optimal dropout-ratio configurations to devices through an\\nexploration-exploitation strategy, achieving efficient and effective\\nfine-tuning. Extensive experiments show that DropPEFT can achieve a\\n1.3-6.3\\\\times speedup in model convergence and a 40%-67% reduction in memory\\nfootprint compared to state-of-the-art methods.', 'published': datetime.datetime(2025, 3, 13, 9, 59, 16, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10217v1'}, {'title': 'Adaptive Preference Aggregation', 'authors': ['Benjamin Heymann'], 'summary': \"AI alignment, the challenge of ensuring AI systems act in accordance with\\nhuman values, has emerged as a critical problem in the development of systems\\nsuch as foundation models and recommender systems. Still, the current dominant\\napproach, reinforcement learning with human feedback (RLHF) faces known\\ntheoretical limitations in aggregating diverse human preferences. Social choice\\ntheory provides a framework to aggregate preferences, but was not developed for\\nthe multidimensional applications typical of AI. Leveraging insights from a\\nrecently published urn process, this work introduces a preference aggregation\\nstrategy that adapts to the user's context and that inherits the good\\nproperties of the maximal lottery, a Condorcet-consistent solution concept.\", 'published': datetime.datetime(2025, 3, 13, 9, 57, 41, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10215v1'}, {'title': 'Deep Learning for Time Series Forecasting: A Survey', 'authors': ['Xiangjie Kong', 'Zhenghao Chen', 'Weiyao Liu', 'Kaili Ning', 'Lechao Zhang', 'Syauqie Muhammad Marier', 'Yichen Liu', 'Yuhao Chen', 'Feng Xia'], 'summary': 'Time series forecasting (TSF) has long been a crucial task in both industry\\nand daily life. Most classical statistical models may have certain limitations\\nwhen applied to practical scenarios in fields such as energy, healthcare,\\ntraffic, meteorology, and economics, especially when high accuracy is required.\\nWith the continuous development of deep learning, numerous new models have\\nemerged in the field of time series forecasting in recent years. However,\\nexisting surveys have not provided a unified summary of the wide range of model\\narchitectures in this field, nor have they given detailed summaries of works in\\nfeature extraction and datasets. To address this gap, in this review, we\\ncomprehensively study the previous works and summarize the general paradigms of\\nDeep Time Series Forecasting (DTSF) in terms of model architectures. Besides,\\nwe take an innovative approach by focusing on the composition of time series\\nand systematically explain important feature extraction methods. Additionally,\\nwe provide an overall compilation of datasets from various domains in existing\\nworks. Finally, we systematically emphasize the significant challenges faced\\nand future research directions in this field.', 'published': datetime.datetime(2025, 3, 13, 9, 32, 1, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10198v1'}, {'title': 'Predicting Chemical Reaction Outcomes Based on Electron Movements Using Machine Learning', 'authors': ['Shuan Chen', 'Kye Sung Park', 'Taewan Kim', 'Sunkyu Han', 'Yousung Jung'], 'summary': 'Accurately predicting chemical reaction outcomes and potential byproducts is\\na fundamental task of modern chemistry, enabling the efficient design of\\nsynthetic pathways and driving progress in chemical science. Reaction\\nmechanism, which tracks electron movements during chemical reactions, is\\ncritical for understanding reaction kinetics and identifying unexpected\\nproducts. Here, we present Reactron, the first electron-based machine learning\\nmodel for general reaction prediction. Reactron integrates electron movement\\ninto its predictions, generating detailed arrow-pushing diagrams that elucidate\\neach mechanistic step leading to product formation. We demonstrate the high\\npredictive performance of Reactron over existing product-only models by a\\nlarge-scale reaction outcome prediction benchmark, and the adaptability of the\\nmodel to learn new reactivity upon providing a few examples. Furthermore, it\\nexplores combinatorial reaction spaces, uncovering novel reactivities beyond\\nits training data. With robust performance in both in- and out-of-distribution\\npredictions, Reactron embodies human-like reasoning in chemistry and opens new\\nfrontiers in reaction discovery and synthesis design.', 'published': datetime.datetime(2025, 3, 13, 9, 31, 51, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10197v1'}, {'title': 'Robustness Tokens: Towards Adversarial Robustness of Transformers', 'authors': ['Brian Pulfer', 'Yury Belousov', 'Slava Voloshynovskiy'], 'summary': 'Recently, large pre-trained foundation models have become widely adopted by\\nmachine learning practitioners for a multitude of tasks. Given that such models\\nare publicly available, relying on their use as backbone models for downstream\\ntasks might result in high vulnerability to adversarial attacks crafted with\\nthe same public model. In this work, we propose Robustness Tokens, a novel\\napproach specific to the transformer architecture that fine-tunes a few\\nadditional private tokens with low computational requirements instead of tuning\\nmodel parameters as done in traditional adversarial training. We show that\\nRobustness Tokens make Vision Transformer models significantly more robust to\\nwhite-box adversarial attacks while also retaining the original downstream\\nperformances.', 'published': datetime.datetime(2025, 3, 13, 9, 26, 19, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10191v1'}, {'title': 'Multi-Agent Q-Learning Dynamics in Random Networks: Convergence due to Exploration and Sparsity', 'authors': ['Aamal Hussain', 'Dan Leonte', 'Francesco Belardinelli', 'Raphael Huser', 'Dario Paccagnan'], 'summary': \"Beyond specific settings, many multi-agent learning algorithms fail to\\nconverge to an equilibrium solution, and instead display complex,\\nnon-stationary behaviours such as recurrent or chaotic orbits. In fact, recent\\nliterature suggests that such complex behaviours are likely to occur when the\\nnumber of agents increases. In this paper, we study Q-learning dynamics in\\nnetwork polymatrix games where the network structure is drawn from classical\\nrandom graph models. In particular, we focus on the Erdos-Renyi model, a\\nwell-studied model for social networks, and the Stochastic Block model, which\\ngeneralizes the above by accounting for community structures within the\\nnetwork. In each setting, we establish sufficient conditions under which the\\nagents' joint strategies converge to a unique equilibrium. We investigate how\\nthis condition depends on the exploration rates, payoff matrices and,\\ncrucially, the sparsity of the network. Finally, we validate our theoretical\\nfindings through numerical simulations and demonstrate that convergence can be\\nreliably achieved in many-agent systems, provided network sparsity is\\ncontrolled.\", 'published': datetime.datetime(2025, 3, 13, 9, 16, 51, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10186v1'}, {'title': 'Through the Magnifying Glass: Adaptive Perception Magnification for Hallucination-Free VLM Decoding', 'authors': ['Shunqi Mao', 'Chaoyi Zhang', 'Weidong Cai'], 'summary': 'Existing vision-language models (VLMs) often suffer from visual\\nhallucination, where the generated responses contain inaccuracies that are not\\ngrounded in the visual input. Efforts to address this issue without model\\nfinetuning primarily mitigate hallucination by reducing biases contrastively or\\namplifying the weights of visual embedding during decoding. However, these\\napproaches improve visual perception at the cost of impairing the language\\nreasoning capability. In this work, we propose the Perception Magnifier (PM), a\\nnovel visual decoding method that iteratively isolates relevant visual tokens\\nbased on attention and magnifies the corresponding regions, spurring the model\\nto concentrate on fine-grained visual details during decoding. Specifically, by\\nmagnifying critical regions while preserving the structural and contextual\\ninformation at each decoding step, PM allows the VLM to enhance its scrutiny of\\nthe visual input, hence producing more accurate and faithful responses.\\nExtensive experimental results demonstrate that PM not only achieves superior\\nhallucination mitigation but also enhances language generation while preserving\\nstrong reasoning capabilities.Code is available at\\nhttps://github.com/ShunqiM/PM .', 'published': datetime.datetime(2025, 3, 13, 9, 14, 11, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10183v1'}, {'title': 'ImageScope: Unifying Language-Guided Image Retrieval via Large Multimodal Model Collective Reasoning', 'authors': ['Pengfei Luo', 'Jingbo Zhou', 'Tong Xu', 'Yuan Xia', 'Linli Xu', 'Enhong Chen'], 'summary': 'With the proliferation of images in online content, language-guided image\\nretrieval (LGIR) has emerged as a research hotspot over the past decade,\\nencompassing a variety of subtasks with diverse input forms. While the\\ndevelopment of large multimodal models (LMMs) has significantly facilitated\\nthese tasks, existing approaches often address them in isolation, requiring the\\nconstruction of separate systems for each task. This not only increases system\\ncomplexity and maintenance costs, but also exacerbates challenges stemming from\\nlanguage ambiguity and complex image content, making it difficult for retrieval\\nsystems to provide accurate and reliable results. To this end, we propose\\nImageScope, a training-free, three-stage framework that leverages collective\\nreasoning to unify LGIR tasks. The key insight behind the unification lies in\\nthe compositional nature of language, which transforms diverse LGIR tasks into\\na generalized text-to-image retrieval process, along with the reasoning of LMMs\\nserving as a universal verification to refine the results. To be specific, in\\nthe first stage, we improve the robustness of the framework by synthesizing\\nsearch intents across varying levels of semantic granularity using\\nchain-of-thought (CoT) reasoning. In the second and third stages, we then\\nreflect on retrieval results by verifying predicate propositions locally, and\\nperforming pairwise evaluations globally. Experiments conducted on six LGIR\\ndatasets demonstrate that ImageScope outperforms competitive baselines.\\nComprehensive evaluations and ablation studies further confirm the\\neffectiveness of our design.', 'published': datetime.datetime(2025, 3, 13, 8, 43, 24, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10166v1'}, {'title': 'Retrieval-Augmented Generation with Hierarchical Knowledge', 'authors': ['Haoyu Huang', 'Yongfeng Huang', 'Junjie Yang', 'Zhenyu Pan', 'Yongqiang Chen', 'Kaili Ma', 'Hongzhi Chen', 'James Cheng'], 'summary': 'Graph-based Retrieval-Augmented Generation (RAG) methods have significantly\\nenhanced the performance of large language models (LLMs) in domain-specific\\ntasks. However, existing RAG methods do not adequately utilize the naturally\\ninherent hierarchical knowledge in human cognition, which limits the\\ncapabilities of RAG systems. In this paper, we introduce a new RAG approach,\\ncalled HiRAG, which utilizes hierarchical knowledge to enhance the semantic\\nunderstanding and structure capturing capabilities of RAG systems in the\\nindexing and retrieval processes. Our extensive experiments demonstrate that\\nHiRAG achieves significant performance improvements over the state-of-the-art\\nbaseline methods. The code of our proposed method is available at\\n\\\\href{https://github.com/hhy-huang/HiRAG}{https://github.com/hhy-huang/HiRAG}.', 'published': datetime.datetime(2025, 3, 13, 8, 22, 31, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10150v1'}, {'title': 'Multiplicative Learning', 'authors': ['Han Kim', 'Hyungjoon Soh', 'Vipul Periwal', 'Junghyo Jo'], 'summary': 'Efficient training of artificial neural networks remains a key challenge in\\ndeep learning. Backpropagation (BP), the standard learning algorithm, relies on\\ngradient descent and typically requires numerous iterations for convergence. In\\nthis study, we introduce Expectation Reflection (ER), a novel learning approach\\nthat updates weights multiplicatively based on the ratio of observed to\\npredicted outputs. Unlike traditional methods, ER maintains consistency without\\nrequiring ad hoc loss functions or learning rate hyperparameters. We extend ER\\nto multilayer networks and demonstrate its effectiveness in performing image\\nclassification tasks. Notably, ER achieves optimal weight updates in a single\\niteration. Additionally, we reinterpret ER as a modified form of gradient\\ndescent incorporating the inverse mapping of target propagation. These findings\\nsuggest that ER provides an efficient and scalable alternative for training\\nneural networks.', 'published': datetime.datetime(2025, 3, 13, 8, 14, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10144v1'}, {'title': 'Gumiho: A Hybrid Architecture to Prioritize Early Tokens in Speculative Decoding', 'authors': ['Jinze Li', 'Yixing Xu', 'Haiduo Huang', 'Xuanwu Yin', 'Dong Li', 'Edith C. H. Ngai', 'Emad Barsoum'], 'summary': 'Speculative decoding (SPD) aims to accelerate the auto-regressive token\\ngeneration process of a target Large Language Model (LLM). Some approaches\\nemploy a draft model with multiple heads to predict a sequence of future\\ntokens, where each head handles a token in the sequence. The target LLM\\nverifies the predicted sequence and accepts aligned tokens, enabling efficient\\nmulti-token generation. However, existing methods assume that all tokens within\\na sequence are equally important, employing identical head structures and\\nrelying on a single-generation paradigm, either serial or parallel. To this\\nend, we theoretically demonstrate that initial tokens in the draft sequence are\\nmore important than later ones. Building on this insight, we propose Gumiho, a\\nhybrid model combining serial and parallel heads. Specifically, given the\\ncritical importance of early tokens, we employ a sophisticated Transformer\\narchitecture for the early draft heads in a serial configuration to improve\\naccuracy. For later tokens, we utilize multiple lightweight MLP heads operating\\nin parallel to enhance efficiency. By allocating more advanced model structures\\nand longer running times to the early heads, Gumiho achieves improved overall\\nperformance. The experimental results demonstrate that our method outperforms\\nexisting approaches, fully validating its effectiveness.', 'published': datetime.datetime(2025, 3, 13, 7, 55, 38, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10135v1'}, {'title': 'Deep Learning-Based Direct Leaf Area Estimation using Two RGBD Datasets for Model Development', 'authors': ['Namal Jayasuriya', 'Yi Guo', 'Wen Hu', 'Oula Ghannoum'], 'summary': 'Estimation of a single leaf area can be a measure of crop growth and a\\nphenotypic trait to breed new varieties. It has also been used to measure leaf\\narea index and total leaf area. Some studies have used hand-held cameras, image\\nprocessing 3D reconstruction and unsupervised learning-based methods to\\nestimate the leaf area in plant images. Deep learning works well for object\\ndetection and segmentation tasks; however, direct area estimation of objects\\nhas not been explored. This work investigates deep learning-based leaf area\\nestimation, for RGBD images taken using a mobile camera setup in real-world\\nscenarios. A dataset for attached leaves captured with a top angle view and a\\ndataset for detached single leaves were collected for model development and\\ntesting. First, image processing-based area estimation was tested on manually\\nsegmented leaves. Then a Mask R-CNN-based model was investigated, and modified\\nto accept RGBD images and to estimate the leaf area. The detached-leaf data set\\nwas then mixed with the attached-leaf plant data set to estimate the single\\nleaf area for plant images, and another network design with two backbones was\\nproposed: one for segmentation and the other for area estimation. Instead of\\ntrying all possibilities or random values, an agile approach was used in\\nhyperparameter tuning. The final model was cross-validated with 5-folds and\\ntested with two unseen datasets: detached and attached leaves. The F1 score\\nwith 90% IoA for segmentation result on unseen detached-leaf data was 1.0,\\nwhile R-squared of area estimation was 0.81. For unseen plant data\\nsegmentation, the F1 score with 90% IoA was 0.59, while the R-squared score was\\n0.57. The research suggests using attached leaves with ground truth area to\\nimprove the results.', 'published': datetime.datetime(2025, 3, 13, 7, 39, 9, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10129v1'}, {'title': 'StepMathAgent: A Step-Wise Agent for Evaluating Mathematical Processes through Tree-of-Error', 'authors': ['Shu-Xun Yang', 'Cunxiang Wang', 'Yidong Wang', 'Xiaotao Gu', 'Minlie Huang', 'Jie Tang'], 'summary': 'Evaluating mathematical capabilities is critical for assessing the overall\\nperformance of large language models (LLMs). However, existing evaluation\\nmethods often focus solely on final answers, resulting in highly inaccurate and\\nuninterpretable evaluation outcomes, as well as their failure to assess proof\\nor open-ended problems. To address these issues, we propose a novel\\nmathematical process evaluation agent based on Tree-of-Error, called\\nStepMathAgent. This agent incorporates four internal core operations: logical\\nstep segmentation, step scoring, score aggregation and error tree generation,\\nalong with four external extension modules: difficulty calibration, simplicity\\nevaluation, completeness validation and format assessment. Furthermore, we\\nintroduce StepMathBench, a benchmark comprising 1,000 step-divided process\\nevaluation instances, derived from 200 high-quality math problems grouped by\\nproblem type, subject category and difficulty level. Experiments on\\nStepMathBench show that our proposed StepMathAgent outperforms all\\nstate-of-the-art methods, demonstrating human-aligned evaluation preferences\\nand broad applicability to various scenarios. Our data and code are available\\nat https://github.com/SHU-XUN/StepMathAgent.', 'published': datetime.datetime(2025, 3, 13, 7, 2, 53, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10105v1'}, {'title': 'Cognitive-Mental-LLM: Leveraging Reasoning in Large Language Models for Mental Health Prediction via Online Text', 'authors': ['Avinash Patil', 'Amardeep Kour Gedhu'], 'summary': 'Large Language Models (LLMs) have demonstrated potential in predicting mental\\nhealth outcomes from online text, yet traditional classification methods often\\nlack interpretability and robustness. This study evaluates structured reasoning\\ntechniques-Chain-of-Thought (CoT), Self-Consistency (SC-CoT), and\\nTree-of-Thought (ToT)-to improve classification accuracy across multiple mental\\nhealth datasets sourced from Reddit. We analyze reasoning-driven prompting\\nstrategies, including Zero-shot CoT and Few-shot CoT, using key performance\\nmetrics such as Balanced Accuracy, F1 score, and Sensitivity/Specificity. Our\\nfindings indicate that reasoning-enhanced techniques improve classification\\nperformance over direct prediction, particularly in complex cases. Compared to\\nbaselines such as Zero Shot non-CoT Prompting, and fine-tuned pre-trained\\ntransformers such as BERT and Mental-RoBerta, and fine-tuned Open Source LLMs\\nsuch as Mental Alpaca and Mental-Flan-T5, reasoning-driven LLMs yield notable\\ngains on datasets like Dreaddit (+0.52\\\\% over M-LLM, +0.82\\\\% over BERT) and\\nSDCNL (+4.67\\\\% over M-LLM, +2.17\\\\% over BERT). However, performance declines in\\nDepression Severity, and CSSRS predictions suggest dataset-specific\\nlimitations, likely due to our using a more extensive test set. Among prompting\\nstrategies, Few-shot CoT consistently outperforms others, reinforcing the\\neffectiveness of reasoning-driven LLMs. Nonetheless, dataset variability\\nhighlights challenges in model reliability and interpretability. This study\\nprovides a comprehensive benchmark of reasoning-based LLM techniques for mental\\nhealth text classification. It offers insights into their potential for\\nscalable clinical applications while identifying key challenges for future\\nimprovements.', 'published': datetime.datetime(2025, 3, 13, 6, 42, 37, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10095v1'}, {'title': 'Semantic Synergy: Unlocking Policy Insights and Learning Pathways Through Advanced Skill Mapping', 'authors': ['Phoebe Koundouri', 'Conrad Landis', 'Georgios Feretzakis'], 'summary': 'This research introduces a comprehensive system based on state-of-the-art\\nnatural language processing, semantic embedding, and efficient search\\ntechniques for retrieving similarities and thus generating actionable insights\\nfrom raw textual information. The system automatically extracts and aggregates\\nnormalized competencies from multiple documents (such as policy files and\\ncurricula vitae) and creates strong relationships between recognized\\ncompetencies, occupation profiles, and related learning courses. To validate\\nits performance, we conducted a multi-tier evaluation that included both\\nexplicit and implicit skill references in synthetic and real-world documents.\\nThe results showed near-human-level accuracy, with F1 scores exceeding 0.95 for\\nexplicit skill detection and above 0.93 for implicit mentions. The system\\nthereby establishes a sound foundation for supporting in-depth collaboration\\nacross the AE4RIA network. The methodology involves a multi-stage pipeline\\nbased on extensive preprocessing and data cleaning, semantic embedding and\\nsegmentation via SentenceTransformer, and skill extraction using a FAISS-based\\nsearch method. The extracted skills are associated with occupation frameworks\\n(as formulated in the ESCO ontology) and with learning paths offered through\\nthe Sustainable Development Goals Academy. Moreover, interactive visualization\\nsoftware, implemented with Dash and Plotly, presents graphs and tables for\\nreal-time exploration and informed decision-making by those involved in\\npolicymaking, training and learning supply, career transitions, and\\nrecruitment. Overall, this system, backed by rigorous validation, offers\\npromising prospects for improved policymaking, human resource development, and\\nlifelong learning by providing structured and actionable insights from raw,\\ncomplex textual information.', 'published': datetime.datetime(2025, 3, 13, 6, 41, 26, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10094v1'}, {'title': 'Parallelizing Multi-objective A* Search', 'authors': ['Saman Ahmadi', 'Nathan R. Sturtevant', 'Andrea Raith', 'Daniel Harabor', 'Mahdi Jalili'], 'summary': \"The Multi-objective Shortest Path (MOSP) problem is a classic network\\noptimization problem that aims to find all Pareto-optimal paths between two\\npoints in a graph with multiple edge costs. Recent studies on multi-objective\\nsearch with A* (MOA*) have demonstrated superior performance in solving\\ndifficult MOSP instances. This paper presents a novel search framework that\\nallows efficient parallelization of MOA* with different objective orders. The\\nframework incorporates a unique upper bounding strategy that helps the search\\nreduce the problem's dimensionality to one in certain cases. Experimental\\nresults demonstrate that the proposed framework can enhance the performance of\\nrecent A*-based solutions, with the speed-up proportional to the problem\\ndimension.\", 'published': datetime.datetime(2025, 3, 13, 5, 43, 49, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10075v1'}, {'title': 'Advanced Tool Learning and Selection System (ATLASS): A Closed-Loop Framework Using LLM', 'authors': ['Mohd Ariful Haque', 'Justin Williams', 'Sunzida Siddique', 'Md. Hujaifa Islam', 'Hasmot Ali', 'Kishor Datta Gupta', 'Roy George'], 'summary': 'The combination of LLM agents with external tools enables models to solve\\ncomplex tasks beyond their knowledge base. Human-designed tools are inflexible\\nand restricted to solutions within the scope of pre-existing tools created by\\nexperts. To address this problem, we propose ATLASS, an advanced tool learning\\nand selection system designed as a closed-loop framework. It enables the LLM to\\nsolve problems by dynamically generating external tools on demand. In this\\nframework, agents play a crucial role in orchestrating tool selection,\\nexecution, and refinement, ensuring adaptive problem-solving capabilities. The\\noperation of ATLASS follows three phases: The first phase, Understanding Tool\\nRequirements, involves the Agents determining whether tools are required and\\nspecifying their functionality; the second phase, Tool Retrieval/Generation,\\ninvolves the Agents retrieving or generating tools based on their availability;\\nand the third phase, Task Solving, involves combining all the component tools\\nnecessary to complete the initial task. The Tool Dataset stores the generated\\ntools, ensuring reusability and minimizing inference cost. Current LLM-based\\ntool generation systems have difficulty creating complex tools that need APIs\\nor external packages. In ATLASS, we solve the problem by automatically setting\\nup the environment, fetching relevant API documentation online, and using a\\nPython interpreter to create a reliable, versatile tool that works in a wider\\nrange of situations. OpenAI GPT-4.0 is used as the LLM agent, and safety and\\nethical concerns are handled through human feedback before executing generated\\ncode. By addressing the limitations of predefined toolsets and enhancing\\nadaptability, ATLASS serves as a real-world solution that empowers users with\\ndynamically generated tools for complex problem-solving.', 'published': datetime.datetime(2025, 3, 13, 5, 39, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10071v1'}, {'title': 'AhaRobot: A Low-Cost Open-Source Bimanual Mobile Manipulator for Embodied AI', 'authors': ['Haiqin Cui', 'Yifu Yuan', 'Yan Zheng', 'Jianye Hao'], 'summary': 'Navigation and manipulation in open-world environments remain unsolved\\nchallenges in the Embodied AI. The high cost of commercial mobile manipulation\\nrobots significantly limits research in real-world scenes. To address this\\nissue, we propose AhaRobot, a low-cost and fully open-source dual-arm mobile\\nmanipulation robot system with a hardware cost of only $1,000 (excluding\\noptional computational resources), which is less than 1/15 of the cost of\\npopular mobile robots. The AhaRobot system consists of three components: (1) a\\nnovel low-cost hardware architecture primarily composed of off-the-shelf\\ncomponents, (2) an optimized control solution to enhance operational precision\\nintegrating dual-motor backlash control and static friction compensation, and\\n(3) a simple remote teleoperation method RoboPilot. We use handles to control\\nthe dual arms and pedals for whole-body movement. The teleoperation process is\\nlow-burden and easy to operate, much like piloting. RoboPilot is designed for\\nremote data collection in embodied scenarios. Experimental results demonstrate\\nthat RoboPilot significantly enhances data collection efficiency in complex\\nmanipulation tasks, achieving a 30% increase compared to methods using 3D mouse\\nand leader-follower systems. It also excels at completing extremely\\nlong-horizon tasks in one go. Furthermore, AhaRobot can be used to learn\\nend-to-end policies and autonomously perform complex manipulation tasks, such\\nas pen insertion and cleaning up the floor. We aim to build an affordable yet\\npowerful platform to promote the development of embodied tasks on real devices,\\nadvancing more robust and reliable embodied AI. All hardware and software\\nsystems are available at https://aha-robot.github.io.', 'published': datetime.datetime(2025, 3, 13, 5, 34, 43, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10070v1'}, {'title': 'Compute Optimal Scaling of Skills: Knowledge vs Reasoning', 'authors': ['Nicholas Roberts', 'Niladri Chatterji', 'Sharan Narang', 'Mike Lewis', 'Dieuwke Hupkes'], 'summary': \"Scaling laws are a critical component of the LLM development pipeline, most\\nfamously as a way to forecast training decisions such as 'compute-optimally'\\ntrading-off parameter count and dataset size, alongside a more recent growing\\nlist of other crucial decisions. In this work, we ask whether compute-optimal\\nscaling behaviour can be skill-dependent. In particular, we examine knowledge\\nand reasoning-based skills such as knowledge-based QA and code generation, and\\nwe answer this question in the affirmative: $\\\\textbf{scaling laws are\\nskill-dependent}$. Next, to understand whether skill-dependent scaling is an\\nartefact of the pretraining datamix, we conduct an extensive ablation of\\ndifferent datamixes and find that, also when correcting for datamix\\ndifferences, $\\\\textbf{knowledge and code exhibit fundamental differences in\\nscaling behaviour}$. We conclude with an analysis of how our findings relate to\\nstandard compute-optimal scaling using a validation set, and find that\\n$\\\\textbf{a misspecified validation set can impact compute-optimal parameter\\ncount by nearly 50%,}$ depending on its skill composition.\", 'published': datetime.datetime(2025, 3, 13, 5, 21, 22, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10061v1'}, {'title': 'Deep Learning Approaches for Anti-Money Laundering on Mobile Transactions: Review, Framework, and Directions', 'authors': ['Jiani Fan', 'Lwin Khin Shar', 'Ruichen Zhang', 'Ziyao Liu', 'Wenzhuo Yang', 'Dusit Niyato', 'Bomin Mao', 'Kwok-Yan Lam'], 'summary': 'Money laundering is a financial crime that obscures the origin of illicit\\nfunds, necessitating the development and enforcement of anti-money laundering\\n(AML) policies by governments and organizations. The proliferation of mobile\\npayment platforms and smart IoT devices has significantly complicated AML\\ninvestigations. As payment networks become more interconnected, there is an\\nincreasing need for efficient real-time detection to process large volumes of\\ntransaction data on heterogeneous payment systems by different operators such\\nas digital currencies, cryptocurrencies and account-based payments. Most of\\nthese mobile payment networks are supported by connected devices, many of which\\nare considered loT devices in the FinTech space that constantly generate data.\\nFurthermore, the growing complexity and unpredictability of transaction\\npatterns across these networks contribute to a higher incidence of false\\npositives. While machine learning solutions have the potential to enhance\\ndetection efficiency, their application in AML faces unique challenges, such as\\naddressing privacy concerns tied to sensitive financial data and managing the\\nreal-world constraint of limited data availability due to data regulations.\\nExisting surveys in the AML literature broadly review machine learning\\napproaches for money laundering detection, but they often lack an in-depth\\nexploration of advanced deep learning techniques - an emerging field with\\nsignificant potential. To address this gap, this paper conducts a comprehensive\\nreview of deep learning solutions and the challenges associated with their use\\nin AML. Additionally, we propose a novel framework that applies the\\nleast-privilege principle by integrating machine learning techniques, codifying\\nAML red flags, and employing account profiling to provide context for\\npredictions and enable effective fraud detection under limited data\\navailability....', 'published': datetime.datetime(2025, 3, 13, 5, 19, 44, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10058v1'}, {'title': 'DTA: Dual Temporal-channel-wise Attention for Spiking Neural Networks', 'authors': ['Minje Kim', 'Minjun Kim', 'Xu Yang'], 'summary': 'Spiking Neural Networks (SNNs) present a more energy-efficient alternative to\\nArtificial Neural Networks (ANNs) by harnessing spatio-temporal dynamics and\\nevent-driven spikes. Effective utilization of temporal information is crucial\\nfor SNNs, leading to the exploration of attention mechanisms to enhance this\\ncapability. Conventional attention operations either apply identical operation\\nor employ non-identical operations across target dimensions. We identify that\\nthese approaches provide distinct perspectives on temporal information. To\\nleverage the strengths of both operations, we propose a novel Dual\\nTemporal-channel-wise Attention (DTA) mechanism that integrates both\\nidentical/non-identical attention strategies. To the best of our knowledge,\\nthis is the first attempt to concentrate on both the correlation and dependency\\nof temporal-channel using both identical and non-identical attention\\noperations. Experimental results demonstrate that the DTA mechanism achieves\\nstate-of-the-art performance on both static datasets (CIFAR10, CIFAR100,\\nImageNet-1k) and dynamic dataset (CIFAR10-DVS), elevating spike representation\\nand capturing complex temporal-channel relationship. We open-source our code:\\nhttps://github.com/MnJnKIM/DTA-SNN.', 'published': datetime.datetime(2025, 3, 13, 5, 9, 48, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10052v1'}, {'title': 'Rapid analysis of point-contact Andreev reflection spectra via machine learning with adaptive data augmentation', 'authors': ['Dongik Lee', 'Valentin Stanev', 'Xiaohang Zhang', 'Mijeong Kang', 'Ichiro Takeuchi', 'Seunghun Lee'], 'summary': \"Delineating the superconducting order parameters is a pivotal task in\\ninvestigating superconductivity for probing pairing mechanisms, as well as\\ntheir symmetry and topology. Point-contact Andreev reflection (PCAR)\\nmeasurement is a simple yet powerful tool for identifying the order parameters.\\nThe PCAR spectra exhibit significant variations depending on the type of the\\norder parameter in a superconductor, including its magnitude\\n($\\\\mathit{\\\\Delta}$), as well as temperature, interfacial quality, Fermi\\nvelocity mismatch, and other factors. The information on the order parameter\\ncan be obtained by finding the combination of these parameters, generating a\\ntheoretical spectrum that fits a measured experimental spectrum. However, due\\nto the complexity of the spectra and the high dimensionality of parameters,\\nextracting the fitting parameters is often time-consuming and labor-intensive.\\nIn this study, we employ a convolutional neural network (CNN) algorithm to\\ncreate models for rapid and automated analysis of PCAR spectra of various\\nsuperconductors with different pairing symmetries (conventional $s$-wave,\\nchiral $p_x+ip_y$-wave, and $d_{x^2-y^2}$-wave). The training datasets are\\ngenerated based on the Blonder-Tinkham-Klapwijk (BTK) theory and further\\nmodified and augmented by selectively incorporating noise and peaks according\\nto the bias voltages. This approach not only replicates the experimental\\nspectra but also brings the model's attention to important features within the\\nspectra. The optimized models provide fitting parameters for experimentally\\nmeasured spectra in less than 100 ms per spectrum. Our approaches and findings\\npave the way for rapid and automated spectral analysis which will help\\naccelerate research on superconductors with complex order parameters.\", 'published': datetime.datetime(2025, 3, 13, 4, 45, 38, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10040v1'}, {'title': 'OR-LLM-Agent: Automating Modeling and Solving of Operations Research Optimization Problem with Reasoning Large Language Model', 'authors': ['Bowen Zhang', 'Pengcheng Luo'], 'summary': 'Operations Research (OR) has been widely applied in various fields such as\\nresource allocation, production planning, and supply chain management. However,\\naddressing real-world OR problems requires OR experts to perform mathematical\\nmodeling and programmers to develop solution algorithms. This traditional\\nmethod, heavily reliant on experts, is costly and has long development cycles,\\nseverely limiting the widespread adoption of OR techniques. Few have considered\\nusing Artificial Intelligence (AI) to replace professionals to achieve fully\\nautomated solutions for OR problems. We propose OR-LLM-Agent, the first AI\\nagent that enables end-to-end automation for solving real-world OR problems.\\nOR-LLM-Agent leverages the Chain-of-Thought (CoT) reasoning capabilities of\\nLarge Language Models (LLMs) to translate natural language problem descriptions\\ninto formal mathematical models and automatically generate Gurobi solver code.\\nIn OR-LLM-Agent, OR-CodeAgent is designed to automate code execution and repair\\nwithin a sandbox environment, facilitating the derivation of the final\\nsolution. Due to the lack of dedicated benchmark datasets for evaluating the\\nautomated solving of OR problems, we construct a benchmark dataset comprising\\n83 real-world OR problems described in natural language. We conduct comparative\\nexperiments with state-of-the-art (SOTA) reasoning LLMs, including GPT-o3-mini,\\nDeepSeek-R1, and Gemini 2.0 Flash Thinking. The OR-LLM-Agent achieved the\\nhighest pass rate of 100% and the highest solution accuracy of 85%,\\ndemonstrating the feasibility of automated OR problem-solving. Data and code\\nhave been publicly available at https://github.com/bwz96sco/or_llm_agent.', 'published': datetime.datetime(2025, 3, 13, 3, 40, 50, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10009v1'}, {'title': 'A New Benchmark for Few-Shot Class-Incremental Learning: Redefining the Upper Bound', 'authors': ['Shiwon Kim', 'Dongjun Hwang', 'Sungwon Woo', 'Rita Singh'], 'summary': 'Class-incremental learning (CIL) aims to continuously adapt to emerging\\nclasses while retaining knowledge of previously learned ones. Few-shot\\nclass-incremental learning (FSCIL) presents an even greater challenge which\\nrequires the model to learn incremental classes with only a limited number of\\nsamples. In conventional CIL, joint training is widely considered the upper\\nbound, serving as both a benchmark and a methodological guide. However, we find\\nthat joint training fails to be a meaningful upper bound in FSCIL due to the\\ninherent difficulty of inter-task class separation (ICS) caused by severe class\\nimbalance. In this work, we introduce a new joint training benchmark tailored\\nfor FSCIL by integrating imbalance-aware techniques, effectively bridging the\\nperformance gap between base and incremental classes. Furthermore, we point out\\ninconsistencies in the experimental setup and evaluation of existing FSCIL\\nmethods. To ensure fair comparisons between different FSCIL approaches and\\njoint training, we standardize training conditions and propose a unified\\nevaluation protocol that simultaneously considers the validation set and\\ncomputational complexity. By establishing a reliable upper bound and a\\nstandardized evaluation framework for FSCIL, our work provides a clear\\nbenchmark and a practical foundation for future research.', 'published': datetime.datetime(2025, 3, 13, 3, 25, 29, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.10003v1'}, {'title': 'Label Unbalance in High-frequency Trading', 'authors': ['Zijian Zhao', 'Xuming Chen', 'Jiayu Wen', 'Mingwen Liu', 'Xiaoteng Ma'], 'summary': 'In financial trading, return prediction is one of the foundation for a\\nsuccessful trading system. By the fast development of the deep learning in\\nvarious areas such as graphical processing, natural language, it has also\\ndemonstrate significant edge in handling with financial data. While the success\\nof the deep learning relies on huge amount of labeled sample, labeling each\\ntime/event as profitable or unprofitable, under the transaction cost,\\nespecially in the high-frequency trading world, suffers from serious label\\nimbalance issue.In this paper, we adopts rigurious end-to-end deep learning\\nframework with comprehensive label imbalance adjustment methods and succeed in\\npredicting in high-frequency return in the Chinese future market. The code for\\nour method is publicly available at\\nhttps://github.com/RS2002/Label-Unbalance-in-High-Frequency-Trading .', 'published': datetime.datetime(2025, 3, 13, 2, 55, 6, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.09988v1'}, {'title': 'Uncertainty-aware Long-tailed Weights Model the Utility of Pseudo-labels for Semi-supervised Learning', 'authors': ['Jiaqi Wu', 'Junbiao Pang', 'Qingming Huang'], 'summary': \"Current Semi-supervised Learning (SSL) adopts the pseudo-labeling strategy\\nand further filters pseudo-labels based on confidence thresholds. However, this\\nmechanism has notable drawbacks: 1) setting the reasonable threshold is an open\\nproblem which significantly influences the selection of the high-quality\\npseudo-labels; and 2) deep models often exhibit the over-confidence phenomenon\\nwhich makes the confidence value an unreliable indicator for assessing the\\nquality of pseudo-labels due to the scarcity of labeled data. In this paper, we\\npropose an Uncertainty-aware Ensemble Structure (UES) to assess the utility of\\npseudo-labels for unlabeled samples. We further model the utility of\\npseudo-labels as long-tailed weights to avoid the open problem of setting the\\nthreshold. Concretely, the advantage of the long-tailed weights ensures that\\neven unreliable pseudo-labels still contribute to enhancing the model's\\nrobustness. Besides, UES is lightweight and architecture-agnostic, easily\\nextending to various computer vision tasks, including classification and\\nregression. Experimental results demonstrate that combining the proposed method\\nwith DualPose leads to a 3.47% improvement in Percentage of Correct Keypoints\\n(PCK) on the Sniffing dataset with 100 data points (30 labeled), a 7.29\\\\%\\nimprovement in PCK on the FLIC dataset with 100 data points (50 labeled), and a\\n3.91% improvement in PCK on the LSP dataset with 200 data points (100 labeled).\\nFurthermore, when combined with FixMatch, the proposed method achieves a 0.2%\\naccuracy improvement on the CIFAR-10 dataset with 40 labeled data points and a\\n0.26% accuracy improvement on the CIFAR-100 dataset with 400 labeled data\\npoints.\", 'published': datetime.datetime(2025, 3, 13, 2, 21, 4, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.09974v1'}, {'title': 'Detecting Dataset Bias in Medical AI: A Generalized and Modality-Agnostic Auditing Framework', 'authors': ['Nathan Drenkow', 'Mitchell Pavlak', 'Keith Harrigian', 'Ayah Zirikly', 'Adarsh Subbaswamy', 'Mathias Unberath'], 'summary': \"Data-driven AI is establishing itself at the center of evidence-based\\nmedicine. However, reports of shortcomings and unexpected behavior are growing\\ndue to AI's reliance on association-based learning. A major reason for this\\nbehavior: latent bias in machine learning datasets can be amplified during\\ntraining and/or hidden during testing. We present a data modality-agnostic\\nauditing framework for generating targeted hypotheses about sources of bias\\nwhich we refer to as Generalized Attribute Utility and Detectability-Induced\\nbias Testing (G-AUDIT) for datasets. Our method examines the relationship\\nbetween task-level annotations and data properties including protected\\nattributes (e.g., race, age, sex) and environment and acquisition\\ncharacteristics (e.g., clinical site, imaging protocols). G-AUDIT automatically\\nquantifies the extent to which the observed data attributes may enable shortcut\\nlearning, or in the case of testing data, hide predictions made based on\\nspurious associations. We demonstrate the broad applicability and value of our\\nmethod by analyzing large-scale medical datasets for three distinct modalities\\nand learning tasks: skin lesion classification in images, stigmatizing language\\nclassification in Electronic Health Records (EHR), and mortality prediction for\\nICU tabular data. In each setting, G-AUDIT successfully identifies subtle\\nbiases commonly overlooked by traditional qualitative methods that focus\\nprimarily on social and ethical objectives, underscoring its practical value in\\nexposing dataset-level risks and supporting the downstream development of\\nreliable AI systems. Our method paves the way for achieving deeper\\nunderstanding of machine learning datasets throughout the AI development\\nlife-cycle from initial prototyping all the way to regulation, and creates\\nopportunities to reduce model bias, enabling safer and more trustworthy AI\\nsystems.\", 'published': datetime.datetime(2025, 3, 13, 2, 16, 48, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.09969v1'}, {'title': 'Optimizing Fire Safety: Reducing False Alarms Using Advanced Machine Learning Techniques', 'authors': ['Muhammad Hassan Jamal', 'Abdulwahab Alazeb', 'Shahid Allah Bakhsh', 'Wadii Boulila', 'Syed Aziz Shah', 'Aizaz Ahmad Khattak', 'Muhammad Shahbaz Khan'], 'summary': 'Fire safety practices are important to reduce the extent of destruction\\ncaused by fire. While smoke alarms help save lives, firefighters struggle with\\nthe increasing number of false alarms. This paper presents a precise and\\nefficient Weighted ensemble model for decreasing false alarms. It estimates the\\ndensity, computes weights according to the high and low-density regions,\\nforwards the high region weights to KNN and low region weights to XGBoost and\\ncombines the predictions. The proposed model is effective at reducing response\\ntime, increasing fire safety, and minimizing the damage that fires cause. A\\nspecifically designed dataset for smoke detection is utilized to test the\\nproposed model. In addition, a variety of ML models, such as Logistic\\nRegression (LR), Decision Tree (DT), Random Forest (RF), Nai:ve Bayes (NB),\\nK-Nearest Neighbour (KNN), Support Vector Machine (SVM), Extreme Gradient\\nBoosting (XGBoost), Adaptive Boosting (ADAB), have also been utilized. To\\nmaximize the use of the smoke detection dataset, all the algorithms utilize the\\nSMOTE re-sampling technique. After evaluating the assessment criteria, this\\npaper presents a concise summary of the comprehensive findings obtained by\\ncomparing the outcomes of all models.', 'published': datetime.datetime(2025, 3, 13, 2, 7, 14, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.09960v1'}, {'title': 'Exploring Mutual Empowerment Between Wireless Networks and RL-based LLMs: A Survey', 'authors': ['Yu Qiao', 'Phuong-Nam Tran', 'Ji Su Yoon', 'Loc X. Nguyen', 'Choong Seon Hong'], 'summary': 'Reinforcement learning (RL)-based large language models (LLMs), such as\\nChatGPT, DeepSeek, and Grok-3, have gained significant attention for their\\nexceptional capabilities in natural language processing and multimodal data\\nunderstanding. Meanwhile, the rapid expansion of information services has\\ndriven the growing need for intelligence, efficient, and adaptable wireless\\nnetworks. Wireless networks require the empowerment of RL-based LLMs while\\nthese models also benefit from wireless networks to broaden their application\\nscenarios. Specifically, RL-based LLMs can enhance wireless communication\\nsystems through intelligent resource allocation, adaptive network optimization,\\nand real-time decision-making. Conversely, wireless networks provide a vital\\ninfrastructure for the efficient training, deployment, and distributed\\ninference of RL-based LLMs, especially in decentralized and edge computing\\nenvironments. This mutual empowerment highlights the need for a deeper\\nexploration of the interplay between these two domains. We first review recent\\nadvancements in wireless communications, highlighting the associated challenges\\nand potential solutions. We then discuss the progress of RL-based LLMs,\\nfocusing on key technologies for LLM training, challenges, and potential\\nsolutions. Subsequently, we explore the mutual empowerment between these two\\nfields, highlighting key motivations, open challenges, and potential solutions.\\nFinally, we provide insights into future directions, applications, and their\\nsocietal impact to further explore this intersection, paving the way for\\nnext-generation intelligent communication systems. Overall, this survey\\nprovides a comprehensive overview of the relationship between RL-based LLMs and\\nwireless networks, offering a vision where these domains empower each other to\\ndrive innovations.', 'published': datetime.datetime(2025, 3, 13, 1, 59, 11, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.09956v1'}, {'title': 'MoFlow: One-Step Flow Matching for Human Trajectory Forecasting via Implicit Maximum Likelihood Estimation based Distillation', 'authors': ['Yuxiang Fu', 'Qi Yan', 'Lele Wang', 'Ke Li', 'Renjie Liao'], 'summary': 'In this paper, we address the problem of human trajectory forecasting, which\\naims to predict the inherently multi-modal future movements of humans based on\\ntheir past trajectories and other contextual cues. We propose a novel motion\\nprediction conditional flow matching model, termed MoFlow, to predict K-shot\\nfuture trajectories for all agents in a given scene. We design a novel flow\\nmatching loss function that not only ensures at least one of the $K$ sets of\\nfuture trajectories is accurate but also encourages all $K$ sets of future\\ntrajectories to be diverse and plausible. Furthermore, by leveraging the\\nimplicit maximum likelihood estimation (IMLE), we propose a novel distillation\\nmethod for flow models that only requires samples from the teacher model.\\nExtensive experiments on the real-world datasets, including SportVU NBA games,\\nETH-UCY, and SDD, demonstrate that both our teacher flow model and the\\nIMLE-distilled student model achieve state-of-the-art performance. These models\\ncan generate diverse trajectories that are physically and socially plausible.\\nMoreover, our one-step student model is $\\\\textbf{100}$ times faster than the\\nteacher flow model during sampling. The code, model, and data are available at\\nour project page: https://moflow-imle.github.io', 'published': datetime.datetime(2025, 3, 13, 1, 53, 5, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.09950v1'}, {'title': 'Identifying Trustworthiness Challenges in Deep Learning Models for Continental-Scale Water Quality Prediction', 'authors': ['Xiaobo Xia', 'Xiaofeng Liu', 'Jiale Liu', 'Kuai Fang', 'Lu Lu', 'Samet Oymak', 'William S. Currie', 'Tongliang Liu'], 'summary': 'Water quality is foundational to environmental sustainability, ecosystem\\nresilience, and public health. Deep learning models, particularly Long\\nShort-Term Memory (LSTM) networks, offer transformative potential for\\nlarge-scale water quality prediction and scientific insights generation.\\nHowever, their widespread adoption in high-stakes decision-making, such as\\npollution mitigation and equitable resource allocation, is prevented by\\nunresolved trustworthiness challenges including fairness, uncertainty,\\ninterpretability, robustness, generalizability, and reproducibility. In this\\nwork, we present the first comprehensive evaluation of trustworthiness in a\\ncontinental-scale multi-task LSTM model predicting 20 water quality variables\\n(encompassing physical/chemical processes, geochemical weathering, and nutrient\\ncycling) across 482 U.S. basins. Our investigation uncovers systematic patterns\\nof model performance disparities linked to basin characteristics, the inherent\\ncomplexity of biogeochemical processes, and variable predictability,\\nemphasizing critical performance fairness concerns. We further propose\\nmethodological frameworks for quantitatively evaluating critical aspects of\\ntrustworthiness, including uncertainty, interpretability, and robustness,\\nidentifying key limitations that could challenge reliable real-world\\ndeployment. This work serves as a timely call to action for advancing\\ntrustworthy data-driven methods for water resources management and provides a\\npathway to offering critical insights for researchers, decision-makers, and\\npractitioners seeking to leverage artificial intelligence (AI) responsibly in\\nenvironmental management.', 'published': datetime.datetime(2025, 3, 13, 1, 50, 50, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.09947v1'}, {'title': 'TGP: Two-modal occupancy prediction with 3D Gaussian and sparse points for 3D Environment Awareness', 'authors': ['Mu Chen', 'Wenyu Chen', 'Mingchuan Yang', 'Yuan Zhang', 'Tao Han', 'Xinchi Li', 'Yunlong Li', 'Huaici Zhao'], 'summary': '3D semantic occupancy has rapidly become a research focus in the fields of\\nrobotics and autonomous driving environment perception due to its ability to\\nprovide more realistic geometric perception and its closer integration with\\ndownstream tasks. By performing occupancy prediction of the 3D space in the\\nenvironment, the ability and robustness of scene understanding can be\\neffectively improved. However, existing occupancy prediction tasks are\\nprimarily modeled using voxel or point cloud-based approaches: voxel-based\\nnetwork structures often suffer from the loss of spatial information due to the\\nvoxelization process, while point cloud-based methods, although better at\\nretaining spatial location information, face limitations in representing\\nvolumetric structural details. To address this issue, we propose a dual-modal\\nprediction method based on 3D Gaussian sets and sparse points, which balances\\nboth spatial location and volumetric structural information, achieving higher\\naccuracy in semantic occupancy prediction. Specifically, our method adopts a\\nTransformer-based architecture, taking 3D Gaussian sets, sparse points, and\\nqueries as inputs. Through the multi-layer structure of the Transformer, the\\nenhanced queries and 3D Gaussian sets jointly contribute to the semantic\\noccupancy prediction, and an adaptive fusion mechanism integrates the semantic\\noutputs of both modalities to generate the final prediction results.\\nAdditionally, to further improve accuracy, we dynamically refine the point\\ncloud at each layer, allowing for more precise location information during\\noccupancy prediction. We conducted experiments on the Occ3DnuScenes dataset,\\nand the experimental results demonstrate superior performance of the proposed\\nmethod on IoU based metrics.', 'published': datetime.datetime(2025, 3, 13, 1, 35, 4, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.09941v1'}, {'title': 'Developing and Evaluating an AI-Assisted Prediction Model for Unplanned Intensive Care Admissions following Elective Neurosurgery using Natural Language Processing within an Electronic Healthcare Record System', 'authors': ['Julia Ive', 'Olatomiwa Olukoya', 'Jonathan P. Funnell', 'James Booker', 'Sze H M Lam', 'Ugan Reddy', 'Kawsar Noor', 'Richard JB Dobson', 'Astri M. V. Luoma', 'Hani J Marcus'], 'summary': 'Introduction: Timely care in a specialised neuro-intensive therapy unit (ITU)\\nreduces mortality and hospital stays, with planned admissions being safer than\\nunplanned ones. However, post-operative care decisions remain subjective. This\\nstudy used artificial intelligence (AI), specifically natural language\\nprocessing (NLP) to analyse electronic health records (EHRs) and predict ITU\\nadmissions for elective surgery patients. Methods: This study analysed the EHRs\\nof elective neurosurgery patients from University College London Hospital\\n(UCLH) using NLP. Patients were categorised into planned high dependency unit\\n(HDU) or ITU admission; unplanned HDU or ITU admission; or ward / overnight\\nrecovery (ONR). The Medical Concept Annotation Tool (MedCAT) was used to\\nidentify SNOMED-CT concepts within the clinical notes. We then explored the\\nutility of these identified concepts for a range of AI algorithms trained to\\npredict ITU admission. Results: The CogStack-MedCAT NLP model, initially\\ntrained on hospital-wide EHRs, underwent two refinements: first with data from\\npatients with Normal Pressure Hydrocephalus (NPH) and then with data from\\nVestibular Schwannoma (VS) patients, achieving a concept detection F1-score of\\n0.93. This refined model was then used to extract concepts from EHR notes of\\n2,268 eligible neurosurgical patients. We integrated the extracted concepts\\ninto AI models, including a decision tree model and a neural time-series model.\\nUsing the simpler decision tree model, we achieved a recall of 0.87 (CI 0.82 -\\n0.91) for ITU admissions, reducing the proportion of unplanned ITU cases missed\\nby human experts from 36% to 4%. Conclusion: The NLP model, refined for\\naccuracy, has proven its efficiency in extracting relevant concepts, providing\\na reliable basis for predictive AI models to use in clinically valid\\napplications.', 'published': datetime.datetime(2025, 3, 13, 0, 48, 48, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.09927v1'}, {'title': 'eXpLogic: Explaining Logic Types and Patterns in DiffLogic Networks', 'authors': ['Stephen Wormald', 'David Koblah', 'Matheus Kunzler Maldaner', 'Domenic Forte', 'Damon L. Woodard'], 'summary': \"Constraining deep neural networks (DNNs) to learn individual logic types per\\nnode, as performed using the DiffLogic network architecture, opens the door to\\nmodel-specific explanation techniques that quell the complexity inherent to\\nDNNs. Inspired by principles of circuit analysis from computer engineering,\\nthis work presents an algorithm (eXpLogic) for producing saliency maps which\\nexplain input patterns that activate certain functions. The eXpLogic\\nexplanations: (1) show the exact set of inputs responsible for a decision,\\nwhich helps interpret false negative and false positive predictions, (2)\\nhighlight common input patterns that activate certain outputs, and (3) help\\nreduce the network size to improve class-specific inference. To evaluate the\\neXpLogic saliency map, we introduce a metric that quantifies how much an input\\nchanges before switching a model's class prediction (the SwitchDist) and use\\nthis metric to compare eXpLogic against the Vanilla Gradients (VG) and\\nIntegrated Gradient (IG) methods. Generally, we show that eXpLogic saliency\\nmaps are better at predicting which inputs will change the class score. These\\nmaps help reduce the network size and inference times by 87\\\\% and 8\\\\%,\\nrespectively, while having a limited impact (-3.8\\\\%) on class-specific\\npredictions. The broader value of this work to machine learning is in\\ndemonstrating how certain DNN architectures promote explainability, which is\\nrelevant to healthcare, defense, and law.\", 'published': datetime.datetime(2025, 3, 13, 0, 1, 36, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2503.09910v1'}]\u001b[00m\n",
            "\u001b[1m\u001b[94m \n",
            "[2025-03-15 00:53:09][🤖 LLM CALL STARTED]: 2025-03-15 00:53:09.995587\u001b[00m\n",
            "\u001b[1m\u001b[94m \n",
            "[2025-03-15 00:53:28][✅ LLM CALL COMPLETED]: 2025-03-15 00:53:28.560688\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSenior Researcher\u001b[00m\n",
            "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
            "1. **Title**: Studying Classifier(-Free) Guidance From a Classifier-Centric Perspective  \n",
            "   **Authors**: Xiaoming Zhao, Alexander G. Schwing  \n",
            "   **Abstract**: Classifier-free guidance has become a staple for conditional generation with denoising diffusion models. However, a comprehensive understanding of classifier-free guidance is still missing. In this work, we carry out an empirical study to provide a fresh perspective on classifier-free guidance. Concretely, instead of solely focusing on classifier-free guidance, we trace back to the root, i.e., classifier guidance, pinpoint the key assumption for the derivation, and conduct a systematic study to understand the role of the classifier. We find that both classifier guidance and classifier-free guidance achieve conditional generation by pushing the denoising diffusion trajectories away from decision boundaries, i.e., areas where conditional information is usually entangled and is hard to learn. Based on this classifier-centric understanding, we propose a generic postprocessing step built upon flow-matching to shrink the gap between the learned distribution for a pre-trained denoising diffusion model and the real data distribution, majorly around the decision boundaries. Experiments on various datasets verify the effectiveness of the proposed approach.  \n",
            "   **Link**: [arXiv:2503.10638v1](http://arxiv.org/abs/2503.10638v1)\n",
            "\n",
            "2. **Title**: A Frustratingly Simple Yet Highly Effective Attack Baseline: Over 90% Success Rate Against the Strong Black-box Models of GPT-4.5/4o/o1  \n",
            "   **Authors**: Zhaoyi Li, Xiaohan Zhao, Dong-Dong Wu, Jiacheng Cui, Zhiqiang Shen  \n",
            "   **Abstract**: Despite promising performance on open-source large vision-language models (LVLMs), transfer-based targeted attacks often fail against black-box commercial LVLMs. Analyzing failed adversarial perturbations reveals that the learned perturbations typically originate from a uniform distribution and lack clear semantic details, resulting in unintended responses. This critical absence of semantic information leads commercial LVLMs to either ignore the perturbation entirely or misinterpret its embedded semantics, thereby causing the attack to fail. To overcome these issues, we notice that identifying core semantic objects is a key objective for models trained with various datasets and methodologies. This insight motivates our approach that refines semantic clarity by encoding explicit semantic details within local regions, thus ensuring interoperability and capturing finer-grained features, and by concentrating modifications on semantically rich areas rather than applying them uniformly. To achieve this, we propose a simple yet highly effective solution...  \n",
            "   **Link**: [arXiv:2503.10635v1](http://arxiv.org/abs/2503.10635v1)\n",
            "\n",
            "3. **Title**: Uncertainty in Action: Confidence Elicitation in Embodied Agents  \n",
            "   **Authors**: Tianjiao Yu, Vedant Shah, Muntasir Wahed, Kiet A. Nguyen, Adheesh Juvekar, Tal August, Ismini Lourentzou  \n",
            "   **Abstract**: Expressing confidence is challenging for embodied agents navigating dynamic multimodal environments, where uncertainty arises from both perception and decision-making processes. We present the first work investigating embodied confidence elicitation in open-ended multimodal environments. We introduce Elicitation Policies, which structure confidence assessment across inductive, deductive, and abductive reasoning, along with Execution Policies, which enhance confidence calibration through scenario reinterpretation, action sampling, and hypothetical reasoning. Evaluating agents in calibration and failure prediction tasks within the Minecraft environment, we show that structured reasoning approaches...  \n",
            "   **Link**: [arXiv:2503.10628v1](http://arxiv.org/abs/2503.10628v1)\n",
            "\n",
            "4. **Title**: SciVerse: Unveiling the Knowledge Comprehension and Visual Reasoning of LMMs on Multi-modal Scientific Problems  \n",
            "   **Authors**: Ziyu Guo, Ray Zhang, Hao Chen, Jialin Gao, Dongzhi Jiang, Jiaze Wang, Pheng-Ann Heng  \n",
            "   **Abstract**: The rapid advancement of Large Multi-modal Models (LMMs) has enabled their application in scientific problem-solving, yet their fine-grained capabilities remain under-explored. In this paper, we introduce SciVerse, a multi-modal scientific evaluation benchmark to thoroughly assess LMMs across 5,735 test instances in five distinct versions. We aim to investigate three key dimensions of LMMs: scientific knowledge comprehension, multi-modal content interpretation, and Chain-of-Thought (CoT) reasoning...  \n",
            "   **Link**: [arXiv:2503.10627v1](http://arxiv.org/abs/2503.10627v1)\n",
            "\n",
            "5. **Title**: NIL: No-data Imitation Learning by Leveraging Pre-trained Video Diffusion Models  \n",
            "   **Authors**: Mert Albaba, Chenhao Li, Markos Diomataris, Omid Taheri, Andreas Krause, Michael Black  \n",
            "   **Abstract**: Acquiring physically plausible motor skills across diverse and unconventional morphologies is essential for advancing character simulation and robotics. Traditional methods, such as reinforcement learning (RL) are task- and body-specific...  \n",
            "   **Link**: [arXiv:2503.10626v1](http://arxiv.org/abs/2503.10626v1)\n",
            "\n",
            "6. **Title**: LHM: Large Animatable Human Reconstruction Model from a Single Image in Seconds  \n",
            "   **Authors**: Lingteng Qiu, Xiaodong Gu, Peihao Li, Qi Zuo, Weichao Shen, Junfei Zhang, Kejie Qiu, Weihao Yuan, Guanying Chen, Zilong Dong, Liefeng Bo  \n",
            "   **Abstract**: Animatable 3D human reconstruction from a single image is a challenging problem due to the ambiguity in decoupling geometry, appearance, and deformation. Recent advances in 3D human reconstruction mainly focus on static human modeling...  \n",
            "   **Link**: [arXiv:2503.10625v1](http://arxiv.org/abs/2503.10625v1)\n",
            "\n",
            "7. **Title**: ETCH: Generalizing Body Fitting to Clothed Humans via Equivariant Tightness  \n",
            "   **Authors**: Boqian Li, Haiwen Feng, Zeyu Cai, Michael J. Black, Yuliang Xiu  \n",
            "   **Abstract**: Fitting a body to a 3D clothed human point cloud is a common yet challenging task. Traditional optimization-based approaches use multi-stage pipelines that are sensitive to pose initialization...  \n",
            "   **Link**: [arXiv:2503.10624v1](http://arxiv.org/abs/2503.10624v1)\n",
            "\n",
            "8. **Title**: Transformers without Normalization  \n",
            "   **Authors**: Jiachen Zhu, Xinlei Chen, Kaiming He, Yann LeCun, Zhuang Liu  \n",
            "   **Abstract**: Normalization layers are ubiquitous in modern neural networks and have long been considered essential. This work demonstrates that Transformers without normalization can achieve the same or better performance using a remarkably simple technique...  \n",
            "   **Link**: [arXiv:2503.10622v1](http://arxiv.org/abs/2503.10622v1)\n",
            "\n",
            "9. **Title**: Siege: Autonomous Multi-Turn Jailbreaking of Large Language Models with Tree Search  \n",
            "   **Authors**: Andy Zhou  \n",
            "   **Abstract**: We introduce Siege, a multi-turn adversarial framework that models the gradual erosion of Large Language Model (LLM) safety through a tree search perspective...  \n",
            "   **Link**: [arXiv:2503.10619v1](http://arxiv.org/abs/2503.10619v1)\n",
            "\n",
            "10. **Title**: Compositional Subspace Representation Fine-tuning for Adaptive Large Language Models  \n",
            "    **Authors**: Andy Zhou  \n",
            "    **Abstract**: Adapting large language models to multiple tasks can cause cross-skill interference, where improvements for one skill degrade another...  \n",
            "    **Link**: [arXiv:2503.10617v1](http://arxiv.org/abs/2503.10617v1)\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m ## Final Result:\u001b[00m \u001b[92m1. **Title**: Studying Classifier(-Free) Guidance From a Classifier-Centric Perspective  \n",
            "   **Authors**: Xiaoming Zhao, Alexander G. Schwing  \n",
            "   **Abstract**: Classifier-free guidance has become a staple for conditional generation with denoising diffusion models. However, a comprehensive understanding of classifier-free guidance is still missing. In this work, we carry out an empirical study to provide a fresh perspective on classifier-free guidance. Concretely, instead of solely focusing on classifier-free guidance, we trace back to the root, i.e., classifier guidance, pinpoint the key assumption for the derivation, and conduct a systematic study to understand the role of the classifier. We find that both classifier guidance and classifier-free guidance achieve conditional generation by pushing the denoising diffusion trajectories away from decision boundaries, i.e., areas where conditional information is usually entangled and is hard to learn. Based on this classifier-centric understanding, we propose a generic postprocessing step built upon flow-matching to shrink the gap between the learned distribution for a pre-trained denoising diffusion model and the real data distribution, majorly around the decision boundaries. Experiments on various datasets verify the effectiveness of the proposed approach.  \n",
            "   **Link**: [arXiv:2503.10638v1](http://arxiv.org/abs/2503.10638v1)\n",
            "\n",
            "2. **Title**: A Frustratingly Simple Yet Highly Effective Attack Baseline: Over 90% Success Rate Against the Strong Black-box Models of GPT-4.5/4o/o1  \n",
            "   **Authors**: Zhaoyi Li, Xiaohan Zhao, Dong-Dong Wu, Jiacheng Cui, Zhiqiang Shen  \n",
            "   **Abstract**: Despite promising performance on open-source large vision-language models (LVLMs), transfer-based targeted attacks often fail against black-box commercial LVLMs. Analyzing failed adversarial perturbations reveals that the learned perturbations typically originate from a uniform distribution and lack clear semantic details, resulting in unintended responses. This critical absence of semantic information leads commercial LVLMs to either ignore the perturbation entirely or misinterpret its embedded semantics, thereby causing the attack to fail. To overcome these issues, we notice that identifying core semantic objects is a key objective for models trained with various datasets and methodologies. This insight motivates our approach that refines semantic clarity by encoding explicit semantic details within local regions, thus ensuring interoperability and capturing finer-grained features, and by concentrating modifications on semantically rich areas rather than applying them uniformly. To achieve this, we propose a simple yet highly effective solution...  \n",
            "   **Link**: [arXiv:2503.10635v1](http://arxiv.org/abs/2503.10635v1)\n",
            "\n",
            "3. **Title**: Uncertainty in Action: Confidence Elicitation in Embodied Agents  \n",
            "   **Authors**: Tianjiao Yu, Vedant Shah, Muntasir Wahed, Kiet A. Nguyen, Adheesh Juvekar, Tal August, Ismini Lourentzou  \n",
            "   **Abstract**: Expressing confidence is challenging for embodied agents navigating dynamic multimodal environments, where uncertainty arises from both perception and decision-making processes. We present the first work investigating embodied confidence elicitation in open-ended multimodal environments. We introduce Elicitation Policies, which structure confidence assessment across inductive, deductive, and abductive reasoning, along with Execution Policies, which enhance confidence calibration through scenario reinterpretation, action sampling, and hypothetical reasoning. Evaluating agents in calibration and failure prediction tasks within the Minecraft environment, we show that structured reasoning approaches...  \n",
            "   **Link**: [arXiv:2503.10628v1](http://arxiv.org/abs/2503.10628v1)\n",
            "\n",
            "4. **Title**: SciVerse: Unveiling the Knowledge Comprehension and Visual Reasoning of LMMs on Multi-modal Scientific Problems  \n",
            "   **Authors**: Ziyu Guo, Ray Zhang, Hao Chen, Jialin Gao, Dongzhi Jiang, Jiaze Wang, Pheng-Ann Heng  \n",
            "   **Abstract**: The rapid advancement of Large Multi-modal Models (LMMs) has enabled their application in scientific problem-solving, yet their fine-grained capabilities remain under-explored. In this paper, we introduce SciVerse, a multi-modal scientific evaluation benchmark to thoroughly assess LMMs across 5,735 test instances in five distinct versions. We aim to investigate three key dimensions of LMMs: scientific knowledge comprehension, multi-modal content interpretation, and Chain-of-Thought (CoT) reasoning...  \n",
            "   **Link**: [arXiv:2503.10627v1](http://arxiv.org/abs/2503.10627v1)\n",
            "\n",
            "5. **Title**: NIL: No-data Imitation Learning by Leveraging Pre-trained Video Diffusion Models  \n",
            "   **Authors**: Mert Albaba, Chenhao Li, Markos Diomataris, Omid Taheri, Andreas Krause, Michael Black  \n",
            "   **Abstract**: Acquiring physically plausible motor skills across diverse and unconventional morphologies is essential for advancing character simulation and robotics. Traditional methods, such as reinforcement learning (RL) are task- and body-specific...  \n",
            "   **Link**: [arXiv:2503.10626v1](http://arxiv.org/abs/2503.10626v1)\n",
            "\n",
            "6. **Title**: LHM: Large Animatable Human Reconstruction Model from a Single Image in Seconds  \n",
            "   **Authors**: Lingteng Qiu, Xiaodong Gu, Peihao Li, Qi Zuo, Weichao Shen, Junfei Zhang, Kejie Qiu, Weihao Yuan, Guanying Chen, Zilong Dong, Liefeng Bo  \n",
            "   **Abstract**: Animatable 3D human reconstruction from a single image is a challenging problem due to the ambiguity in decoupling geometry, appearance, and deformation. Recent advances in 3D human reconstruction mainly focus on static human modeling...  \n",
            "   **Link**: [arXiv:2503.10625v1](http://arxiv.org/abs/2503.10625v1)\n",
            "\n",
            "7. **Title**: ETCH: Generalizing Body Fitting to Clothed Humans via Equivariant Tightness  \n",
            "   **Authors**: Boqian Li, Haiwen Feng, Zeyu Cai, Michael J. Black, Yuliang Xiu  \n",
            "   **Abstract**: Fitting a body to a 3D clothed human point cloud is a common yet challenging task. Traditional optimization-based approaches use multi-stage pipelines that are sensitive to pose initialization...  \n",
            "   **Link**: [arXiv:2503.10624v1](http://arxiv.org/abs/2503.10624v1)\n",
            "\n",
            "8. **Title**: Transformers without Normalization  \n",
            "   **Authors**: Jiachen Zhu, Xinlei Chen, Kaiming He, Yann LeCun, Zhuang Liu  \n",
            "   **Abstract**: Normalization layers are ubiquitous in modern neural networks and have long been considered essential. This work demonstrates that Transformers without normalization can achieve the same or better performance using a remarkably simple technique...  \n",
            "   **Link**: [arXiv:2503.10622v1](http://arxiv.org/abs/2503.10622v1)\n",
            "\n",
            "9. **Title**: Siege: Autonomous Multi-Turn Jailbreaking of Large Language Models with Tree Search  \n",
            "   **Authors**: Andy Zhou  \n",
            "   **Abstract**: We introduce Siege, a multi-turn adversarial framework that models the gradual erosion of Large Language Model (LLM) safety through a tree search perspective...  \n",
            "   **Link**: [arXiv:2503.10619v1](http://arxiv.org/abs/2503.10619v1)\n",
            "\n",
            "10. **Title**: Compositional Subspace Representation Fine-tuning for Adaptive Large Language Models  \n",
            "    **Authors**: Andy Zhou  \n",
            "    **Abstract**: Adapting large language models to multiple tasks can cause cross-skill interference, where improvements for one skill degrade another...  \n",
            "    **Link**: [arXiv:2503.10617v1](http://arxiv.org/abs/2503.10617v1)\u001b[00m\n",
            "\u001b[1m\u001b[93m \n",
            "\n",
            "=====\n",
            "## HUMAN FEEDBACK: Provide feedback on the Final Result and Agent's actions.\n",
            "Please follow these guidelines:\n",
            " - If you are happy with the result, simply hit Enter without typing anything.\n",
            " - Otherwise, provide specific improvement requests.\n",
            " - You can provide multiple rounds of feedback until satisfied.\n",
            "=====\n",
            "\u001b[00m\n",
            "\n",
            "\u001b[1m\u001b[94m \n",
            "[2025-03-15 00:53:37][✅ AGENT 'SENIOR RESEARCHER' COMPLETED TASK]: 2025-03-15 00:53:37.824552\u001b[00m\n",
            "\u001b[1m\u001b[94m \n",
            "[2025-03-15 00:53:37][✅ TASK COMPLETED:  FIND THE TOP 10 RESEARCH PAPERS FROM THE SEARCH RESULTS FROM ARXIV ON 2025-03-13.]: 2025-03-15 00:53:37.825162\u001b[00m\n",
            "\u001b[1m\u001b[94m \n",
            "[2025-03-15 00:53:37][📋 TASK STARTED: COMPILE THE RESULTS INTO A DETAILED REPORT IN A HTML FILE.]: 2025-03-15 00:53:37.845171\u001b[00m\n",
            "\u001b[1m\u001b[94m \n",
            "[2025-03-15 00:53:37][🤖 AGENT 'SENIOR FRONTEND & AI ENGINEER' STARTED TASK]: 2025-03-15 00:53:37.847339\u001b[00m\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSenior Frontend & AI Engineer\u001b[00m\n",
            "\u001b[95m## Task:\u001b[00m \u001b[92mCompile the results into a detailed report in a HTML file.\u001b[00m\n",
            "\u001b[1m\u001b[94m \n",
            "[2025-03-15 00:53:37][🤖 LLM CALL STARTED]: 2025-03-15 00:53:37.847703\u001b[00m\n",
            "\u001b[1m\u001b[94m \n",
            "[2025-03-15 00:53:50][✅ LLM CALL COMPLETED]: 2025-03-15 00:53:50.650912\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSenior Frontend & AI Engineer\u001b[00m\n",
            "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
            "```html\n",
            "<!DOCTYPE html>\n",
            "<html lang=\"en\">\n",
            "<head>\n",
            "    <meta charset=\"UTF-8\">\n",
            "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
            "    <title>Top 10 AI Research Papers - March 13, 2025</title>\n",
            "    <style>\n",
            "        body {\n",
            "            font-family: Arial, sans-serif;\n",
            "            margin: 20px;\n",
            "            background-color: #f9f9f9;\n",
            "        }\n",
            "        h1 {\n",
            "            text-align: center;\n",
            "        }\n",
            "        table {\n",
            "            width: 100%;\n",
            "            border-collapse: collapse;\n",
            "            margin-top: 20px;\n",
            "        }\n",
            "        th, td {\n",
            "            border: 1px solid #ccc;\n",
            "            padding: 12px;\n",
            "            text-align: left;\n",
            "            background-color: #fff;\n",
            "        }\n",
            "        th {\n",
            "            background-color: #f2f2f2;\n",
            "        }\n",
            "        a {\n",
            "            color: #007BFF;\n",
            "            text-decoration: none;\n",
            "        }\n",
            "        a:hover {\n",
            "            text-decoration: underline;\n",
            "        }\n",
            "    </style>\n",
            "</head>\n",
            "<body>\n",
            "    <h1>Top 10 AI Research Papers - March 13, 2025</h1>\n",
            "    <table>\n",
            "        <thead>\n",
            "            <tr>\n",
            "                <th>Title</th>\n",
            "                <th>Authors</th>\n",
            "                <th>Short Summary</th>\n",
            "            </tr>\n",
            "        </thead>\n",
            "        <tbody>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10638v1\" target=\"_blank\">Studying Classifier(-Free) Guidance From a Classifier-Centric Perspective</a></td>\n",
            "                <td>Xiaoming Zhao, Alexander G. Schwing</td>\n",
            "                <td>This work explores classifier-free guidance in denoising diffusion models, tracing back to classifier guidance. It proposes a postprocessing step to improve the model's learning around decision boundaries, validated through experiments.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10635v1\" target=\"_blank\">A Frustratingly Simple Yet Highly Effective Attack Baseline: Over 90% Success Rate Against the Strong Black-box Models of GPT-4.5/4o/o1</a></td>\n",
            "                <td>Zhaoyi Li, Xiaohan Zhao, Dong-Dong Wu, Jiacheng Cui, Zhiqiang Shen</td>\n",
            "                <td>This paper presents a novel attack method against black-box large vision-language models by enhancing semantic clarity in adversarial perturbations, which often lack meaningful details, leading to higher attack success rates.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10628v1\" target=\"_blank\">Uncertainty in Action: Confidence Elicitation in Embodied Agents</a></td>\n",
            "                <td>Tianjiao Yu, Vedant Shah, Muntasir Wahed, Kiet A. Nguyen, Adheesh Juvekar, Tal August, Ismini Lourentzou</td>\n",
            "                <td>The study investigates how embodied agents can express confidence in uncertain environments by implementing structured confidence assessment and scenarios to improve confidence calibration through various reasoning strategies.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10627v1\" target=\"_blank\">SciVerse: Unveiling the Knowledge Comprehension and Visual Reasoning of LMMs on Multi-modal Scientific Problems</a></td>\n",
            "                <td>Ziyu Guo, Ray Zhang, Hao Chen, Jialin Gao, Dongzhi Jiang, Jiaze Wang, Pheng-Ann Heng</td>\n",
            "                <td>SciVerse is introduced as a multimodal evaluation benchmark for large multi-modal models, assessing their capabilities in scientific knowledge comprehension, interpretation, and reasoning across various instances.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10626v1\" target=\"_blank\">NIL: No-data Imitation Learning by Leveraging Pre-trained Video Diffusion Models</a></td>\n",
            "                <td>Mert Albaba, Chenhao Li, Markos Diomataris, Omid Taheri, Andreas Krause, Michael Black</td>\n",
            "                <td>This paper approaches the challenge of acquiring motor skills for character simulation by leveraging pre-trained video diffusion models to facilitate imitation learning without the need for specific task data.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10625v1\" target=\"_blank\">LHM: Large Animatable Human Reconstruction Model from a Single Image in Seconds</a></td>\n",
            "                <td>Lingteng Qiu, Xiaodong Gu, Peihao Li, Qi Zuo, Weichao Shen, Junfei Zhang, Kejie Qiu, Weihao Yuan, Guanying Chen, Zilong Dong, Liefeng Bo</td>\n",
            "                <td>This research tackles the problem of 3D human reconstruction from single images, focusing on creating animatable models to more effectively represent geometry, appearance, and deformation.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10624v1\" target=\"_blank\">ETCH: Generalizing Body Fitting to Clothed Humans via Equivariant Tightness</a></td>\n",
            "                <td>Boqian Li, Haiwen Feng, Zeyu Cai, Michael J. Black, Yuliang Xiu</td>\n",
            "                <td>This paper addresses the challenge of fitting models to 3D clothed human point clouds by introducing a technique that enhances fitting efficiency and robustness against pose variations, overcoming traditional optimization limitations.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10622v1\" target=\"_blank\">Transformers without Normalization</a></td>\n",
            "                <td>Jiachen Zhu, Xinlei Chen, Kaiming He, Yann LeCun, Zhuang Liu</td>\n",
            "                <td>This study explores the efficiency of Transformers without normalization layers, demonstrating that they can achieve similar, if not superior, performance through a simplified approach, reshaping conventional neural network designs.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10619v1\" target=\"_blank\">Siege: Autonomous Multi-Turn Jailbreaking of Large Language Models with Tree Search</a></td>\n",
            "                <td>Andy Zhou</td>\n",
            "                <td>The paper proposes a multi-turn adversarial framework for analyzing and overcoming the safety limitations of large language models, utilizing a tree search approach to model the erosion of constraints over interactions.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10617v1\" target=\"_blank\">Compositional Subspace Representation Fine-tuning for Adaptive Large Language Models</a></td>\n",
            "                <td>Andy Zhou</td>\n",
            "                <td>This research focuses on adapting large language models to multiple tasks by developing strategies to minimize cross-skill interference, optimizing the model performance across varied capabilities.</td>\n",
            "            </tr>\n",
            "        </tbody>\n",
            "    </table>\n",
            "</body>\n",
            "</html>\n",
            "```\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m ## Final Result:\u001b[00m \u001b[92m```html\n",
            "<!DOCTYPE html>\n",
            "<html lang=\"en\">\n",
            "<head>\n",
            "    <meta charset=\"UTF-8\">\n",
            "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
            "    <title>Top 10 AI Research Papers - March 13, 2025</title>\n",
            "    <style>\n",
            "        body {\n",
            "            font-family: Arial, sans-serif;\n",
            "            margin: 20px;\n",
            "            background-color: #f9f9f9;\n",
            "        }\n",
            "        h1 {\n",
            "            text-align: center;\n",
            "        }\n",
            "        table {\n",
            "            width: 100%;\n",
            "            border-collapse: collapse;\n",
            "            margin-top: 20px;\n",
            "        }\n",
            "        th, td {\n",
            "            border: 1px solid #ccc;\n",
            "            padding: 12px;\n",
            "            text-align: left;\n",
            "            background-color: #fff;\n",
            "        }\n",
            "        th {\n",
            "            background-color: #f2f2f2;\n",
            "        }\n",
            "        a {\n",
            "            color: #007BFF;\n",
            "            text-decoration: none;\n",
            "        }\n",
            "        a:hover {\n",
            "            text-decoration: underline;\n",
            "        }\n",
            "    </style>\n",
            "</head>\n",
            "<body>\n",
            "    <h1>Top 10 AI Research Papers - March 13, 2025</h1>\n",
            "    <table>\n",
            "        <thead>\n",
            "            <tr>\n",
            "                <th>Title</th>\n",
            "                <th>Authors</th>\n",
            "                <th>Short Summary</th>\n",
            "            </tr>\n",
            "        </thead>\n",
            "        <tbody>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10638v1\" target=\"_blank\">Studying Classifier(-Free) Guidance From a Classifier-Centric Perspective</a></td>\n",
            "                <td>Xiaoming Zhao, Alexander G. Schwing</td>\n",
            "                <td>This work explores classifier-free guidance in denoising diffusion models, tracing back to classifier guidance. It proposes a postprocessing step to improve the model's learning around decision boundaries, validated through experiments.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10635v1\" target=\"_blank\">A Frustratingly Simple Yet Highly Effective Attack Baseline: Over 90% Success Rate Against the Strong Black-box Models of GPT-4.5/4o/o1</a></td>\n",
            "                <td>Zhaoyi Li, Xiaohan Zhao, Dong-Dong Wu, Jiacheng Cui, Zhiqiang Shen</td>\n",
            "                <td>This paper presents a novel attack method against black-box large vision-language models by enhancing semantic clarity in adversarial perturbations, which often lack meaningful details, leading to higher attack success rates.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10628v1\" target=\"_blank\">Uncertainty in Action: Confidence Elicitation in Embodied Agents</a></td>\n",
            "                <td>Tianjiao Yu, Vedant Shah, Muntasir Wahed, Kiet A. Nguyen, Adheesh Juvekar, Tal August, Ismini Lourentzou</td>\n",
            "                <td>The study investigates how embodied agents can express confidence in uncertain environments by implementing structured confidence assessment and scenarios to improve confidence calibration through various reasoning strategies.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10627v1\" target=\"_blank\">SciVerse: Unveiling the Knowledge Comprehension and Visual Reasoning of LMMs on Multi-modal Scientific Problems</a></td>\n",
            "                <td>Ziyu Guo, Ray Zhang, Hao Chen, Jialin Gao, Dongzhi Jiang, Jiaze Wang, Pheng-Ann Heng</td>\n",
            "                <td>SciVerse is introduced as a multimodal evaluation benchmark for large multi-modal models, assessing their capabilities in scientific knowledge comprehension, interpretation, and reasoning across various instances.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10626v1\" target=\"_blank\">NIL: No-data Imitation Learning by Leveraging Pre-trained Video Diffusion Models</a></td>\n",
            "                <td>Mert Albaba, Chenhao Li, Markos Diomataris, Omid Taheri, Andreas Krause, Michael Black</td>\n",
            "                <td>This paper approaches the challenge of acquiring motor skills for character simulation by leveraging pre-trained video diffusion models to facilitate imitation learning without the need for specific task data.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10625v1\" target=\"_blank\">LHM: Large Animatable Human Reconstruction Model from a Single Image in Seconds</a></td>\n",
            "                <td>Lingteng Qiu, Xiaodong Gu, Peihao Li, Qi Zuo, Weichao Shen, Junfei Zhang, Kejie Qiu, Weihao Yuan, Guanying Chen, Zilong Dong, Liefeng Bo</td>\n",
            "                <td>This research tackles the problem of 3D human reconstruction from single images, focusing on creating animatable models to more effectively represent geometry, appearance, and deformation.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10624v1\" target=\"_blank\">ETCH: Generalizing Body Fitting to Clothed Humans via Equivariant Tightness</a></td>\n",
            "                <td>Boqian Li, Haiwen Feng, Zeyu Cai, Michael J. Black, Yuliang Xiu</td>\n",
            "                <td>This paper addresses the challenge of fitting models to 3D clothed human point clouds by introducing a technique that enhances fitting efficiency and robustness against pose variations, overcoming traditional optimization limitations.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10622v1\" target=\"_blank\">Transformers without Normalization</a></td>\n",
            "                <td>Jiachen Zhu, Xinlei Chen, Kaiming He, Yann LeCun, Zhuang Liu</td>\n",
            "                <td>This study explores the efficiency of Transformers without normalization layers, demonstrating that they can achieve similar, if not superior, performance through a simplified approach, reshaping conventional neural network designs.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10619v1\" target=\"_blank\">Siege: Autonomous Multi-Turn Jailbreaking of Large Language Models with Tree Search</a></td>\n",
            "                <td>Andy Zhou</td>\n",
            "                <td>The paper proposes a multi-turn adversarial framework for analyzing and overcoming the safety limitations of large language models, utilizing a tree search approach to model the erosion of constraints over interactions.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10617v1\" target=\"_blank\">Compositional Subspace Representation Fine-tuning for Adaptive Large Language Models</a></td>\n",
            "                <td>Andy Zhou</td>\n",
            "                <td>This research focuses on adapting large language models to multiple tasks by developing strategies to minimize cross-skill interference, optimizing the model performance across varied capabilities.</td>\n",
            "            </tr>\n",
            "        </tbody>\n",
            "    </table>\n",
            "</body>\n",
            "</html>\n",
            "```\u001b[00m\n",
            "\u001b[1m\u001b[93m \n",
            "\n",
            "=====\n",
            "## HUMAN FEEDBACK: Provide feedback on the Final Result and Agent's actions.\n",
            "Please follow these guidelines:\n",
            " - If you are happy with the result, simply hit Enter without typing anything.\n",
            " - Otherwise, provide specific improvement requests.\n",
            " - You can provide multiple rounds of feedback until satisfied.\n",
            "=====\n",
            "\u001b[00m\n",
            "Please remove '''html''' from the output\n",
            "\u001b[96m \n",
            "Processing your feedback...\u001b[00m\n",
            "\u001b[1m\u001b[94m \n",
            "[2025-03-15 00:54:08][🤖 LLM CALL STARTED]: 2025-03-15 00:54:08.621361\u001b[00m\n",
            "\u001b[1m\u001b[94m \n",
            "[2025-03-15 00:54:18][✅ LLM CALL COMPLETED]: 2025-03-15 00:54:18.333949\u001b[00m\n",
            "\u001b[1m\u001b[94m \n",
            "[2025-03-15 00:54:18][🤖 LLM CALL STARTED]: 2025-03-15 00:54:18.335401\u001b[00m\n",
            "\u001b[1m\u001b[94m \n",
            "[2025-03-15 00:54:32][✅ LLM CALL COMPLETED]: 2025-03-15 00:54:32.954371\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSenior Frontend & AI Engineer\u001b[00m\n",
            "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
            "``` \n",
            "<!DOCTYPE html>\n",
            "<html lang=\"en\">\n",
            "<head>\n",
            "    <meta charset=\"UTF-8\">\n",
            "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
            "    <title>Top 10 AI Research Papers - March 13, 2025</title>\n",
            "    <style>\n",
            "        body {\n",
            "            font-family: Arial, sans-serif;\n",
            "            margin: 20px;\n",
            "            background-color: #f9f9f9;\n",
            "        }\n",
            "        h1 {\n",
            "            text-align: center;\n",
            "        }\n",
            "        table {\n",
            "            width: 100%;\n",
            "            border-collapse: collapse;\n",
            "            margin-top: 20px;\n",
            "        }\n",
            "        th, td {\n",
            "            border: 1px solid #ccc;\n",
            "            padding: 12px;\n",
            "            text-align: left;\n",
            "            background-color: #fff;\n",
            "        }\n",
            "        th {\n",
            "            background-color: #f2f2f2;\n",
            "        }\n",
            "        a {\n",
            "            color: #007BFF;\n",
            "            text-decoration: none;\n",
            "        }\n",
            "        a:hover {\n",
            "            text-decoration: underline;\n",
            "        }\n",
            "    </style>\n",
            "</head>\n",
            "<body>\n",
            "    <h1>Top 10 AI Research Papers - March 13, 2025</h1>\n",
            "    <table>\n",
            "        <thead>\n",
            "            <tr>\n",
            "                <th>Title</th>\n",
            "                <th>Authors</th>\n",
            "                <th>Short Summary</th>\n",
            "            </tr>\n",
            "        </thead>\n",
            "        <tbody>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10638v1\" target=\"_blank\">Studying Classifier(-Free) Guidance From a Classifier-Centric Perspective</a></td>\n",
            "                <td>Xiaoming Zhao, Alexander G. Schwing</td>\n",
            "                <td>This work explores classifier-free guidance in denoising diffusion models, tracing back to classifier guidance. It proposes a postprocessing step to improve the model's learning around decision boundaries, validated through experiments.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10635v1\" target=\"_blank\">A Frustratingly Simple Yet Highly Effective Attack Baseline: Over 90% Success Rate Against the Strong Black-box Models of GPT-4.5/4o/o1</a></td>\n",
            "                <td>Zhaoyi Li, Xiaohan Zhao, Dong-Dong Wu, Jiacheng Cui, Zhiqiang Shen</td>\n",
            "                <td>This paper presents a novel attack method against black-box large vision-language models by enhancing semantic clarity in adversarial perturbations, which often lack meaningful details, leading to higher attack success rates.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10628v1\" target=\"_blank\">Uncertainty in Action: Confidence Elicitation in Embodied Agents</a></td>\n",
            "                <td>Tianjiao Yu, Vedant Shah, Muntasir Wahed, Kiet A. Nguyen, Adheesh Juvekar, Tal August, Ismini Lourentzou</td>\n",
            "                <td>The study investigates how embodied agents can express confidence in uncertain environments by implementing structured confidence assessment and scenarios to improve confidence calibration through various reasoning strategies.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10627v1\" target=\"_blank\">SciVerse: Unveiling the Knowledge Comprehension and Visual Reasoning of LMMs on Multi-modal Scientific Problems</a></td>\n",
            "                <td>Ziyu Guo, Ray Zhang, Hao Chen, Jialin Gao, Dongzhi Jiang, Jiaze Wang, Pheng-Ann Heng</td>\n",
            "                <td>SciVerse is introduced as a multimodal evaluation benchmark for large multi-modal models, assessing their capabilities in scientific knowledge comprehension, interpretation, and reasoning across various instances.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10626v1\" target=\"_blank\">NIL: No-data Imitation Learning by Leveraging Pre-trained Video Diffusion Models</a></td>\n",
            "                <td>Mert Albaba, Chenhao Li, Markos Diomataris, Omid Taheri, Andreas Krause, Michael Black</td>\n",
            "                <td>This paper approaches the challenge of acquiring motor skills for character simulation by leveraging pre-trained video diffusion models to facilitate imitation learning without the need for specific task data.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10625v1\" target=\"_blank\">LHM: Large Animatable Human Reconstruction Model from a Single Image in Seconds</a></td>\n",
            "                <td>Lingteng Qiu, Xiaodong Gu, Peihao Li, Qi Zuo, Weichao Shen, Junfei Zhang, Kejie Qiu, Weihao Yuan, Guanying Chen, Zilong Dong, Liefeng Bo</td>\n",
            "                <td>This research tackles the problem of 3D human reconstruction from single images, focusing on creating animatable models to more effectively represent geometry, appearance, and deformation.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10624v1\" target=\"_blank\">ETCH: Generalizing Body Fitting to Clothed Humans via Equivariant Tightness</a></td>\n",
            "                <td>Boqian Li, Haiwen Feng, Zeyu Cai, Michael J. Black, Yuliang Xiu</td>\n",
            "                <td>This paper addresses the challenge of fitting models to 3D clothed human point clouds by introducing a technique that enhances fitting efficiency and robustness against pose variations, overcoming traditional optimization limitations.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10622v1\" target=\"_blank\">Transformers without Normalization</a></td>\n",
            "                <td>Jiachen Zhu, Xinlei Chen, Kaiming He, Yann LeCun, Zhuang Liu</td>\n",
            "                <td>This study explores the efficiency of Transformers without normalization layers, demonstrating that they can achieve similar, if not superior, performance through a simplified approach, reshaping conventional neural network designs.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10619v1\" target=\"_blank\">Siege: Autonomous Multi-Turn Jailbreaking of Large Language Models with Tree Search</a></td>\n",
            "                <td>Andy Zhou</td>\n",
            "                <td>The paper proposes a multi-turn adversarial framework for analyzing and overcoming the safety limitations of large language models, utilizing a tree search approach to model the erosion of constraints over interactions.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10617v1\" target=\"_blank\">Compositional Subspace Representation Fine-tuning for Adaptive Large Language Models</a></td>\n",
            "                <td>Andy Zhou</td>\n",
            "                <td>This research focuses on adapting large language models to multiple tasks by developing strategies to minimize cross-skill interference, optimizing the model performance across varied capabilities.</td>\n",
            "            </tr>\n",
            "        </tbody>\n",
            "    </table>\n",
            "</body>\n",
            "</html>\n",
            "```\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m ## Final Result:\u001b[00m \u001b[92m``` \n",
            "<!DOCTYPE html>\n",
            "<html lang=\"en\">\n",
            "<head>\n",
            "    <meta charset=\"UTF-8\">\n",
            "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
            "    <title>Top 10 AI Research Papers - March 13, 2025</title>\n",
            "    <style>\n",
            "        body {\n",
            "            font-family: Arial, sans-serif;\n",
            "            margin: 20px;\n",
            "            background-color: #f9f9f9;\n",
            "        }\n",
            "        h1 {\n",
            "            text-align: center;\n",
            "        }\n",
            "        table {\n",
            "            width: 100%;\n",
            "            border-collapse: collapse;\n",
            "            margin-top: 20px;\n",
            "        }\n",
            "        th, td {\n",
            "            border: 1px solid #ccc;\n",
            "            padding: 12px;\n",
            "            text-align: left;\n",
            "            background-color: #fff;\n",
            "        }\n",
            "        th {\n",
            "            background-color: #f2f2f2;\n",
            "        }\n",
            "        a {\n",
            "            color: #007BFF;\n",
            "            text-decoration: none;\n",
            "        }\n",
            "        a:hover {\n",
            "            text-decoration: underline;\n",
            "        }\n",
            "    </style>\n",
            "</head>\n",
            "<body>\n",
            "    <h1>Top 10 AI Research Papers - March 13, 2025</h1>\n",
            "    <table>\n",
            "        <thead>\n",
            "            <tr>\n",
            "                <th>Title</th>\n",
            "                <th>Authors</th>\n",
            "                <th>Short Summary</th>\n",
            "            </tr>\n",
            "        </thead>\n",
            "        <tbody>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10638v1\" target=\"_blank\">Studying Classifier(-Free) Guidance From a Classifier-Centric Perspective</a></td>\n",
            "                <td>Xiaoming Zhao, Alexander G. Schwing</td>\n",
            "                <td>This work explores classifier-free guidance in denoising diffusion models, tracing back to classifier guidance. It proposes a postprocessing step to improve the model's learning around decision boundaries, validated through experiments.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10635v1\" target=\"_blank\">A Frustratingly Simple Yet Highly Effective Attack Baseline: Over 90% Success Rate Against the Strong Black-box Models of GPT-4.5/4o/o1</a></td>\n",
            "                <td>Zhaoyi Li, Xiaohan Zhao, Dong-Dong Wu, Jiacheng Cui, Zhiqiang Shen</td>\n",
            "                <td>This paper presents a novel attack method against black-box large vision-language models by enhancing semantic clarity in adversarial perturbations, which often lack meaningful details, leading to higher attack success rates.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10628v1\" target=\"_blank\">Uncertainty in Action: Confidence Elicitation in Embodied Agents</a></td>\n",
            "                <td>Tianjiao Yu, Vedant Shah, Muntasir Wahed, Kiet A. Nguyen, Adheesh Juvekar, Tal August, Ismini Lourentzou</td>\n",
            "                <td>The study investigates how embodied agents can express confidence in uncertain environments by implementing structured confidence assessment and scenarios to improve confidence calibration through various reasoning strategies.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10627v1\" target=\"_blank\">SciVerse: Unveiling the Knowledge Comprehension and Visual Reasoning of LMMs on Multi-modal Scientific Problems</a></td>\n",
            "                <td>Ziyu Guo, Ray Zhang, Hao Chen, Jialin Gao, Dongzhi Jiang, Jiaze Wang, Pheng-Ann Heng</td>\n",
            "                <td>SciVerse is introduced as a multimodal evaluation benchmark for large multi-modal models, assessing their capabilities in scientific knowledge comprehension, interpretation, and reasoning across various instances.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10626v1\" target=\"_blank\">NIL: No-data Imitation Learning by Leveraging Pre-trained Video Diffusion Models</a></td>\n",
            "                <td>Mert Albaba, Chenhao Li, Markos Diomataris, Omid Taheri, Andreas Krause, Michael Black</td>\n",
            "                <td>This paper approaches the challenge of acquiring motor skills for character simulation by leveraging pre-trained video diffusion models to facilitate imitation learning without the need for specific task data.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10625v1\" target=\"_blank\">LHM: Large Animatable Human Reconstruction Model from a Single Image in Seconds</a></td>\n",
            "                <td>Lingteng Qiu, Xiaodong Gu, Peihao Li, Qi Zuo, Weichao Shen, Junfei Zhang, Kejie Qiu, Weihao Yuan, Guanying Chen, Zilong Dong, Liefeng Bo</td>\n",
            "                <td>This research tackles the problem of 3D human reconstruction from single images, focusing on creating animatable models to more effectively represent geometry, appearance, and deformation.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10624v1\" target=\"_blank\">ETCH: Generalizing Body Fitting to Clothed Humans via Equivariant Tightness</a></td>\n",
            "                <td>Boqian Li, Haiwen Feng, Zeyu Cai, Michael J. Black, Yuliang Xiu</td>\n",
            "                <td>This paper addresses the challenge of fitting models to 3D clothed human point clouds by introducing a technique that enhances fitting efficiency and robustness against pose variations, overcoming traditional optimization limitations.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10622v1\" target=\"_blank\">Transformers without Normalization</a></td>\n",
            "                <td>Jiachen Zhu, Xinlei Chen, Kaiming He, Yann LeCun, Zhuang Liu</td>\n",
            "                <td>This study explores the efficiency of Transformers without normalization layers, demonstrating that they can achieve similar, if not superior, performance through a simplified approach, reshaping conventional neural network designs.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10619v1\" target=\"_blank\">Siege: Autonomous Multi-Turn Jailbreaking of Large Language Models with Tree Search</a></td>\n",
            "                <td>Andy Zhou</td>\n",
            "                <td>The paper proposes a multi-turn adversarial framework for analyzing and overcoming the safety limitations of large language models, utilizing a tree search approach to model the erosion of constraints over interactions.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10617v1\" target=\"_blank\">Compositional Subspace Representation Fine-tuning for Adaptive Large Language Models</a></td>\n",
            "                <td>Andy Zhou</td>\n",
            "                <td>This research focuses on adapting large language models to multiple tasks by developing strategies to minimize cross-skill interference, optimizing the model performance across varied capabilities.</td>\n",
            "            </tr>\n",
            "        </tbody>\n",
            "    </table>\n",
            "</body>\n",
            "</html>\n",
            "```\u001b[00m\n",
            "\u001b[1m\u001b[93m \n",
            "\n",
            "=====\n",
            "## HUMAN FEEDBACK: Provide feedback on the Final Result and Agent's actions.\n",
            "Please follow these guidelines:\n",
            " - If you are happy with the result, simply hit Enter without typing anything.\n",
            " - Otherwise, provide specific improvement requests.\n",
            " - You can provide multiple rounds of feedback until satisfied.\n",
            "=====\n",
            "\u001b[00m\n",
            "remove the '''''' from top and bottom of the html\n",
            "\u001b[96m \n",
            "Processing your feedback...\u001b[00m\n",
            "\u001b[1m\u001b[94m \n",
            "[2025-03-15 00:54:53][🤖 LLM CALL STARTED]: 2025-03-15 00:54:53.135169\u001b[00m\n",
            "\u001b[1m\u001b[94m \n",
            "[2025-03-15 00:55:05][✅ LLM CALL COMPLETED]: 2025-03-15 00:55:05.679772\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSenior Frontend & AI Engineer\u001b[00m\n",
            "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
            "<!DOCTYPE html>\n",
            "<html lang=\"en\">\n",
            "<head>\n",
            "    <meta charset=\"UTF-8\">\n",
            "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
            "    <title>Top 10 AI Research Papers - March 13, 2025</title>\n",
            "    <style>\n",
            "        body {\n",
            "            font-family: Arial, sans-serif;\n",
            "            margin: 20px;\n",
            "            background-color: #f9f9f9;\n",
            "        }\n",
            "        h1 {\n",
            "            text-align: center;\n",
            "        }\n",
            "        table {\n",
            "            width: 100%;\n",
            "            border-collapse: collapse;\n",
            "            margin-top: 20px;\n",
            "        }\n",
            "        th, td {\n",
            "            border: 1px solid #ccc;\n",
            "            padding: 12px;\n",
            "            text-align: left;\n",
            "            background-color: #fff;\n",
            "        }\n",
            "        th {\n",
            "            background-color: #f2f2f2;\n",
            "        }\n",
            "        a {\n",
            "            color: #007BFF;\n",
            "            text-decoration: none;\n",
            "        }\n",
            "        a:hover {\n",
            "            text-decoration: underline;\n",
            "        }\n",
            "    </style>\n",
            "</head>\n",
            "<body>\n",
            "    <h1>Top 10 AI Research Papers - March 13, 2025</h1>\n",
            "    <table>\n",
            "        <thead>\n",
            "            <tr>\n",
            "                <th>Title</th>\n",
            "                <th>Authors</th>\n",
            "                <th>Short Summary</th>\n",
            "            </tr>\n",
            "        </thead>\n",
            "        <tbody>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10638v1\" target=\"_blank\">Studying Classifier(-Free) Guidance From a Classifier-Centric Perspective</a></td>\n",
            "                <td>Xiaoming Zhao, Alexander G. Schwing</td>\n",
            "                <td>This work explores classifier-free guidance in denoising diffusion models, tracing back to classifier guidance. It proposes a postprocessing step to improve the model's learning around decision boundaries, validated through experiments.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10635v1\" target=\"_blank\">A Frustratingly Simple Yet Highly Effective Attack Baseline: Over 90% Success Rate Against the Strong Black-box Models of GPT-4.5/4o/o1</a></td>\n",
            "                <td>Zhaoyi Li, Xiaohan Zhao, Dong-Dong Wu, Jiacheng Cui, Zhiqiang Shen</td>\n",
            "                <td>This paper presents a novel attack method against black-box large vision-language models by enhancing semantic clarity in adversarial perturbations, which often lack meaningful details, leading to higher attack success rates.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10628v1\" target=\"_blank\">Uncertainty in Action: Confidence Elicitation in Embodied Agents</a></td>\n",
            "                <td>Tianjiao Yu, Vedant Shah, Muntasir Wahed, Kiet A. Nguyen, Adheesh Juvekar, Tal August, Ismini Lourentzou</td>\n",
            "                <td>The study investigates how embodied agents can express confidence in uncertain environments by implementing structured confidence assessment and scenarios to improve confidence calibration through various reasoning strategies.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10627v1\" target=\"_blank\">SciVerse: Unveiling the Knowledge Comprehension and Visual Reasoning of LMMs on Multi-modal Scientific Problems</a></td>\n",
            "                <td>Ziyu Guo, Ray Zhang, Hao Chen, Jialin Gao, Dongzhi Jiang, Jiaze Wang, Pheng-Ann Heng</td>\n",
            "                <td>SciVerse is introduced as a multimodal evaluation benchmark for large multi-modal models, assessing their capabilities in scientific knowledge comprehension, interpretation, and reasoning across various instances.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10626v1\" target=\"_blank\">NIL: No-data Imitation Learning by Leveraging Pre-trained Video Diffusion Models</a></td>\n",
            "                <td>Mert Albaba, Chenhao Li, Markos Diomataris, Omid Taheri, Andreas Krause, Michael Black</td>\n",
            "                <td>This paper approaches the challenge of acquiring motor skills for character simulation by leveraging pre-trained video diffusion models to facilitate imitation learning without the need for specific task data.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10625v1\" target=\"_blank\">LHM: Large Animatable Human Reconstruction Model from a Single Image in Seconds</a></td>\n",
            "                <td>Lingteng Qiu, Xiaodong Gu, Peihao Li, Qi Zuo, Weichao Shen, Junfei Zhang, Kejie Qiu, Weihao Yuan, Guanying Chen, Zilong Dong, Liefeng Bo</td>\n",
            "                <td>This research tackles the problem of 3D human reconstruction from single images, focusing on creating animatable models to more effectively represent geometry, appearance, and deformation.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10624v1\" target=\"_blank\">ETCH: Generalizing Body Fitting to Clothed Humans via Equivariant Tightness</a></td>\n",
            "                <td>Boqian Li, Haiwen Feng, Zeyu Cai, Michael J. Black, Yuliang Xiu</td>\n",
            "                <td>This paper addresses the challenge of fitting models to 3D clothed human point clouds by introducing a technique that enhances fitting efficiency and robustness against pose variations, overcoming traditional optimization limitations.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10622v1\" target=\"_blank\">Transformers without Normalization</a></td>\n",
            "                <td>Jiachen Zhu, Xinlei Chen, Kaiming He, Yann LeCun, Zhuang Liu</td>\n",
            "                <td>This study explores the efficiency of Transformers without normalization layers, demonstrating that they can achieve similar, if not superior, performance through a simplified approach, reshaping conventional neural network designs.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10619v1\" target=\"_blank\">Siege: Autonomous Multi-Turn Jailbreaking of Large Language Models with Tree Search</a></td>\n",
            "                <td>Andy Zhou</td>\n",
            "                <td>The paper proposes a multi-turn adversarial framework for analyzing and overcoming the safety limitations of large language models, utilizing a tree search approach to model the erosion of constraints over interactions.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10617v1\" target=\"_blank\">Compositional Subspace Representation Fine-tuning for Adaptive Large Language Models</a></td>\n",
            "                <td>Andy Zhou</td>\n",
            "                <td>This research focuses on adapting large language models to multiple tasks by developing strategies to minimize cross-skill interference, optimizing the model performance across varied capabilities.</td>\n",
            "            </tr>\n",
            "        </tbody>\n",
            "    </table>\n",
            "</body>\n",
            "</html>\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m ## Final Result:\u001b[00m \u001b[92m<!DOCTYPE html>\n",
            "<html lang=\"en\">\n",
            "<head>\n",
            "    <meta charset=\"UTF-8\">\n",
            "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
            "    <title>Top 10 AI Research Papers - March 13, 2025</title>\n",
            "    <style>\n",
            "        body {\n",
            "            font-family: Arial, sans-serif;\n",
            "            margin: 20px;\n",
            "            background-color: #f9f9f9;\n",
            "        }\n",
            "        h1 {\n",
            "            text-align: center;\n",
            "        }\n",
            "        table {\n",
            "            width: 100%;\n",
            "            border-collapse: collapse;\n",
            "            margin-top: 20px;\n",
            "        }\n",
            "        th, td {\n",
            "            border: 1px solid #ccc;\n",
            "            padding: 12px;\n",
            "            text-align: left;\n",
            "            background-color: #fff;\n",
            "        }\n",
            "        th {\n",
            "            background-color: #f2f2f2;\n",
            "        }\n",
            "        a {\n",
            "            color: #007BFF;\n",
            "            text-decoration: none;\n",
            "        }\n",
            "        a:hover {\n",
            "            text-decoration: underline;\n",
            "        }\n",
            "    </style>\n",
            "</head>\n",
            "<body>\n",
            "    <h1>Top 10 AI Research Papers - March 13, 2025</h1>\n",
            "    <table>\n",
            "        <thead>\n",
            "            <tr>\n",
            "                <th>Title</th>\n",
            "                <th>Authors</th>\n",
            "                <th>Short Summary</th>\n",
            "            </tr>\n",
            "        </thead>\n",
            "        <tbody>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10638v1\" target=\"_blank\">Studying Classifier(-Free) Guidance From a Classifier-Centric Perspective</a></td>\n",
            "                <td>Xiaoming Zhao, Alexander G. Schwing</td>\n",
            "                <td>This work explores classifier-free guidance in denoising diffusion models, tracing back to classifier guidance. It proposes a postprocessing step to improve the model's learning around decision boundaries, validated through experiments.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10635v1\" target=\"_blank\">A Frustratingly Simple Yet Highly Effective Attack Baseline: Over 90% Success Rate Against the Strong Black-box Models of GPT-4.5/4o/o1</a></td>\n",
            "                <td>Zhaoyi Li, Xiaohan Zhao, Dong-Dong Wu, Jiacheng Cui, Zhiqiang Shen</td>\n",
            "                <td>This paper presents a novel attack method against black-box large vision-language models by enhancing semantic clarity in adversarial perturbations, which often lack meaningful details, leading to higher attack success rates.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10628v1\" target=\"_blank\">Uncertainty in Action: Confidence Elicitation in Embodied Agents</a></td>\n",
            "                <td>Tianjiao Yu, Vedant Shah, Muntasir Wahed, Kiet A. Nguyen, Adheesh Juvekar, Tal August, Ismini Lourentzou</td>\n",
            "                <td>The study investigates how embodied agents can express confidence in uncertain environments by implementing structured confidence assessment and scenarios to improve confidence calibration through various reasoning strategies.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10627v1\" target=\"_blank\">SciVerse: Unveiling the Knowledge Comprehension and Visual Reasoning of LMMs on Multi-modal Scientific Problems</a></td>\n",
            "                <td>Ziyu Guo, Ray Zhang, Hao Chen, Jialin Gao, Dongzhi Jiang, Jiaze Wang, Pheng-Ann Heng</td>\n",
            "                <td>SciVerse is introduced as a multimodal evaluation benchmark for large multi-modal models, assessing their capabilities in scientific knowledge comprehension, interpretation, and reasoning across various instances.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10626v1\" target=\"_blank\">NIL: No-data Imitation Learning by Leveraging Pre-trained Video Diffusion Models</a></td>\n",
            "                <td>Mert Albaba, Chenhao Li, Markos Diomataris, Omid Taheri, Andreas Krause, Michael Black</td>\n",
            "                <td>This paper approaches the challenge of acquiring motor skills for character simulation by leveraging pre-trained video diffusion models to facilitate imitation learning without the need for specific task data.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10625v1\" target=\"_blank\">LHM: Large Animatable Human Reconstruction Model from a Single Image in Seconds</a></td>\n",
            "                <td>Lingteng Qiu, Xiaodong Gu, Peihao Li, Qi Zuo, Weichao Shen, Junfei Zhang, Kejie Qiu, Weihao Yuan, Guanying Chen, Zilong Dong, Liefeng Bo</td>\n",
            "                <td>This research tackles the problem of 3D human reconstruction from single images, focusing on creating animatable models to more effectively represent geometry, appearance, and deformation.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10624v1\" target=\"_blank\">ETCH: Generalizing Body Fitting to Clothed Humans via Equivariant Tightness</a></td>\n",
            "                <td>Boqian Li, Haiwen Feng, Zeyu Cai, Michael J. Black, Yuliang Xiu</td>\n",
            "                <td>This paper addresses the challenge of fitting models to 3D clothed human point clouds by introducing a technique that enhances fitting efficiency and robustness against pose variations, overcoming traditional optimization limitations.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10622v1\" target=\"_blank\">Transformers without Normalization</a></td>\n",
            "                <td>Jiachen Zhu, Xinlei Chen, Kaiming He, Yann LeCun, Zhuang Liu</td>\n",
            "                <td>This study explores the efficiency of Transformers without normalization layers, demonstrating that they can achieve similar, if not superior, performance through a simplified approach, reshaping conventional neural network designs.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10619v1\" target=\"_blank\">Siege: Autonomous Multi-Turn Jailbreaking of Large Language Models with Tree Search</a></td>\n",
            "                <td>Andy Zhou</td>\n",
            "                <td>The paper proposes a multi-turn adversarial framework for analyzing and overcoming the safety limitations of large language models, utilizing a tree search approach to model the erosion of constraints over interactions.</td>\n",
            "            </tr>\n",
            "            <tr>\n",
            "                <td><a href=\"http://arxiv.org/abs/2503.10617v1\" target=\"_blank\">Compositional Subspace Representation Fine-tuning for Adaptive Large Language Models</a></td>\n",
            "                <td>Andy Zhou</td>\n",
            "                <td>This research focuses on adapting large language models to multiple tasks by developing strategies to minimize cross-skill interference, optimizing the model performance across varied capabilities.</td>\n",
            "            </tr>\n",
            "        </tbody>\n",
            "    </table>\n",
            "</body>\n",
            "</html>\u001b[00m\n",
            "\u001b[1m\u001b[93m \n",
            "\n",
            "=====\n",
            "## HUMAN FEEDBACK: Provide feedback on the Final Result and Agent's actions.\n",
            "Please follow these guidelines:\n",
            " - If you are happy with the result, simply hit Enter without typing anything.\n",
            " - Otherwise, provide specific improvement requests.\n",
            " - You can provide multiple rounds of feedback until satisfied.\n",
            "=====\n",
            "\u001b[00m\n",
            "\n",
            "\u001b[1m\u001b[94m \n",
            "[2025-03-15 00:55:07][✅ AGENT 'SENIOR FRONTEND & AI ENGINEER' COMPLETED TASK]: 2025-03-15 00:55:07.702236\u001b[00m\n",
            "\u001b[1m\u001b[94m \n",
            "[2025-03-15 00:55:07][✅ TASK COMPLETED: COMPILE THE RESULTS INTO A DETAILED REPORT IN A HTML FILE.]: 2025-03-15 00:55:07.703525\u001b[00m\n",
            "\u001b[1m\u001b[94m \n",
            "[2025-03-15 00:55:07][✅ CREW 'CREW' COMPLETED, 03B5FF71-49EF-43F9-85D3-3B37AFB8712D]: 2025-03-15 00:55:07.718391\u001b[00m\n"
          ]
        }
      ],
      "source": [
        "result = arxiv_research_crew.kickoff(inputs = crew_inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwLrZqWBVXnL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

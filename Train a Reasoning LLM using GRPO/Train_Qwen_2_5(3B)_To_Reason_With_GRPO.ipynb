{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "E_N-Wlh6DD4k"
   },
   "outputs": [],
   "source": [
    "import os, numpy\n",
    "os.environ[\"UNSLOTH_VLLM_STANDBY\"] = \"1\"  # To get extra 30% context length\n",
    "\n",
    "# Get current numpy version to prevent breaking changes\n",
    "numpy_version = f\"numpy=={numpy.__version__}\"\n",
    "\n",
    "# Install dependencies with numpy version preservation\n",
    "!uv pip install unsloth_zoo\n",
    "!uv pip install --upgrade unsloth vllm==0.9.2 {numpy_version} torchvision bitsandbytes xformers\n",
    "!uv pip install triton==3.2.0\n",
    "!uv pip install transformers==4.55.4\n",
    "!uv pip install --no-deps trl==0.22.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ipMNhcjGDMHA"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Context length\n",
    "max_seq_length = 1024\n",
    "\n",
    "# Load model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen2.5-3B-Instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = True,\n",
    "    fast_inference = True, # Enable vLLM fast inference\n",
    "    max_lora_rank = 8,\n",
    "    gpu_memory_utilization = 0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F_7qcT1dD7SC",
    "outputId": "65dcd3fd-74b1-4cb6-9cd8-e75900514fab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.9.6 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "# LoRA for parameter-efficient fine-tuning\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 8,\n",
    "    # Modules to fine-tune\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha = 8,\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 1234,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZteldNI-wYOf"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# System prompt\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "# Template for wrapping the reasoning and answer\n",
    "XML_COT_FORMAT = \"\"\"\\\n",
    "<reasoning>\n",
    "{reasoning}\n",
    "</reasoning>\n",
    "<answer>\n",
    "{answer}\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "# Function to extract the text inside <answer>...</answer> from a model output\n",
    "def extract_xml_answer(text):\n",
    "    if \"<answer>\" not in text or \"</answer>\" not in text:\n",
    "        return \"\"\n",
    "    return text.split(\"<answer>\", 1)[-1].split(\"</answer>\", 1)[0].strip()\n",
    "\n",
    "# Function to extract the correct answer from GSM8K labels that are in the form '... #### final_answer'\n",
    "def extract_hash_answer(text):\n",
    "    return text.split(\"####\")[-1].strip() if \"####\" in text else None\n",
    "\n",
    "# Function to load the GSM8K dataset and format it as chat-style prompts\n",
    "def get_gsm8k_dataset(split = \"train\"):\n",
    "    data = load_dataset(\"openai/gsm8k\", \"main\")[split]\n",
    "    return data.map(\n",
    "        lambda x: {\n",
    "            \"prompt\": [\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": x[\"question\"]},\n",
    "            ],\n",
    "            \"answer\": extract_hash_answer(x[\"answer\"]),\n",
    "        }\n",
    "    )\n",
    "\n",
    "dataset = get_gsm8k_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ncMSNsr1LIqs"
   },
   "outputs": [],
   "source": [
    "# Reward function that checks if the extracted answer from the completion\n",
    "# exactly matches the given ground truth answer.\n",
    "# Returns 2.0 for a correct match, otherwise 0.0.\n",
    "def correctness_reward_func(prompts, completions, answer, **kwargs):\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    q = prompts[0][-1]['content']\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    print('-'*20, f\"Question:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n",
    "    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n",
    "\n",
    "# Reward function that checks if the extracted response is an integer.\n",
    "# Returns 0.5 if it is a digit, otherwise 0.0.\n",
    "def int_reward_func(completions, **kwargs):\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]\n",
    "\n",
    "# Reward function that enforces a strict XML format where\n",
    "# the response must match the exact structure:\n",
    "# <reasoning>\\n...\\n</reasoning>\\n<answer>\\n...\\n</answer>\\n\n",
    "# Returns 0.5 if the format is correct, otherwise 0.0.\n",
    "def strict_format_reward_func(completions, **kwargs):\n",
    "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "# Reward function that enforces a softer XML format check:\n",
    "# The response must contain <reasoning>...</reasoning> and <answer>...</answer>,\n",
    "# but allows for some flexibility in spacing and newlines.\n",
    "# Returns 0.5 if pattern is matched, otherwise 0.0.\n",
    "def soft_format_reward_func(completions, **kwargs):\n",
    "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "# Helper function that counts and scores XML tag occurrences\n",
    "def count_xml(text):\n",
    "    count = 0.0\n",
    "    if text.count(\"<reasoning>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"\\n</reasoning>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"\\n<answer>\\n\") == 1:\n",
    "        count += 0.125\n",
    "        count -= len(text.split(\"\\n</answer>\\n\")[-1])*0.001\n",
    "    if text.count(\"\\n</answer>\") == 1:\n",
    "        count += 0.125\n",
    "        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1)*0.001\n",
    "    return count\n",
    "\n",
    "# Reward function that applies the count_xml to completions.\n",
    "# Scores XML structure based on correct tags, with penalties for trailing content.\n",
    "def xmlcount_reward_func(completions, **kwargs):\n",
    "    contents = [completion[0][\"content\"] for completion in completions]\n",
    "    return [count_xml(c) for c in contents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "FPUyEFkKP6uU"
   },
   "outputs": [],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "# Training arguments\n",
    "training_args = GRPOConfig(\n",
    "    use_vllm = True, # use vLLM for fast inference\n",
    "    learning_rate = 5e-6,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.99,\n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    optim = \"adamw_8bit\",\n",
    "    logging_steps = 1,\n",
    "    per_device_train_batch_size = 4,\n",
    "    gradient_accumulation_steps = 1,\n",
    "    num_generations = 4,\n",
    "    max_prompt_length = 256,\n",
    "    max_completion_length = 200,\n",
    "    max_steps = 250,\n",
    "    save_steps = 250,\n",
    "    max_grad_norm = 0.1,\n",
    "    report_to = \"none\",\n",
    "    output_dir = \"outputs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "sCsMwO62QC1m"
   },
   "outputs": [],
   "source": [
    "# GRPO Trainer\n",
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = [\n",
    "        xmlcount_reward_func,\n",
    "        soft_format_reward_func,\n",
    "        strict_format_reward_func,\n",
    "        int_reward_func,\n",
    "        correctness_reward_func,\n",
    "    ],\n",
    "    args = training_args,\n",
    "    train_dataset = dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gNUgpG8cQIiR"
   },
   "outputs": [],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "QmH3TsVHZNFz"
   },
   "outputs": [],
   "source": [
    "# Save LoRA\n",
    "model.save_lora(\"grpo_saved_lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 99,
     "referenced_widgets": [
      "ac59d401b9ba4504af4cdbb2dffe267c",
      "dedb45036e2f40c5b3f6fe4ac7c4cc19",
      "04d2c571be674cf7b185fa2add3c59dd",
      "a6b9195f3d4e4947b0b99c48ca018507",
      "a9d8896fd56546e1a0ce8d3158c08713",
      "5413a0c02cd945f7bf51f5843acaa659",
      "1a88e21b2ac34f0f97f39253c5995a47",
      "1a416a353e674834a6c501e3e0ea4649",
      "5151b350328c48d1a89c208d68078cec",
      "a6984e2c83104404902920b79e8b3420",
      "3155543413b74c8ab0a98fe7098c4aaa",
      "b318f3f84a7a43f9bf34a72c06566a15",
      "d412f3e8ed9b4f38b602ea4abcd49328",
      "4b10186661e74cf7a89ab98c7a2c2dd7",
      "5c828f34bd1646568cc880865695555c",
      "a3f97433a0e041d0b5b2bdf272c41298",
      "64c0c979660142c1bd3e90822c8725fb",
      "f6c58a8a970e48c8b9ef61be3a529232",
      "0cbadb1680fb4febb38c06e4bdec5742",
      "d820c0304d9c447d83efeeb2ff5df1dc",
      "1bb7dc5aad8b401eb02cbe6e25b1fec1",
      "e0f1cec09fa64c6a94c5078ac9c0aa8b"
     ]
    },
    "id": "T1ALPJCJQJDw",
    "outputId": "c5a1ce05-6c91-464e-ac72-30ba0e36d768"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac59d401b9ba4504af4cdbb2dffe267c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b318f3f84a7a43f9bf34a72c06566a15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the word \"strawberry,\" there is one occurrence of the letter \"r.\"\n"
     ]
    }
   ],
   "source": [
    "from vllm import SamplingParams\n",
    "\n",
    "# Inference with model before training\n",
    "query = \"How many r's are in strawberry?\"\n",
    "\n",
    "text = tokenizer.apply_chat_template([\n",
    "    {\"role\" : \"user\", \"content\" : query},\n",
    "], tokenize = False, add_generation_prompt = True)\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 0.8,\n",
    "    top_p = 0.95,\n",
    "    max_tokens = 1024,\n",
    ")\n",
    "\n",
    "output = model.fast_generate(\n",
    "    [text],\n",
    "    sampling_params = sampling_params,\n",
    "    lora_request = None,\n",
    ")[0].outputs[0].text\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188,
     "referenced_widgets": [
      "9b94df8b1cf54a288b4e6303a017abdf",
      "622cfc1a83f54dbfa6232e966a417631",
      "f97bc309efa343778b1c75e145f62ac2",
      "bbded4f876994df6bf4947099edf3c56",
      "3ab3fdce889541b29a19d983ebca5af8",
      "41e364033bd44524a5ee8fc532385913",
      "d1dcaf3a8e37409689ab2cfe3517b825",
      "3fe30a70a48c47fdaccd85e1980e6caf",
      "7268dc2076e1420ca69d58fab0317739",
      "0cf9a0a8c1f04ec1920bc68a78a5b4ee",
      "48d9f338c8e64dcba30dd05987bb7b1a",
      "338a474e91554bfea17cc24a147d9560",
      "695f0def92bf43e4ba7914c99afa74c1",
      "2cb9525bd9994b14aeb83aa1b7eaafa6",
      "e94f493f92a84055bf074f390566632a",
      "a9a664cadfca48ecbdad77d60531bad6",
      "f4ff6da3096c4346a87f6d86b7946f6a",
      "d76fc11d96554192ad988bb4444eede1",
      "b40a6a765a3545c1b68e7d02dd6c74ed",
      "2de85796899c4fceaa0b060c12bd8e30",
      "1bae699f95ea4e628f7410eaface4ad5",
      "f327cac189f44d939a7f0eb20dce960a"
     ]
    },
    "id": "EG6xpcQEZLQi",
    "outputId": "826bab20-f228-467b-c74b-76ea682408b8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b94df8b1cf54a288b4e6303a017abdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "338a474e91554bfea17cc24a147d9560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<reasoning>\n",
      "To find out how many times the letter 'r' appears in the word 'strawberry', I will go through the word character by character and count each occurrence of 'r'.\n",
      "</reasoning>\n",
      "<answer>\n",
      "I found that the letter 'r' appears 3 times in the word 'strawberry'. \n",
      "</answer>\n"
     ]
    }
   ],
   "source": [
    "# Inference with model after training\n",
    "text = tokenizer.apply_chat_template([\n",
    "    {\"role\" : \"system\", \"content\" : SYSTEM_PROMPT},\n",
    "    {\"role\" : \"user\", \"content\" : query},\n",
    "], tokenize = False, add_generation_prompt = True)\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 0.8,\n",
    "    top_p = 0.95,\n",
    "    max_tokens = 1024,\n",
    ")\n",
    "\n",
    "output = model.fast_generate(\n",
    "    text,\n",
    "    sampling_params = sampling_params,\n",
    "    lora_request = model.load_lora(\"grpo_saved_lora\"),\n",
    ")[0].outputs[0].text\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LcTGrjB2G1pn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "EL_2KyAP3po1"
      },
      "outputs": [],
      "source": [
        "!uv pip install unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAPGhNyBJBZ5"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Llama-3.2-1B-Instruct\",\n",
        "    max_seq_length = 2048,\n",
        "    load_in_4bit = True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7vydYV3I8N7"
      },
      "outputs": [],
      "source": [
        "# Test the base model on a medical question\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model.eval()\n",
        "\n",
        "prompt = \"\"\"\n",
        "Q. A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7째F (36.5째C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus.\n",
        "Which of the following is the best treatment for this patient?\n",
        "\n",
        "Options:\n",
        "A: Ampicillin\n",
        "B: Ceftriaxone\n",
        "C: Ciprofloxacin\n",
        "D: Doxycycline\n",
        "E: Nitrofurantoin\n",
        "\n",
        "Please choose the correct answer.\n",
        "\"\"\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "input_len = inputs[\"input_ids\"].shape[-1]\n",
        "\n",
        "output = model.generate(**inputs, max_new_tokens=128)\n",
        "\n",
        "response = tokenizer.decode(output[0][input_len:], skip_special_tokens=True)\n",
        "\n",
        "print(\"Test Prompt:\\n\", prompt.strip())\n",
        "print(\"\\nTest Response:\\n\", response.strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6OzvPIfILOw"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"Neelectric/MedQA-USMLE\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFpqiNvNJYzK"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkgxOw65Jhrd"
      },
      "outputs": [],
      "source": [
        "print(\"Question:\\n\", dataset[0][\"question\"])\n",
        "print(\"\\nOptions:\\n\", dataset[0][\"options\"])\n",
        "print(\"\\nAnswer:\\n\", dataset[0][\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8nWaifqIvKw"
      },
      "outputs": [],
      "source": [
        "# Format text according to the model\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "tokenizer = get_chat_template(tokenizer, chat_template=\"llama-3.2\")\n",
        "\n",
        "def format_prompts(examples):\n",
        "    questions = examples[\"question\"]\n",
        "    options_list = examples[\"options\"]\n",
        "    answers = examples[\"answer\"]\n",
        "\n",
        "    conversations = []\n",
        "    for q, opts, ans in zip(questions, options_list, answers):\n",
        "        options_text = \"\\n\".join([f\"{key}. {val}\" for key, val in opts.items()])\n",
        "        prompt = f\"{q}\\n\\nOptions:\\n{options_text}\\n\\nPlease choose the correct answer.\"\n",
        "        response = f\"The correct answer is {ans}.\"\n",
        "        conversations.append([\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "            {\"role\": \"assistant\", \"content\": response},\n",
        "        ])\n",
        "\n",
        "    texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in conversations]\n",
        "\n",
        "    return {\"text\": texts}\n",
        "\n",
        "dataset = dataset.map(format_prompts, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wp_oHEEPKEWh"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.map(format_prompts, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wS6_4xF_KRQB"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-VMUPBuKSxh"
      },
      "outputs": [],
      "source": [
        "print(\"Question:\\n\", dataset[0][\"question\"])\n",
        "print(\"\\nOptions:\\n\", dataset[0][\"options\"])\n",
        "print(\"\\nAnswer:\\n\", dataset[0][\"answer\"])\n",
        "print(\"\\nFormatted Text:\\n\", dataset[0][\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hx0zOxv8bAkG"
      },
      "outputs": [],
      "source": [
        "# LoRA configuration\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=42,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaB_ExpQbAiO"
      },
      "outputs": [],
      "source": [
        "# Setting up SFTTrainer\n",
        "\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length= 2048,\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=5,\n",
        "        max_steps=200,\n",
        "        learning_rate=2e-4,\n",
        "        logging_steps=10,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=700,\n",
        "        output_dir=\"outputs\",\n",
        "        report_to=\"none\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VB0Lh7HAjKzF"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import train_on_responses_only\n",
        "\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part=\"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
        "    response_part=\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWmycRBHgHp1"
      },
      "outputs": [],
      "source": [
        "# Memory stats\n",
        "\n",
        "import torch\n",
        "\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrupgjvvgL7V"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJP51ptFxKGZ"
      },
      "outputs": [],
      "source": [
        "# Save LoRA fine tuned model and tokenizer\n",
        "\n",
        "output_dir = \"medqa_finetuned_model\"\n",
        "\n",
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gd7h9d9NgR7M"
      },
      "outputs": [],
      "source": [
        "# Final memory and time stats\n",
        "\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbJE7Fuhx4-B"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NGyNWS7x6g_"
      },
      "outputs": [],
      "source": [
        "base_model_name = \"unsloth/Llama-3.2-1B-Instruct\"\n",
        "lora_path = \"medqa_finetuned_model\"\n",
        "max_seq_length = 2048\n",
        "\n",
        "# === Load Base Model ===\n",
        "base_model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = base_model_name,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = torch.float16,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer = get_chat_template(tokenizer, chat_template=\"llama-3.2\")\n",
        "\n",
        "base_model.eval()\n",
        "\n",
        "# === Load LoRA Fine-Tuned Model ===\n",
        "lora_model, _ = FastLanguageModel.from_pretrained(\n",
        "    model_name = base_model_name,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = torch.float16,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "\n",
        "lora_model.load_adapter(lora_path)\n",
        "lora_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGsGJex7yGl_"
      },
      "outputs": [],
      "source": [
        "def compare_model_outputs(base_model, lora_model, tokenizer, prompt, max_new_tokens=128):\n",
        "    def generate_response(model, prompt):\n",
        "        formatted_prompt = tokenizer.apply_chat_template(\n",
        "            [{\"role\": \"user\", \"content\": prompt.strip()}],\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True,\n",
        "        )\n",
        "\n",
        "        inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
        "        input_len = inputs[\"input_ids\"].shape[-1]\n",
        "\n",
        "        output = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
        "        response = tokenizer.decode(output[0][input_len:], skip_special_tokens=True)\n",
        "\n",
        "        return response.strip()\n",
        "\n",
        "    print(\"Prompt:\\n\")\n",
        "    print(prompt.strip())\n",
        "\n",
        "    print(\"\\nBase Model Response:\\n\")\n",
        "    print(generate_response(base_model, prompt))\n",
        "\n",
        "    print(\"\\nFine-Tuned (LoRA) Model Response:\\n\")\n",
        "    print(generate_response(lora_model, prompt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qL8cTN0W4bld"
      },
      "outputs": [],
      "source": [
        "# Test question\n",
        "\n",
        "prompt = \"\"\"\n",
        "Q. A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7째F (36.5째C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus.\n",
        "Which of the following is the best treatment for this patient?\n",
        "\n",
        "Options:\n",
        "A: Ampicillin\n",
        "B: Ceftriaxone\n",
        "C: Ciprofloxacin\n",
        "D: Doxycycline\n",
        "E: Nitrofurantoin\n",
        "\n",
        "Please choose the correct answer.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-QsMW659m09"
      },
      "outputs": [],
      "source": [
        "compare_model_outputs(base_model, lora_model, tokenizer, prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tg1ik76R9obI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
